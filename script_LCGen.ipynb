{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa711f22-c313-4343-a584-e3efdecd3ad4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Scripts for analyzing of phantom outputs.\\n\\nThis script generate lightcurves (LC) by doing radiative transfer on a grid.\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Scripts for analyzing of phantom outputs.\n",
    "\n",
    "This script generate lightcurves (LC) by doing radiative transfer on a grid.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b19be2-030c-4fa9-ade5-f4bb38a0c13a",
   "metadata": {},
   "source": [
    ".\n",
    "\n",
    "# Def\n",
    "\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035afbe0-6134-4121-8416-f0dbbab169fb",
   "metadata": {},
   "source": [
    "## Imports & Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "093b4271-dab0-4b64-8b31-1588fc264966",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import pi\n",
    "from astropy import units\n",
    "from astropy import constants as const\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from numba import jit\n",
    "import sarracen\n",
    "import itertools\n",
    "from scipy import integrate, fft\n",
    "from scipy.spatial import kdtree\n",
    "from datetime import datetime\n",
    "#from moviepy.editor import ImageSequenceClip\n",
    "#from os import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c75c48c-a738-4783-a6af-560603d629a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import my modules listed in ./main/\n",
    "\n",
    "import clmuphantomlib as mupl\n",
    "from clmuphantomlib            import MyPhantomDataFrames, get_eos\n",
    "from clmuphantomlib.log        import is_verbose, say\n",
    "#from clmuphantomlib.settings   import DEFAULT_SETTINGS as settings\n",
    "from clmuphantomlib.units_util import get_val_in_unit, set_as_quantity #, get_units_field_name, get_units_cgs\n",
    "from clmuphantomlib.io         import json_dump, json_load\n",
    "from clmuphantomlib.eos        import get_eos_opacity\n",
    "from clmuphantomlib.light      import get_optical_depth_by_ray_tracing_3D, get_photosphere_on_ray\n",
    "\n",
    "from multiprocessing import cpu_count, Pool #Process, Queue\n",
    "NPROCESSES = 1 if cpu_count() is None else max(cpu_count(), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "859f4f11-9307-4f36-b896-3778f252d65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*   Note   :    script:\n",
      "\tWill use 8 processes for parallelization\n"
     ]
    }
   ],
   "source": [
    "# settings\n",
    "#\n",
    "#   imported from script_input.py file\n",
    "\n",
    "\n",
    "from script_LCGen__input import verbose, interm_dir, output_dir, unitsOut, PHOTOSPHERE_TAU, JOB_PROFILES_DICT\n",
    "from script_LCGen__input import job_nicknames, xyzs_list, no_xy, no_xy_txt, verbose_loop\n",
    "from _sharedFuncs import mpdf_read\n",
    "\n",
    "unitsOutTxt = {  key  : unitsOut[key].to_string('latex_inline') for key in unitsOut.keys() }\n",
    "\n",
    "\n",
    "# set metadata\n",
    "with open(\"_metadata__input.json\", 'r') as f:\n",
    "    metadata = json_load(f)\n",
    "metadata['Title'] = \"Getting light curves by intergrating across a grid of rays\"\n",
    "metadata['Description'] = f\"\"\"Getting light curves by intergrating across a grid of rays with the same directions\n",
    "for dump file data generated by phantom\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "if __name__ == '__main__' and is_verbose(verbose, 'note'):\n",
    "    # remember to check if name is '__main__' if you wanna say anything\n",
    "    #    so when you do multiprocessing the program doesn't freak out\n",
    "    say('note', \"script\", verbose, f\"Will use {NPROCESSES} processes for parallelization\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4853c017-d945-4a40-9098-74dd24e60180",
   "metadata": {},
   "outputs": [],
   "source": [
    "from clmuphantomlib.log import say, is_verbose\n",
    "from clmuphantomlib.geometry import get_dist2_between_2pt, get_closest_pt_on_line\n",
    "from clmuphantomlib.sph_interp import get_sph_interp, get_h_from_rho, get_no_neigh, _get_sph_interp_phantom_np\n",
    "from clmuphantomlib.units_util import set_as_quantity, set_as_quantity_temperature, get_units_field_name\n",
    "from clmuphantomlib.eos_base import EoS_Base\n",
    "from clmuphantomlib.light import integrate_along_ray_grid, integrate_along_ray_gridxy\n",
    "\n",
    "#  import (general)\n",
    "import numpy as np\n",
    "from numpy import typing as npt\n",
    "import numba\n",
    "from numba import jit, prange\n",
    "import sarracen\n",
    "\n",
    "from clmuphantomlib.geometry import get_dist2_from_pts_to_line, get_dist2_from_pt_to_line_nb, get_ray_unit_vec, get_rays_unit_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645cc989-5868-4956-9009-e3fd293c8736",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fba0ff6-57ab-4dc7-88d8-874c885a5e45",
   "metadata": {},
   "source": [
    "### LC integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3e20ab-9650-46fc-898f-b676d35540b8",
   "metadata": {},
   "source": [
    "#### Backup codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8551e10-e248-4947-bd7a-1671298482db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_along_ray(\n",
    "    sdf, ray, srcfuncs, photosphere_tau=1.,\n",
    "    verbose: int = 3,\n",
    "):\n",
    "    pts_on_ray, dtaus, pts_order = get_optical_depth_by_ray_tracing_3D(sdf=sdf, ray=ray)\n",
    "    if is_verbose(verbose, 'debug'):\n",
    "        say('debug', None, verbose, 'optical depth got.')\n",
    "\n",
    "    dtaus_ordered = dtaus[pts_order]\n",
    "    if is_verbose(verbose, 'debug'):\n",
    "        say('debug', None, verbose, 'ordered.')\n",
    "    srcfuncs_ordered = srcfuncs[pts_order]\n",
    "    if is_verbose(verbose, 'debug'):\n",
    "        say('debug', None, verbose, 'srcfuncs_ordered.')\n",
    "    dat_steps = np.full_like(dtaus_ordered, np.nan)\n",
    "\n",
    "    if True:\n",
    "    #if backwards:\n",
    "        # closest to observer to furtherest\n",
    "        dat = 0.\n",
    "        taus_ordered = np.array([0., *np.cumsum(dtaus_ordered[:-1])])\n",
    "        # dat_bwd_inc: dat_backward_contributions\n",
    "        dat_bwd_inc = np.exp(-taus_ordered) * (1 - np.exp(-dtaus_ordered)) * srcfuncs_ordered\n",
    "        dat = np.sum(dat_bwd_inc)\n",
    "        if is_verbose(verbose, 'info'):\n",
    "            say('info', None, verbose,\n",
    "                f\"backward dat = {dat}\")    # debug\n",
    "        if False:\n",
    "            # commented\n",
    "            # get the percentage of contribution to lum from photosphere outwards\n",
    "            photosphere_loc_index = np.searchsorted(taus_ordered, photosphere_tau) - 1\n",
    "            photosphere_contri_percent = np.sum(dat_bwd_inc[:photosphere_loc_index+2]) / dat * 100\n",
    "            dat_percent_index = np.where(np.cumsum(dat_bwd_inc) / dat<0.5)[0][-1]\n",
    "            if is_verbose(verbose, 'info'):\n",
    "                say('info', None, verbose,\n",
    "                    f\"\\tContribution to L from photosphere and outwards is: {photosphere_contri_percent} %\",\n",
    "                    f\"\\t50% Contributed correspond to tau = {taus_ordered[dat_percent_index]} \")\n",
    "        taus_ordered = taus_ordered[::-1]\n",
    "\n",
    "    else:\n",
    "        # furtherest to observer to closest\n",
    "        dat = 0.\n",
    "        #  pts_order[::-1]: reverse pts_order so that the furtherest particles comes first\n",
    "        taus_ordered = np.array([0., *np.cumsum(dtaus_ordered[:-1])])[::-1]\n",
    "        exp_mdtaus_r = np.exp(-dtaus_ordered[::-1])\n",
    "        srcfuncs_ordered_r = srcfuncs_ordered[::-1]\n",
    "        for index, srcfunc in enumerate(srcfuncs_ordered_r):\n",
    "            exp_mdtau = exp_mdtaus_r[index]\n",
    "            dat = exp_mdtau * dat + (1-exp_mdtau) * srcfunc\n",
    "            dat_steps[index] = dat\n",
    "        if is_verbose(verbose, 'info'):\n",
    "            say('info', None, verbose,\n",
    "                f\"forward dat = {dat}\")    # debug\n",
    "        \n",
    "    dtaus_ordered = dtaus_ordered[::-1]\n",
    "    pts_order = pts_order[::-1]  # furtherest to observer to closest\n",
    "    pts_on_ray_ordered = pts_on_ray[pts_order]\n",
    "    \n",
    "    \n",
    "    return  pts_order, pts_on_ray, dtaus_ordered, taus_ordered, \\\n",
    "            dat, dat_steps, dat_bwd_inc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc0e7a4e-ec6f-42a0-b0df-9786ca451f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_error_along_ray(\n",
    "    dtaus_ordered, # furtherest to closest\n",
    "    srcfuncs_ordered, srcfuncs_ordered_errp, srcfuncs_ordered_errm,\n",
    "    photosphere_tau=1.,\n",
    "):\n",
    "    #if backwards:\n",
    "    if True:\n",
    "        \n",
    "        # closest to observer to furtherest\n",
    "        dtaus_ordered = dtaus_ordered[::-1]\n",
    "        \n",
    "        # calc data + error\n",
    "        taus_ordered = np.array([0., *np.cumsum(dtaus_ordered[:-1])])\n",
    "        srcfuncs_errs = np.stack([srcfuncs_ordered, srcfuncs_ordered_errp, srcfuncs_ordered_errm])\n",
    "        srcfuncs_errs = np.flip(srcfuncs_errs, axis=-1) # flip it since we are working backwards\n",
    "        dat_bwd_inc_errs = np.exp(-taus_ordered) * (1 - np.exp(-dtaus_ordered)) * srcfuncs_errs\n",
    "        dat_errs = np.sum(dat_bwd_inc_errs, axis=-1)\n",
    "        \n",
    "        dat_bwd_inc_errs = np.flip(dat_bwd_inc_errs, axis=-1)\n",
    "        \n",
    "        if False:\n",
    "            # get data\n",
    "            dat_bwd_inc = dat_bwd_inc_errs[0]\n",
    "            dat = dat_errs[0]\n",
    "            dat_errp = dat_errs[1]\n",
    "            dat_errm = dat_errs[2]\n",
    "        \n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "        # furtherest to observer to closest\n",
    "        dat = 0.\n",
    "        dat_steps = np.full_like(dtaus_ordered, np.nan)\n",
    "        #  pts_order[::-1]: reverse pts_order so that the furtherest particles comes first\n",
    "        taus_ordered = np.array([0., *np.cumsum(dtaus_ordered[:-1])])[::-1]\n",
    "        exp_mdtaus_r = np.exp(-dtaus_ordered[::-1])\n",
    "        srcfuncs_ordered_r = srcfuncs_ordered[::-1]\n",
    "        for index, srcfunc in enumerate(srcfuncs_ordered_r):\n",
    "            exp_mdtau = exp_mdtaus_r[index]\n",
    "            dat = exp_mdtau * dat + (1-exp_mdtau) * srcfunc\n",
    "            dat_steps[index] = dat\n",
    "        print(\"forward dat = \", dat)    # debug\n",
    "        \n",
    "    #dtaus_ordered = dtaus_ordered[::-1]\n",
    "    return dat_errs, dat_bwd_inc_errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30983d75-44d0-4eec-9ae8-efb05e6cc93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True, parallel=True)\n",
    "def _integrate_along_ray_gridxy_sub_parallel_analysis_old_bkp(\n",
    "    pts_ordered          : np.ndarray,    # (npart, 3)-shaped\n",
    "    hs_ordered           : np.ndarray,    # (npart,  )-shaped\n",
    "    mkappa_div_h2_ordered: np.ndarray,    # (npart,  )-shaped\n",
    "    srcfuncs_ordered     : np.ndarray,    # (npart,  )-shaped\n",
    "    rays                 : np.ndarray,    # (nray, 2, 3)-shaped\n",
    "    kernel_rad           : float,\n",
    "    col_kernel           : numba.core.registry.CPUDispatcher,\n",
    "    pts_order            : np.ndarray,    # (npart,  )-shaped\n",
    "    rel_tol              : float = 1e-16, # because float64 is only has only 16 digits accuracy\n",
    ") -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Sub process for integrate_along_ray_gridxy(). Numba parallel version (using prange).\n",
    "\n",
    "    Unit vec must be [0., 0., 1.] (i.e. all rays must point upwards towards +z).\n",
    "\n",
    "    Private function. Assumes specific input type. See source code comments.\n",
    "\n",
    "    \"\"\"\n",
    "    #raise NotImplementedError\n",
    "\n",
    "    nray  = len(rays)\n",
    "    npart = len(srcfuncs_ordered)\n",
    "    ndim  = pts_ordered.shape[-1]\n",
    "    anses = np.zeros(nray)\n",
    "    indes = np.zeros(nray, dtype=np.int64)    # indexes of max contribution particle\n",
    "    contr = np.zeros(nray)    # contribution of the max contribution particle\n",
    "    jused = np.full(npart, False)    # is j-th particle in the ordered list used for this calculation?\n",
    "\n",
    "    # error tolerance of tau (part 1)\n",
    "    tol_tau_base = np.log(srcfuncs_ordered.sum()) - np.log(rel_tol)\n",
    "\n",
    "    # hr = h * kernel_rad\n",
    "    hrs_ordered = hs_ordered * kernel_rad\n",
    "\n",
    "    # loop over ray\n",
    "    for i in prange(nray):\n",
    "        ray = rays[i]\n",
    "        tau = 0.\n",
    "        ans = 0.\n",
    "        dans= 0.\n",
    "        dans_max_tmp = 0.\n",
    "        ind = -1\n",
    "\n",
    "        #   xy-grid specific solution\n",
    "        ray_x = ray[0, 0]\n",
    "        ray_y = ray[0, 1]\n",
    "        \n",
    "        # loop over particles\n",
    "        #for pt, hr, mkappa_div_h2, srcfunc in zip(\n",
    "        #    pts_ordered, hrs_ordered, mkappa_div_h2_ordered, srcfuncs_ordered):\n",
    "        for j in range(npart):\n",
    "            pt = pts_ordered[j]\n",
    "            hr = hrs_ordered[j]\n",
    "            \n",
    "            # check if the particle is within range\n",
    "            #   general solution\n",
    "            #q = get_dist2_from_pt_to_line_nb(pt, ray)**0.5 / h\n",
    "            #if q < kernel_rad:\n",
    "            #   xy-grid specific solution\n",
    "            if ray_x - hr < pt[0] and pt[0] < ray_x + hr and ray_y - hr < pt[1] and pt[1] < ray_y + hr:\n",
    "                h = hs_ordered[ j]\n",
    "                q = ((pt[0] - ray_x)**2 + (pt[1] - ray_y)**2)**0.5 / h\n",
    "                if q < kernel_rad:\n",
    "\n",
    "                    jused[j] = True\n",
    "                    \n",
    "                    # now do radiative transfer\n",
    "                    \n",
    "                    mkappa_div_h2 = mkappa_div_h2_ordered[j]\n",
    "                    srcfunc = srcfuncs_ordered[j]\n",
    "                    \n",
    "                    dtau = mkappa_div_h2 * col_kernel(q, ndim-1)\n",
    "                    #tau += dtau/2.\n",
    "                    dans = np.exp(-tau) * (1. - np.exp(-dtau)) * srcfunc\n",
    "                    ans += dans\n",
    "                    tau += dtau#/2.\n",
    "\n",
    "                    # note down the largest contributor\n",
    "                    if dans > dans_max_tmp:\n",
    "                        dans_max_tmp = dans\n",
    "                        ind = pts_order[j]\n",
    "    \n",
    "                    # terminate the calc for this ray if tau is sufficient large\n",
    "                    #    such that the relative error on ans is smaller than rel_tol\n",
    "                    # i.e. since when tau > np.log(srcfuncs_ordered.sum()) - np.log(rel_tol) - np.log(ans),\n",
    "                    #    we know that ans[i] - ans[i][k] < rel_tol * ans[i]\n",
    "                    # see my notes for derivation\n",
    "                    if tau > tol_tau_base - np.log(ans):\n",
    "                        break\n",
    "            \n",
    "        anses[i] = ans\n",
    "        indes[i] = ind\n",
    "        contr[i] = dans_max_tmp / ans\n",
    "    \n",
    "    return anses, indes, contr, jused\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394b11f2-040f-484e-9ed4-6132700848e1",
   "metadata": {},
   "source": [
    "#### Test codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2dc4b419-b471-4c9b-8405-2faf75fae819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test runs\n",
    "@jit(nopython=True, parallel=True)\n",
    "def _integrate_along_ray_gridxy_sub_parallel_analysis_test(\n",
    "    pts_ordered          : npt.NDArray[np.float_],    # (npart, 3)-shaped\n",
    "    hs_ordered           : npt.NDArray[np.float_],    # (npart,  )-shaped\n",
    "    mkappa_div_h2_ordered: npt.NDArray[np.float_],    # (npart,  )-shaped\n",
    "    srcfuncs_ordered     : npt.NDArray[np.float_],    # (npart,  )-shaped\n",
    "    rays                 : npt.NDArray[np.float_],    # (nray, 2, 3)-shaped\n",
    "    kernel_rad           : float,\n",
    "    col_kernel           : numba.core.registry.CPUDispatcher,\n",
    "    pts_order            : npt.NDArray[np.float_],    # (npart,  )-shaped\n",
    "    rel_tol              : float = 1e-16, # because float64 is only has only 16 digits accuracy\n",
    ") -> tuple[\n",
    "    npt.NDArray[np.float_],    # anses\n",
    "    npt.NDArray[np.float_],    # pones\n",
    "    npt.NDArray[np.float_],    # ptaus\n",
    "    npt.NDArray[np.int64 ],    # indes\n",
    "    npt.NDArray[np.float_],    # contr\n",
    "    npt.NDArray[np.bool_ ],    # jused\n",
    "]:\n",
    "    \"\"\"Sub process for integrate_along_ray_gridxy(). Numba parallel version (using prange).\n",
    "\n",
    "    Unit vec must be [0., 0., 1.] (i.e. all rays must point upwards towards +z).\n",
    "\n",
    "    Private function. Assumes specific input type. See source code comments.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    anses, pones, ptaus, indes, contr, jused\n",
    "    \n",
    "    anses: (nray,)-shaped np.ndarray[float]\n",
    "        Radiance (i.e. specific intensities) for each ray.\n",
    "\n",
    "    pones: (nray,)-shaped np.ndarray[float]\n",
    "        <1> for each pixel,\n",
    "        i.e. same integration of radiance but for a constant 'srcfunc' of '1', for each ray.\n",
    "        Helpful for consistency check (should be more or less 1 where ptaus is nan.)\n",
    "        do weighted average by weight of areas per pixel to get the total area of the object!\n",
    "\n",
    "    ptaus: (nray,)-shaped np.ndarray[float]\n",
    "        The optical depth for each pixel\n",
    "        *** WILL BE np.nan IF OPTICAL DEPTH IS DEEP (which will be MOST OF THE TIME.)  ***\n",
    "        can be used as an alternative way to calculate the area of the object.\n",
    "\n",
    "    indes: (nray,)-shaped np.ndarray[int]\n",
    "        indexes of max contribution particle\n",
    "\n",
    "    contr: (nray,)-shaped np.ndarray[float]\n",
    "        relative contribution (in fractions) of the max contribution particle\n",
    "\n",
    "    jused: (npart,)-shaped np.ndarray[bool]\n",
    "        whether j-th particle was used in the calculation.\n",
    "\n",
    "    \"\"\"\n",
    "    #raise NotImplementedError\n",
    "\n",
    "    nray  = len(rays)\n",
    "    npart = len(srcfuncs_ordered)\n",
    "    ndim  = pts_ordered.shape[-1]\n",
    "    anses = np.zeros(nray)\n",
    "    indes = np.zeros(nray, dtype=np.int64)    # indexes of max contribution particle\n",
    "    contr = np.zeros(nray)    # relative contribution of the max contribution particle\n",
    "    jused = np.full( npart, False)    # is j-th particle in the ordered list used for this calculation?\n",
    "    pones = np.zeros(nray)\n",
    "    ptaus = np.full(nray, np.nan)    # lower bound of the optical depth\n",
    "    \n",
    "\n",
    "    # error tolerance of tau (part 1)\n",
    "    tol_tau_base = np.log(srcfuncs_ordered.sum()) - np.log(rel_tol)\n",
    "\n",
    "    # hr = h * kernel_rad\n",
    "    hrs_ordered = hs_ordered * kernel_rad\n",
    "\n",
    "    # loop over ray\n",
    "    for i in prange(nray):\n",
    "        ray = rays[i]\n",
    "        tau = 0.\n",
    "        ans = 0.\n",
    "        dans= 0.\n",
    "        dans_max_tmp = 0.\n",
    "        ind = -1\n",
    "        fac = 0. # effectively <1>\n",
    "        dfac= 0. # factor\n",
    "\n",
    "        #   xy-grid specific solution\n",
    "        ray_x = ray[0, 0]\n",
    "        ray_y = ray[0, 1]\n",
    "        \n",
    "        # loop over particles\n",
    "        #for pt, hr, mkappa_div_h2, srcfunc in zip(\n",
    "        #    pts_ordered, hrs_ordered, mkappa_div_h2_ordered, srcfuncs_ordered):\n",
    "        for j in range(npart):\n",
    "            pt = pts_ordered[j]\n",
    "            hr = hrs_ordered[j]\n",
    "            \n",
    "            # check if the particle is within range\n",
    "            #   general solution\n",
    "            #q = get_dist2_from_pt_to_line_nb(pt, ray)**0.5 / h\n",
    "            #if q < kernel_rad:\n",
    "            #   xy-grid specific solution\n",
    "            if ray_x - hr < pt[0] and pt[0] < ray_x + hr and ray_y - hr < pt[1] and pt[1] < ray_y + hr:\n",
    "                h = hs_ordered[ j]\n",
    "                q = ((pt[0] - ray_x)**2 + (pt[1] - ray_y)**2)**0.5 / h\n",
    "                if q < kernel_rad:\n",
    "\n",
    "                    jused[j] = True\n",
    "                    \n",
    "                    # now do radiative transfer\n",
    "                    \n",
    "                    mkappa_div_h2 = mkappa_div_h2_ordered[j]\n",
    "                    srcfunc = srcfuncs_ordered[j]\n",
    "                    \n",
    "                    dtau = mkappa_div_h2 * col_kernel(q, ndim-1)\n",
    "                    #tau += dtau/2.\n",
    "                    dfac = np.exp(-tau) * (1. - np.exp(-dtau))\n",
    "                    dans = dfac * srcfunc\n",
    "                    ans += dans\n",
    "                    fac += dfac\n",
    "                    tau += dtau#/2.\n",
    "\n",
    "                    # note down the largest contributor\n",
    "                    if dans > dans_max_tmp:\n",
    "                        dans_max_tmp = dans\n",
    "                        ind = pts_order[j]\n",
    "    \n",
    "                    # terminate the calc for this ray if tau is sufficient large\n",
    "                    #    such that the relative error on ans is smaller than rel_tol\n",
    "                    # i.e. since when tau > np.log(srcfuncs_ordered.sum()) - np.log(rel_tol) - np.log(ans),\n",
    "                    #    we know that ans[i] - ans[i][k] < rel_tol * ans[i]\n",
    "                    # see my notes for derivation\n",
    "                    if tau > tol_tau_base - np.log(ans):\n",
    "                        break\n",
    "\n",
    "        else:\n",
    "            ptaus[i]=tau\n",
    "            \n",
    "        anses[i] = ans\n",
    "        indes[i] = ind\n",
    "        contr[i] = dans_max_tmp / ans\n",
    "        pones[i] = fac\n",
    "    \n",
    "    return anses, pones, ptaus, indes, contr, jused\n",
    "\n",
    "\n",
    "_integrate_along_ray_gridxy_sub_parallel_analysis = _integrate_along_ray_gridxy_sub_parallel_analysis_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4246b679-eef6-4b44-bbb8-039c050202f9",
   "metadata": {},
   "source": [
    "#### stable code (mostly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9aea1f60-0d2b-456f-b542-b87ad2b73105",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True, parallel=True)\n",
    "def _integrate_along_ray_gridxy_sub_parallel_err_ind(\n",
    "    pts_ordered          : np.ndarray,    # (npart, 3)-shaped\n",
    "    hs_ordered           : np.ndarray,    # (npart,  )-shaped\n",
    "    mkappa_div_h2_ordered: np.ndarray,    # (npart,  )-shaped\n",
    "    srcfuncs_ordered     : np.ndarray,    # (npart,  )-shaped\n",
    "    srcfuncs_err_ordered : np.ndarray,    # (npart,  )-shaped\n",
    "    rays                 : np.ndarray,    # (nray, 2, 3)-shaped\n",
    "    kernel_rad           : float,\n",
    "    col_kernel           : numba.core.registry.CPUDispatcher,\n",
    "    pts_order            : np.ndarray,    # (npart,  )-shaped\n",
    "    rel_tol              : float = 1e-16, # because float64 is only has only 16 digits accuracy\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Sub process for integrate_along_ray_gridxy(). Numba parallel version (using prange).\n",
    "\n",
    "    Unit vec must be [0., 0., 1.] (i.e. all rays must point upwards towards +z).\n",
    "\n",
    "    Private function. Assumes specific input type. See source code comments.\n",
    "\n",
    "    \"\"\"\n",
    "    #raise NotImplementedError\n",
    "\n",
    "    nray  = len(rays)\n",
    "    npart = len(srcfuncs_ordered)\n",
    "    ndim  = pts_ordered.shape[-1]\n",
    "    anses = np.zeros(nray)\n",
    "    erres = np.zeros(nray)\n",
    "\n",
    "    # error tolerance of tau (part 1)\n",
    "    tol_tau_base = np.log(srcfuncs_ordered.sum()) - np.log(rel_tol)\n",
    "\n",
    "    # hr = h * kernel_rad\n",
    "    hrs_ordered = hs_ordered * kernel_rad\n",
    "\n",
    "    # loop over ray\n",
    "    for i in prange(nray):\n",
    "        ray = rays[i]\n",
    "        tau = 0.\n",
    "        ans = 0.\n",
    "        err = 0.\n",
    "        dans= 0.\n",
    "\n",
    "        #   xy-grid specific solution\n",
    "        ray_x = ray[0, 0]\n",
    "        ray_y = ray[0, 1]\n",
    "        \n",
    "        # loop over particles\n",
    "        #for pt, hr, mkappa_div_h2, srcfunc in zip(\n",
    "        #    pts_ordered, hrs_ordered, mkappa_div_h2_ordered, srcfuncs_ordered):\n",
    "        for j in range(npart):\n",
    "            pt = pts_ordered[j]\n",
    "            hr = hrs_ordered[j]\n",
    "            \n",
    "            # check if the particle is within range\n",
    "            #   general solution\n",
    "            #q = get_dist2_from_pt_to_line_nb(pt, ray)**0.5 / h\n",
    "            #if q < kernel_rad:\n",
    "            #   xy-grid specific solution\n",
    "            if ray_x - hr < pt[0] and pt[0] < ray_x + hr and ray_y - hr < pt[1] and pt[1] < ray_y + hr:\n",
    "                h = hs_ordered[ j]\n",
    "                q = ((pt[0] - ray_x)**2 + (pt[1] - ray_y)**2)**0.5 / h\n",
    "                if q < kernel_rad:\n",
    "                    \n",
    "                    # now do radiative transfer\n",
    "                    \n",
    "                    mkappa_div_h2 = mkappa_div_h2_ordered[j]\n",
    "                    srcfunc = srcfuncs_ordered[j]\n",
    "                    srcfunc_err = srcfuncs_err_ordered[j]\n",
    "\n",
    "                    dtau = mkappa_div_h2 * col_kernel(q, ndim-1)\n",
    "                    #tau += dtau/2.\n",
    "                    dans = np.exp(-tau) * (1. - np.exp(-dtau)) * srcfunc\n",
    "                    err += np.exp(-tau) * (1. - np.exp(-dtau)) * srcfunc_err\n",
    "                    ans += dans\n",
    "                    tau += dtau#/2.\n",
    "    \n",
    "                    # terminate the calc for this ray if tau is sufficient large\n",
    "                    #    such that the relative error on ans is smaller than rel_tol\n",
    "                    # i.e. since when tau > np.log(srcfuncs_ordered.sum()) - np.log(rel_tol) - np.log(ans),\n",
    "                    #    we know that ans[i] - ans[i][k] < rel_tol * ans[i]\n",
    "                    # see my notes for derivation\n",
    "                    if tau > tol_tau_base - np.log(ans):\n",
    "                        break\n",
    "            \n",
    "        anses[i] = ans\n",
    "        erres[i] = err\n",
    "    \n",
    "    return anses, erres\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882b95a0-7c74-4249-a863-c7612ad7316a",
   "metadata": {},
   "source": [
    "#### integrate only, no error estiamtes (faster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a391db2-a336-4e5b-96be-d3c0692e96ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# integrate only, no error estiamtes\n",
    "\n",
    "def integrate_along_ray_gridxy_ind(\n",
    "    sdf     : sarracen.SarracenDataFrame,\n",
    "    srcfuncs: np.ndarray,\n",
    "    rays    : np.ndarray,\n",
    "    ray_unit_vec: np.ndarray|None = None,\n",
    "    kernel  : sarracen.kernels.BaseKernel = None,\n",
    "    parallel: bool = False,\n",
    "    err_h   : float = 1.0,\n",
    "    rel_tol : float = 1e-15,\n",
    "    sdf_kdtree : kdtree.KDTree = None,\n",
    "    xyzs_names_list : list = ['x', 'y', 'z'],\n",
    "    verbose : int = 3,\n",
    ") -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Backward integration of source functions along a grided ray (traced backwards), weighted by optical depth.\n",
    "    \n",
    "    Assuming all rays facing +z direction. (with the same ray_unit_vec [0., 0., 1.])\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sdf: sarracen.SarracenDataFrame\n",
    "        Must contain columns: x, y, z, h, m, kappa\n",
    "        \n",
    "    rays: (nray, 2, 3)-shaped array\n",
    "        Representing the ray trajectory. Currently only straight infinite lines are supported.\n",
    "        each ray is of the format:\n",
    "        [[begin point], [end point]]\n",
    "        where the end point is closer to the observer.\n",
    "\n",
    "    srcfuncs: 1D array\n",
    "        arrays describing the source function for every particle\n",
    "        \n",
    "    kernel: sarracen.kernels.base_kernel\n",
    "        Smoothing kernel for SPH data interpolation.\n",
    "        If None, will use the one in sdf.\n",
    "\n",
    "    parallel: bool\n",
    "        If to use the numba parallel function\n",
    "\n",
    "    err_h: float ( > 0. )\n",
    "        determine confidence level.\n",
    "        e.g.,\n",
    "            1.0 will give error assuming error range is +/-1.0 smoothing length h;\n",
    "            0.5 will give error assuming error range is +/-0.5 smoothing length h;\n",
    "            etc. etc.\n",
    "            \n",
    "    rel_tol : float\n",
    "        maximum relative error tolerence per ray.\n",
    "        Default 1e-15 because float64 is only accurate to ~16th digits.\n",
    "\n",
    "    sdf_kdtree : kdtree.KDTree\n",
    "        KDTree built from sdf[['x', 'y', 'z']], for fast neighbour search.\n",
    "        \n",
    "    xyzs_names_list: list\n",
    "        list of names of the columns that represents x, y, z axes (i.e. coord axes names)\n",
    "        MUST INCLUDE ALL THREE AXES LABELS.\n",
    "        If only 2 is included, WILL ASSUME IT IS 2D CACULATIONS.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    anses, indes, contr, pts_order_used\n",
    "    \n",
    "    anses: np.ndarray\n",
    "        Radiance (i.e. specific intensities) for each ray.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # init\n",
    "    npart : int = len(sdf)\n",
    "    nray  : int = len(rays)\n",
    "    if kernel is None: kernel = sdf.kernel\n",
    "    kernel_rad = float(kernel.get_radius())\n",
    "    col_kernel = kernel.get_column_kernel_func(samples=1000) # w integrated from z\n",
    "    if ray_unit_vec is None: ray_unit_vec = get_ray_unit_vec(rays[0])\n",
    "    \n",
    "    pts    = np.array(sdf[xyzs_names_list], order='C')    # (npart, 3)-shaped array (must be this shape for pts_order sorting below)\n",
    "    hs     = np.array(sdf[ 'h'           ], order='C')    # npart-shaped array\n",
    "    masses = np.array(sdf[ 'm'           ], order='C')\n",
    "    kappas = np.array(sdf[ 'kappa'       ], order='C')\n",
    "    srcfuncs = np.array(srcfuncs          , order='C')\n",
    "    ndim   = pts.shape[-1]\n",
    "    mkappa_div_h2_arr = masses * kappas / hs**(ndim-1)\n",
    "    \n",
    "    # sanity check\n",
    "    if is_verbose(verbose, 'err') and not np.allclose(ray_unit_vec, get_rays_unit_vec(rays)):\n",
    "        raise ValueError(f\"Inconsistent ray_unit_vec {ray_unit_vec} with the rays.\")\n",
    "\n",
    "    if is_verbose(verbose, 'warn') and ndim != 3:\n",
    "        say('warn', None, verbose, f\"ndim == {ndim} is not 3.\")\n",
    "\n",
    "    # (npart-shaped array of the indices of the particles from closest to the observer to the furthest)\n",
    "    pts_order             = np.argsort( np.sum(pts * ray_unit_vec, axis=-1) )[::-1]\n",
    "    pts_ordered           = pts[     pts_order]\n",
    "    hs_ordered            = hs[      pts_order]\n",
    "    mkappa_div_h2_ordered = mkappa_div_h2_arr[pts_order]\n",
    "    srcfuncs_ordered      = srcfuncs[pts_order]\n",
    "\n",
    "    # get used particles indexes\n",
    "    if parallel:\n",
    "        #anses, indes, contr, jused = _integrate_along_ray_gridxy_sub_parallel_analysis(\n",
    "        anses, areas, ptaus, indes, contr, jused = _integrate_along_ray_gridxy_sub_parallel_analysis(\n",
    "            pts_ordered, hs_ordered, mkappa_div_h2_ordered, srcfuncs_ordered, rays, kernel_rad, col_kernel, pts_order, rel_tol=rel_tol)\n",
    "    else:\n",
    "        raise NotImplementedError(\"parallel=False version of this function not yet implemented.\")\n",
    "\n",
    "    pts_order_used = pts_order[jused]\n",
    "    if is_verbose(verbose, 'info'):\n",
    "        nused = len(pts_order_used)\n",
    "        say('info', None, verbose,\n",
    "            f\"{nused} particles actually participated calculation\",\n",
    "            f\"({int(nused/npart*10000)/100.}% of all particles,\",\n",
    "            f\"average {int(nused/nray*100)/100.} per ray.)\", sep=' ')\n",
    "\n",
    "    \n",
    "    return anses, areas, ptaus, indes, contr, pts_order_used\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e03222-3349-40cc-ac4d-fab3dcdef35b",
   "metadata": {},
   "source": [
    "#### integrate with error estiamtes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3517015f-7833-48fd-ae31-31678d6c50df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# integrate and integrate error\n",
    "\n",
    "\n",
    "def integrate_along_ray_gridxy_err_ind(\n",
    "    sdf     : sarracen.SarracenDataFrame,\n",
    "    srcfuncs: np.ndarray,\n",
    "    rays    : np.ndarray,\n",
    "    ray_unit_vec: np.ndarray|None = None,\n",
    "    kernel  : sarracen.kernels.BaseKernel = None,\n",
    "    parallel: bool = False,\n",
    "    err_h   : float = 1.0,\n",
    "    rel_tol : float = 1e-16,\n",
    "    sdf_kdtree : kdtree.KDTree = None,\n",
    "    xyzs_names_list : list = ['x', 'y', 'z'],\n",
    "    verbose : int = 3,\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Backward integration of source functions along a grided ray (traced backwards), weighted by optical depth.\n",
    "    \n",
    "    Assuming all rays facing +z direction. (with the same ray_unit_vec [0., 0., 1.])\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sdf: sarracen.SarracenDataFrame\n",
    "        Must contain columns: x, y, z, h, m, kappa\n",
    "        \n",
    "    rays: (nray, 2, 3)-shaped array\n",
    "        Representing the ray trajectory. Currently only straight infinite lines are supported.\n",
    "        each ray is of the format:\n",
    "        [[begin point], [end point]]\n",
    "        where the end point is closer to the observer.\n",
    "\n",
    "    srcfuncs: 1D array\n",
    "        arrays describing the source function for every particle\n",
    "        \n",
    "    kernel: sarracen.kernels.base_kernel\n",
    "        Smoothing kernel for SPH data interpolation.\n",
    "        If None, will use the one in sdf.\n",
    "\n",
    "    parallel: bool\n",
    "        If to use the numba parallel function\n",
    "\n",
    "    err_h: float ( > 0. )\n",
    "        determine confidence level.\n",
    "        e.g.,\n",
    "            1.0 will give error assuming error range is +/-1.0 smoothing length h;\n",
    "            0.5 will give error assuming error range is +/-0.5 smoothing length h;\n",
    "            etc. etc.\n",
    "            \n",
    "    rel_tol : float\n",
    "        maximum relative error tolerence per ray.\n",
    "        Default 1e-15 because float64 is only accurate to ~16th digits.\n",
    "\n",
    "    sdf_kdtree : kdtree.KDTree\n",
    "        KDTree built from sdf[['x', 'y', 'z']], for fast neighbour search.\n",
    "        \n",
    "    xyzs_names_list: list\n",
    "        list of names of the columns that represents x, y, z axes (i.e. coord axes names)\n",
    "        MUST INCLUDE ALL THREE AXES LABELS.\n",
    "        If only 2 is included, WILL ASSUME IT IS 2D CACULATIONS.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    rads, errs\n",
    "    \n",
    "    rads: np.ndarray\n",
    "        Radiance (i.e. specific intensities) for each ray.\n",
    "\n",
    "    errs: np.ndarray\n",
    "        Uncertainties of Radiance (i.e. specific intensities) for each ray.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # init\n",
    "    npart : int = len(sdf)\n",
    "    nray  : int = len(rays)\n",
    "    if kernel is None: kernel = sdf.kernel\n",
    "    kernel_rad = float(kernel.get_radius())\n",
    "    col_kernel = kernel.get_column_kernel_func(samples=1000) # w integrated from z\n",
    "    if ray_unit_vec is None: ray_unit_vec = get_ray_unit_vec(rays[0])\n",
    "    \n",
    "    pts    = np.array(sdf[xyzs_names_list], order='C')    # (npart, 3)-shaped array (must be this shape for pts_order sorting below)\n",
    "    hs     = np.array(sdf[ 'h'           ], order='C')    # npart-shaped array\n",
    "    masses = np.array(sdf[ 'm'           ], order='C')\n",
    "    kappas = np.array(sdf[ 'kappa'       ], order='C')\n",
    "    srcfuncs = np.array(srcfuncs          , order='C')\n",
    "    ndim   = pts.shape[-1]\n",
    "    mkappa_div_h2_arr = masses * kappas / hs**(ndim-1)\n",
    "    \n",
    "    # sanity check\n",
    "    if is_verbose(verbose, 'err') and not np.allclose(ray_unit_vec, get_rays_unit_vec(rays)):\n",
    "        raise ValueError(f\"Inconsistent ray_unit_vec {ray_unit_vec} with the rays.\")\n",
    "\n",
    "    if is_verbose(verbose, 'warn') and ndim != 3:\n",
    "        say('warn', None, verbose, f\"ndim == {ndim} is not 3.\")\n",
    "\n",
    "    # (npart-shaped array of the indices of the particles from closest to the observer to the furthest)\n",
    "    pts_order             = np.argsort( np.sum(pts * ray_unit_vec, axis=-1) )[::-1]\n",
    "    pts_ordered           = pts[     pts_order]\n",
    "    hs_ordered            = hs[      pts_order]\n",
    "    mkappa_div_h2_ordered = mkappa_div_h2_arr[pts_order]\n",
    "    srcfuncs_ordered      = srcfuncs[pts_order]\n",
    "\n",
    "    # get used particles indexes\n",
    "    if parallel:\n",
    "        #anses, indes, contr, jused = _integrate_along_ray_gridxy_sub_parallel_analysis(\n",
    "        anses, areas, ptaus, indes, contr, jused = _integrate_along_ray_gridxy_sub_parallel_analysis(\n",
    "            pts_ordered, hs_ordered, mkappa_div_h2_ordered, srcfuncs_ordered, rays, kernel_rad, col_kernel, pts_order, rel_tol=rel_tol)\n",
    "    else:\n",
    "        raise NotImplementedError(\"Non-parallel version of this function not yet implemented.\")\n",
    "\n",
    "    pts_order_used = pts_order[jused]\n",
    "    if is_verbose(verbose, 'info'):\n",
    "        nused = len(pts_order_used)\n",
    "        say('info', None, verbose,\n",
    "            f\"{nused} particles actually participated in calculation\",\n",
    "            f\"({int(nused/npart*10000)/100.}% of all particles,\",\n",
    "            f\"average {int(nused/nray*100)/100.} per ray.)\", sep=' ')\n",
    "\n",
    "\n",
    "    # calculate error in those particles\n",
    "    sdf['_srcfunc'] = srcfuncs\n",
    "    srcfuncs_err_orderedu = get_sph_error(sdf, '_srcfunc', pts_order_used, err_h=err_h, sdf_kdtree=sdf_kdtree,\n",
    "                                 kernel=kernel, xyzs_names_list=xyzs_names_list, verbose=verbose)[:, 0]\n",
    "    sdf['_srcfunc_err'] = np.nan\n",
    "    sdf['_srcfunc_err'].iloc[pts_order_used] = srcfuncs_err_orderedu\n",
    "\n",
    "    if is_verbose(verbose, 'debug'):\n",
    "        nused = len(pts_order_used)\n",
    "        say('debug', None, verbose,\n",
    "            f\"error calculation completed.\",\n",
    "            f\"average relative error of srcfunc\",\n",
    "            f\"{int((srcfuncs_err_orderedu/srcfuncs_ordered[jused]).sum()/len(pts_order_used)*10000)/100.}%\",\n",
    "            sep=' ')\n",
    "\n",
    "    rads, errs = _integrate_along_ray_gridxy_sub_parallel_err_ind(\n",
    "        pts_ordered[jused], hs_ordered[jused], mkappa_div_h2_ordered[jused], srcfuncs_ordered[jused],\n",
    "        srcfuncs_err_orderedu, rays, kernel_rad, col_kernel, pts_order, rel_tol=rel_tol)\n",
    "    #raise NotImplementedError\n",
    "    \n",
    "    return rads, errs, areas, ptaus\n",
    "\n",
    "#integrate_along_ray_gridxy_ind = integrate_along_ray_gridxy_err_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ea50ff-b1e8-4239-9172-0982ef13ede3",
   "metadata": {},
   "source": [
    "### rays grid generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5aa6efe-41e2-4f0a-be7f-b5307c1b7708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xy_grids_of_rays(\n",
    "    sdf  : None|sarracen.SarracenDataFrame = None,\n",
    "    #dXs  : None|list[list[float], list[float]]|np.ndarray= None,\n",
    "    no_xy: tuple[int, int] = (32, 32),\n",
    "    #orig_vec: np.ndarray = np.zeros(3),\n",
    "    frac_contained: float = 100., #99.73,\n",
    "    use_adaptive_grid: bool = False,\n",
    "    xyzs_names_list : list = ['x', 'y', 'z'],\n",
    "    w_rad: None|float = None,\n",
    "    verbose: int = 3,\n",
    ") -> tuple[np.ndarray, np.ndarray, list[np.ndarray]]:\n",
    "    \"\"\"Get a grid of rays (must pointing at z direction (i.e. xyzs_names_list[-1] direction) for now).\n",
    "\n",
    "    Supply either sdf or both dx and dy.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sdf: sarracen.SarracenDataFrame\n",
    "\n",
    "    no_xy: tuple[int, int]\n",
    "        number of the rays per axes.\n",
    "        \n",
    "    frac_contained : float\n",
    "        Suggested percentage of the particle that are contained within the grid. in (0, 100]\n",
    "\n",
    "    use_adaptive_grid : bool\n",
    "        if True,\n",
    "            will scale dXs according to particle distribution instead of even intervals,\n",
    "            if dXs is None or (None, None).\n",
    "\n",
    "    xyzs_names_list: list\n",
    "        list of names of the columns that represents x, y, z axes (i.e. coord axes names)\n",
    "        WARNING: since ndim==len(xyzs_names_list), if len(xyzs_names_list) !=3 will resulting non-3D results.\n",
    "            In which case you will need to change no_xy as well.\n",
    "\n",
    "\n",
    "    Returns: rays, areas, dXs\n",
    "    -------\n",
    "    rays: (no_ray, 2, 3)-shaped np.ndarray\n",
    "\n",
    "    areas: (no_ray)-shaped np.ndarray\n",
    "        areas corresponding to each ray in the grid\n",
    "        \n",
    "    dXs: list of no_xy[i]-shaped np.ndarray\n",
    "        width of the grid cells. in sdf units['dist'].\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    unit_vec = np.zeros(len(xyzs_names_list))\n",
    "    unit_vec[-1] = 1.\n",
    "    #x0, y0, z0 = orig_vec\n",
    "    z0 = 0.    # z value for rays\n",
    "    if w_rad is None:\n",
    "        w_rad = sdf.kernel.get_radius()\n",
    "\n",
    "    # sanity checks\n",
    "    if is_verbose(verbose, 'warn') and len(xyzs_names_list) != 3:\n",
    "        say('warn', 'get_xy_grids_of_rays()', verbose,\n",
    "            f\"xyzs_names_list being {xyzs_names_list}, its len = {len(xyzs_names_list)} is not 3.\",\n",
    "            f\"This means we are assuming {len(xyzs_names_list)}D.\")\n",
    "    if is_verbose(verbose, 'error') and len(xyzs_names_list) != len(no_xy) + 1:\n",
    "        say('error', 'get_xy_grids_of_rays()', verbose,\n",
    "            f\"ndim (=={len(xyzs_names_list)}) != len(no_xy) (=={len(no_xy)}) + 1\",\n",
    "            f\"i.e. asked ray grid dimension {no_xy} does not makes sense.\",\n",
    "            \"This will likely cause error in the next steps.\")\n",
    "    \n",
    "    # get dx & dy\n",
    "    frac_contained_m = 50. - frac_contained / 2.\n",
    "    frac_contained_p = 50. + frac_contained / 2.\n",
    "\n",
    "    Xs_edges = []\n",
    "    for i, label in enumerate(xyzs_names_list[:-1]):\n",
    "        #i0 = orig_vec[i]\n",
    "        if use_adaptive_grid:\n",
    "            # fraction points for the adaptive grid\n",
    "            fracs = np.linspace(frac_contained_m, frac_contained_p, no_xy[i]+1)\n",
    "            # edge points for the grid\n",
    "            Xs_edges.append(\n",
    "                np.percentile(np.concatenate((sdf[label] - w_rad*sdf['h'], sdf[label] + w_rad*sdf['h'])), fracs))\n",
    "        else:\n",
    "            Xs_edges.append(\n",
    "                np.linspace(\n",
    "                    *np.percentile(\n",
    "                        np.concatenate(\n",
    "                            (sdf[label] - w_rad*sdf['h'], sdf[label] + w_rad*sdf['h']),\n",
    "                        ), (frac_contained_m, frac_contained_p),\n",
    "                    ), no_xy[i]+1,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    dXs = np.array([np.diff(Xi_edges) for Xi_edges in Xs_edges])    # each item is (no_xy[i]+1,)-shaped\n",
    "    Xs_centers = np.array([Xi_edges[:-1] + dXi/2. for dXi, Xi_edges in zip(dXs, Xs_edges)])    # each item is (no_xy[i],)-shaped\n",
    "\n",
    "    # Note: orig_vecs must be 2D (i.e. in shape of (no_ray, 3))\n",
    "    orig_vecs = np.array([[*xy, z0] for xy in itertools.product(*Xs_centers)])\n",
    "    #orig_vecs = [[[x, y, z0] for x, y in zip(xs, ys)] for xs, ys in zip(*np.meshgrid(*xys))]\n",
    "    areas = np.array([dx*dy for dy in dXs[1] for dx in dXs[0]])\n",
    "\n",
    "    rays = mupl.geometry.get_rays(orig_vecs=orig_vecs, unit_vecs=unit_vec)\n",
    "    \n",
    "    return rays, areas, dXs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d16cde-9d77-4113-844d-c50637f3a07a",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97c973f4-9f35-4dd6-ba34-7836d5a16625",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_imshow(\n",
    "    no_xy: tuple[int, int],\n",
    "    rays: units.Quantity|np.ndarray,\n",
    "    data: units.Quantity|np.ndarray,\n",
    "    job_profile  : dict= None,\n",
    "    file_index   : int = -1,\n",
    "    title_suffix : str =\"\",\n",
    "    notes        : dict= None,\n",
    "    data_label   : str =\"\",\n",
    "    save_label   : str =\"\",\n",
    "    xyzs         : str|list[str] = 'xyz',\n",
    "    out_exts     : list[str] = ['pdf', 'png'],\n",
    "    norm=None,\n",
    "    cmap=None,\n",
    "    output_dir:str|None=None,\n",
    "    verbose = 3,\n",
    "):\n",
    "    \"\"\"Plotting a heatmap (contourf) of 1D data located at rays\"\"\"\n",
    "\n",
    "\n",
    "    if not isinstance(data, units.Quantity):\n",
    "        data = set_as_quantity(data, units.dimensionless_unscaled)\n",
    "\n",
    "    if not isinstance(rays, units.Quantity):\n",
    "        rays = set_as_quantity(rays, units.dimensionless_unscaled)\n",
    "\n",
    "    if job_profile is None:\n",
    "        job_profile = {\n",
    "            'plot_title_suffix': '',\n",
    "            'nickname'         : '',\n",
    "        }\n",
    "\n",
    "    #Xs = rays[:, 0, 0]\n",
    "    #Ys = rays[:, 0, 1]\n",
    "    rays_val = rays.reshape(*no_xy, *rays.shape[1:]).value\n",
    "    extent = (\n",
    "        rays_val[ 0, 0, 0, 0] - (rays_val[ 1, 0, 0, 0] - rays_val[ 0, 0, 0, 0])/2,\n",
    "        rays_val[-1,-1, 0, 0] + (rays_val[-1,-1, 0, 0] - rays_val[-2,-1, 0, 0])/2,\n",
    "        rays_val[ 0, 0, 0, 1] - (rays_val[ 0, 1, 0, 1] - rays_val[ 0, 0, 0, 1])/2,\n",
    "        rays_val[-1,-1, 0, 1] + (rays_val[-1,-1, 0, 1] - rays_val[-1,-2, 0, 1])/2,\n",
    "    )\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    \n",
    "    cax = ax.imshow(data.reshape(no_xy).T.value, norm=norm, cmap=cmap, origin='lower', extent=extent)\n",
    "    #cax = ax.contourf(Xs.reshape(no_xy), Ys.reshape(no_xy), data.reshape(no_xy), cmap=cmap)\n",
    "    fig.colorbar(cax, label=f\"{data_label} / {data.unit.to_string('latex_inline')}\")\n",
    "    ax.set_xlabel(f\"${xyzs[0]}$ / {rays.unit.to_string('latex_inline')}\")\n",
    "    ax.set_ylabel(f\"${xyzs[1]}$ / {rays.unit.to_string('latex_inline')}\")\n",
    "    if notes is not None:\n",
    "        ax.text(\n",
    "            0.98, 0.98,\n",
    "            f\"Time = {notes['time']:.1f}\\n\" + \\\n",
    "            f\" $L$ = {notes['lum']:.0f}\",\n",
    "            #color = \"black\",\n",
    "            ha = 'right', va = 'top',\n",
    "            transform=ax.transAxes,\n",
    "        )\n",
    "\n",
    "    no_xy_txt = 'x'.join([f'{i}' for i in no_xy])\n",
    "    outfilename_noext = f\"{output_dir}heat_{job_profile['nickname']}_{file_index:05d}_{''.join(xyzs)}_{save_label}_{no_xy_txt}\"\n",
    "    outfilenames = []\n",
    "\n",
    "    # write pdf\n",
    "    for out_ext in out_exts:\n",
    "        outfilename = f\"{outfilename_noext}.{out_ext}\"\n",
    "        if out_ext == 'pdf':\n",
    "            ax.set_title('')\n",
    "        else:\n",
    "            ax.set_title(f\"Heatmap of {data_label}\\n{job_profile['plot_title_suffix']}\")\n",
    "        fig.savefig(outfilename)\n",
    "        outfilenames.append(outfilename)\n",
    "        if is_verbose(verbose, 'note'):\n",
    "            say('note', None, verbose, f\"Fig saved to {outfilename}.\")\n",
    "        \n",
    "    return fig, ax, outfilenames\n",
    "\n",
    "## example\n",
    "#fig, ax = plot_imshow(\n",
    "#    no_xy, rays * units.Rsun, anses, data_label=\"$I$\", save_label=\"I_xyz\",\n",
    "#    job_profile=job_profile, file_index=file_index, output_dir=output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8daf8b-0b46-4deb-80a1-f6d4ad05bc06",
   "metadata": {},
   "source": [
    "### Error estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9cd2dae-0051-430f-b6f4-ee4d84316ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sph_neighbours(\n",
    "    sdf_kdtree : kdtree.KDTree,\n",
    "    xyz_i      : np.ndarray,\n",
    "    h_i        : float,\n",
    "    w_rad      : float,\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Find neighbours of xyz_i within (w_rad*h_i) distance, using k-d tree.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sdf_kdtree : kdtree.KDTree\n",
    "        KDTree built from sdf[['x', 'y', 'z']], for fast neighbour search.\n",
    "    xyz_i : (ndim,)-shaped numpy array\n",
    "        position of the querying point\n",
    "    h_i : float\n",
    "        Smoothing length\n",
    "    w_rad: float\n",
    "        radius of the smoothing kernel w.\n",
    "    \n",
    "    Returns: dists, indices\n",
    "    -------\n",
    "    dists : np.ndarray\n",
    "        distances of the neighbouring points to the querying point\n",
    "    indices : np.ndarray\n",
    "        indices of the neighbouring points\n",
    "    \"\"\"\n",
    "    npart = sdf_kdtree.n\n",
    "    dists, indices = sdf_kdtree.query(xyz_i, k=npart, distance_upper_bound=w_rad*h_i)\n",
    "    indices_indices = indices<npart\n",
    "    return dists[indices_indices], indices[indices_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6cc574e5-6910-40f0-b845-66e99c39055b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sph error estimation\n",
    "\n",
    "# found this func in my old codes ../photosphere/Analysis_PhLoc.ipynb\n",
    "\n",
    "def get_sph_error(\n",
    "    sdf        : sarracen.SarracenDataFrame,\n",
    "    target_labels   : str|list[str],\n",
    "    target_indicies : int|list[int]|np.ndarray = [],\n",
    "    err_h      : float    = 1.0,\n",
    "    sdf_kdtree : kdtree.KDTree = None,\n",
    "    kernel     : sarracen.kernels.BaseKernel = None,\n",
    "    xyzs_names_list : list = ['x', 'y', 'z'],\n",
    "    verbose: int = 3,\n",
    ") -> np.ndarray:    # (ntarget, nval)-shaped\n",
    "    \"\"\"Calculate error bar for sarracen data frame.\n",
    "    \n",
    "    Assuming 3D.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sdf: sarracen.SarracenDataFrame\n",
    "        Need to contain columns: x, y, z, m, h, rho.\n",
    "        If density (rho) is not in sdf, will compute rho.\n",
    "        \n",
    "    target_labels: str or list of str (len>2)\n",
    "        Column label of the target data in sdf for error computing\n",
    "        \n",
    "    target_indicies: int or list of int or np.ndarray\n",
    "        indices for particles in sdf for error calculating\n",
    "        \n",
    "    err_h: float ( > 0. )\n",
    "        determine confidence level.\n",
    "        e.g.,\n",
    "            1.0 will give error assuming error range is +/-1.0 smoothing length h;\n",
    "            0.5 will give error assuming error range is +/-0.5 smoothing length h;\n",
    "            etc. etc.\n",
    "            \n",
    "    sdf_kdtree: kdtree.KDTree\n",
    "        KDTree built from sdf[['x', 'y', 'z']], for fast neighbour search.\n",
    "        If None, will build one.\n",
    "        \n",
    "    kernel: sarracen.kernels.base_kernel\n",
    "        Smoothing kernel for SPH data interpolation.\n",
    "        If None, will use the one in sdf.\n",
    "\n",
    "    xyzs_names_list: list\n",
    "        list of names of the columns that represents x, y, z axes (i.e. coord axes names)\n",
    "        MUST INCLUDE ALL THREE AXES LABELS.\n",
    "        If only 2 is included, WILL ASSUME IT IS 2D CACULATIONS.\n",
    "    \n",
    "            \n",
    "    Returns: dvals\n",
    "    -------\n",
    "    dvalsp: (ntarget, nval)-shaped ndarray\n",
    "        error.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # init\n",
    "    \n",
    "    xyzs = sdf[xyzs_names_list].to_numpy()\n",
    "    ms   = sdf['m'   ].to_numpy()\n",
    "    hs   = sdf['h'   ].to_numpy()\n",
    "    rhos = sdf['rho' ].to_numpy()\n",
    "    target_indicies = np.atleast_1d(target_indicies)\n",
    "    dxes = err_h * hs[target_indicies]\n",
    "    # assuming 3D in the following calc\n",
    "    locs = xyzs[target_indicies]\n",
    "    vals =  sdf[ target_labels ].to_numpy()[target_indicies]\n",
    "    if vals.ndim == 2: nval = vals.shape[1]\n",
    "    else:              nval = 1\n",
    "    ntarget = len(target_indicies)\n",
    "    ndim = len(xyzs_names_list)\n",
    "    \n",
    "    if sdf_kdtree is None:\n",
    "        sdf_kdtree = kdtree.KDTree(xyzs)\n",
    "    if kernel is None:\n",
    "        kernel = sdf.kernel\n",
    "    kernel_rad = float(kernel.get_radius())\n",
    "    kernel_w   = kernel.w\n",
    "        \n",
    "    neigh_rad = kernel_rad + err_h\n",
    "    \n",
    "    # ans array\n",
    "    dvals = np.full((ntarget, nval), np.nan)\n",
    "    \n",
    "    \n",
    "    for i in range(ntarget):\n",
    "        loc = locs[i]\n",
    "        val = vals[i]\n",
    "        dx  = dxes[i]\n",
    "        h   = hs[target_indicies[i]]\n",
    "        # find all neighbours within 3h, this includes all points needed for the calc of error for this particle\n",
    "        _, neigh_inds = get_sph_neighbours(sdf_kdtree, loc, h, neigh_rad)\n",
    "\n",
    "        # prepare data\n",
    "        sdf_temp = sdf.iloc[neigh_inds]\n",
    "        \n",
    "        loc_plus_dx = [loc for i in range(ndim*2)]\n",
    "        for j in range(ndim):\n",
    "            loc_plus_dx[j][j] += dx\n",
    "            loc_plus_dx[ndim+j][j] -= dx\n",
    "\n",
    "        dval_xyz = get_sph_interp(sdf_temp, target_labels, loc_plus_dx, kernel=kernel, verbose=0) - val\n",
    "        dvals[i] = ((dval_xyz**2).sum(axis=0)/len(dval_xyz))**0.5\n",
    "    \n",
    "    #dvals = dvals.squeeze()\n",
    "    return dvals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b89f446-18c0-4b53-a8c6-6a208b05a055",
   "metadata": {},
   "source": [
    ".\n",
    "\n",
    "## Main\n",
    "\n",
    ".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "781a4c3a-af5e-4354-aece-629b1ebe4077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Note: Density column rho already exist in self.time = 0.0.\n",
      "Start: 2024-04-19T03:42:32.457008\n",
      "\tWorking on 2md_00000_xyz...\n",
      "    Debug  :    run_code() ==> <module>() ==> integrate_along_ray_gridxy_ind():\n",
      "\t1182 particles actually participated calculation (0.08% of all particles, average 4.61 per ray.)\n",
      "    Debug  :    main():\n",
      "\tlum = 439115.73539288 solLum\n",
      "\tarea (from <1>) = ( 59.9%) 14.479410853799656 AU2\n",
      "\tarea (from tau) = ( 60.2%) 14.530375774819007 AU2\n",
      "\tsize (from <1>) = 3.8051821052085875 AU\n",
      "\tsize (from tau) = 3.8118730008775223 AU\n",
      "\ttotal possible area = 24.154390898400422 AU2\n",
      "\tlower bound of the # of particles at photosphere, weighted avg over lum per pixels = 1.771053279986438 \n",
      "Ended: 2024-04-19T03:42:39.387508\n",
      "Time Used: 0:00:06.930500\n",
      "\n",
      "Start: 2024-04-19T03:42:39.387529\n",
      "\tWorking on 2md_00000_xzy...\n",
      "    Debug  :    run_code() ==> <module>() ==> integrate_along_ray_gridxy_ind():\n",
      "\t1144 particles actually participated calculation (0.08% of all particles, average 4.46 per ray.)\n",
      "    Debug  :    main():\n",
      "\tlum = 444044.35456836666 solLum\n",
      "\tarea (from <1>) = ( 54.4%) 14.331689361591422 AU2\n",
      "\tarea (from tau) = ( 54.7%) 14.399495671597814 AU2\n",
      "\tsize (from <1>) = 3.785721775512752 AU\n",
      "\tsize (from tau) = 3.794666740518568 AU\n",
      "\ttotal possible area = 26.330506370921718 AU2\n",
      "\tlower bound of the # of particles at photosphere, weighted avg over lum per pixels = 1.8273901359923794 \n",
      "Ended: 2024-04-19T03:42:41.683536\n",
      "Time Used: 0:00:02.296007\n",
      "\n",
      "    Note: Density column rho already exist in self.time = 60000.0.\n",
      "Start: 2024-04-19T03:42:43.497015\n",
      "\tWorking on 2md_01200_xyz...\n",
      "    Debug  :    run_code() ==> <module>() ==> integrate_along_ray_gridxy_ind():\n",
      "\t16286 particles actually participated calculation (1.18% of all particles, average 63.61 per ray.)\n",
      "    Debug  :    main():\n",
      "\tlum = 415294.4469207031 solLum\n",
      "\tarea (from <1>) = ( 43.6%) 4369.119478730748 AU2\n",
      "\tarea (from tau) = ( 43.8%) 4380.5264407892 AU2\n",
      "\tsize (from <1>) = 66.09931526673138 AU\n",
      "\tsize (from tau) = 66.18554555784216 AU\n",
      "\ttotal possible area = 10012.631864661027 AU2\n",
      "\tlower bound of the # of particles at photosphere, weighted avg over lum per pixels = 7.423094804341289 \n",
      "Ended: 2024-04-19T03:42:45.680384\n",
      "Time Used: 0:00:02.183369\n",
      "\n",
      "Start: 2024-04-19T03:42:45.680408\n",
      "\tWorking on 2md_01200_xzy...\n",
      "    Debug  :    run_code() ==> <module>() ==> integrate_along_ray_gridxy_ind():\n",
      "\t13641 particles actually participated calculation (0.99% of all particles, average 53.28 per ray.)\n",
      "    Debug  :    main():\n",
      "\tlum = 119889.38446373936 solLum\n",
      "\tarea (from <1>) = ( 31.9%) 4465.700154480037 AU2\n",
      "\tarea (from tau) = ( 32.0%) 4486.638383234032 AU2\n",
      "\tsize (from <1>) = 66.82589434104146 AU\n",
      "\tsize (from tau) = 66.98237367572182 AU\n",
      "\ttotal possible area = 14007.066172047707 AU2\n",
      "\tlower bound of the # of particles at photosphere, weighted avg over lum per pixels = 5.5104287838753825 \n",
      "Ended: 2024-04-19T03:42:48.162114\n",
      "Time Used: 0:00:02.481706\n",
      "\n",
      "    Note: Density column rho already exist in self.time = 240000.0.\n",
      "Start: 2024-04-19T03:42:49.786826\n",
      "\tWorking on 2md_04800_xyz...\n",
      "    Debug  :    run_code() ==> <module>() ==> integrate_along_ray_gridxy_ind():\n",
      "\t40568 particles actually participated calculation (2.99% of all particles, average 158.46 per ray.)\n",
      "    Debug  :    main():\n",
      "\tlum = 91923.84671973076 solLum\n",
      "\tarea (from <1>) = ( 28.6%) 81194.04708678479 AU2\n",
      "\tarea (from tau) = ( 28.9%) 82132.75302667737 AU2\n",
      "\tsize (from <1>) = 284.9456914690671 AU\n",
      "\tsize (from tau) = 286.58812436435215 AU\n",
      "\ttotal possible area = 284134.92938958656 AU2\n",
      "\tlower bound of the # of particles at photosphere, weighted avg over lum per pixels = 7.524508308337024 \n",
      "Ended: 2024-04-19T03:42:51.965900\n",
      "Time Used: 0:00:02.179074\n",
      "\n",
      "Start: 2024-04-19T03:42:51.965931\n",
      "\tWorking on 2md_04800_xzy...\n",
      "    Debug  :    run_code() ==> <module>() ==> integrate_along_ray_gridxy_ind():\n",
      "\t28380 particles actually participated calculation (2.09% of all particles, average 110.85 per ray.)\n",
      "    Debug  :    main():\n",
      "\tlum = 71728.47639164087 solLum\n",
      "\tarea (from <1>) = ( 18.3%) 70357.45721323628 AU2\n",
      "\tarea (from tau) = ( 18.0%) 69005.28631621355 AU2\n",
      "\tsize (from <1>) = 265.24980153288766 AU\n",
      "\tsize (from tau) = 262.688572869498 AU\n",
      "\ttotal possible area = 384029.41949892766 AU2\n",
      "\tlower bound of the # of particles at photosphere, weighted avg over lum per pixels = 9.196337708286917 \n",
      "Ended: 2024-04-19T03:42:54.261317\n",
      "Time Used: 0:00:02.295386\n",
      "\n",
      "    Note: Density column rho already exist in self.time = 880000.0.\n",
      "Start: 2024-04-19T03:42:56.223711\n",
      "\tWorking on 2md_17600_xyz...\n",
      "    Debug  :    run_code() ==> <module>() ==> integrate_along_ray_gridxy_ind():\n",
      "\t49680 particles actually participated calculation (3.67% of all particles, average 194.06 per ray.)\n",
      "    Debug  :    main():\n",
      "\tlum = 162741.92558928943 solLum\n",
      "\tarea (from <1>) = ( 19.9%) 943963.9138848494 AU2\n",
      "\tarea (from tau) = ( 19.9%) 945204.1510527966 AU2\n",
      "\tsize (from <1>) = 971.5780534186893 AU\n",
      "\tsize (from tau) = 972.2161030618638 AU\n",
      "\ttotal possible area = 4744554.16999051 AU2\n",
      "\tlower bound of the # of particles at photosphere, weighted avg over lum per pixels = 23.537435490974318 \n",
      "Ended: 2024-04-19T03:42:59.057442\n",
      "Time Used: 0:00:02.833731\n",
      "\n",
      "Start: 2024-04-19T03:42:59.057470\n",
      "\tWorking on 2md_17600_xzy...\n",
      "    Debug  :    run_code() ==> <module>() ==> integrate_along_ray_gridxy_ind():\n",
      "\t44601 particles actually participated calculation (3.29% of all particles, average 174.22 per ray.)\n",
      "    Debug  :    main():\n",
      "\tlum = 136045.78372428837 solLum\n",
      "\tarea (from <1>) = ( 17.1%) 932414.0081736317 AU2\n",
      "\tarea (from tau) = ( 16.8%) 918021.6033221266 AU2\n",
      "\tsize (from <1>) = 965.6158698849308 AU\n",
      "\tsize (from tau) = 958.1344390648561 AU\n",
      "\ttotal possible area = 5465430.940708475 AU2\n",
      "\tlower bound of the # of particles at photosphere, weighted avg over lum per pixels = 25.790618490689635 \n",
      "Ended: 2024-04-19T03:43:01.672560\n",
      "Time Used: 0:00:02.615090\n",
      "\n",
      "*   Note   :    run_ast_nodes() ==> run_code() ==> <module>():\n",
      "\tFig saved to ../fig/20240222_LCGen/16x16/test_LC_2md_16x16.pdf.\n",
      "*   Note   :    run_ast_nodes() ==> run_code() ==> <module>():\n",
      "\tFig saved to ../fig/20240222_LCGen/16x16/test_LC_2md_16x16.png.\n",
      "    Note: Density column rho already exist in self.time = 0.0.\n",
      "Start: 2024-04-19T03:43:04.598524\n",
      "\tWorking on 4md_00000_xyz...\n",
      "    Debug  :    run_code() ==> <module>() ==> integrate_along_ray_gridxy_ind():\n",
      "\t324 particles actually participated calculation (0.02% of all particles, average 1.26 per ray.)\n",
      "    Debug  :    main():\n",
      "\tlum = 1278861.623769637 solLum\n",
      "\tarea (from <1>) = ( 69.5%) 15.865730547567795 AU2\n",
      "\tarea (from tau) = ( 69.5%) 15.862853207496547 AU2\n",
      "\tsize (from <1>) = 3.983180958426041 AU\n",
      "\tsize (from tau) = 3.9828197558383867 AU\n",
      "\ttotal possible area = 22.813991129882673 AU2\n",
      "\tlower bound of the # of particles at photosphere, weighted avg over lum per pixels = 1.1271777491886839 \n",
      "Ended: 2024-04-19T03:43:06.929640\n",
      "Time Used: 0:00:02.331116\n",
      "\n",
      "Start: 2024-04-19T03:43:06.929690\n",
      "\tWorking on 4md_00000_xzy...\n",
      "    Debug  :    run_code() ==> <module>() ==> integrate_along_ray_gridxy_ind():\n",
      "\t276 particles actually participated calculation (0.02% of all particles, average 1.07 per ray.)\n",
      "    Debug  :    main():\n",
      "\tlum = 1254373.710798105 solLum\n",
      "\tarea (from <1>) = ( 68.8%) 15.695659812351554 AU2\n",
      "\tarea (from tau) = ( 69.1%) 15.76781572509952 AU2\n",
      "\tsize (from <1>) = 3.961774831101782 AU\n",
      "\tsize (from tau) = 3.970870902598008 AU\n",
      "\ttotal possible area = 22.805428393364274 AU2\n",
      "\tlower bound of the # of particles at photosphere, weighted avg over lum per pixels = 1.1247877214052406 \n",
      "Ended: 2024-04-19T03:43:09.698432\n",
      "Time Used: 0:00:02.768742\n",
      "\n",
      "    Note: Density column rho already exist in self.time = 60000.0.\n",
      "Start: 2024-04-19T03:43:11.788450\n",
      "\tWorking on 4md_01200_xyz...\n",
      "    Debug  :    run_code() ==> <module>() ==> integrate_along_ray_gridxy_ind():\n",
      "\t2872 particles actually participated calculation (0.2% of all particles, average 11.21 per ray.)\n",
      "    Debug  :    main():\n",
      "\tlum = 4289339.055022065 solLum\n",
      "\tarea (from <1>) = ( 13.8%) 2332.8265784415316 AU2\n",
      "\tarea (from tau) = ( 12.5%) 2109.623860493768 AU2\n",
      "\tsize (from <1>) = 48.299343457665465 AU\n",
      "\tsize (from tau) = 45.930641847178315 AU\n",
      "\ttotal possible area = 16876.990883950144 AU2\n",
      "\tlower bound of the # of particles at photosphere, weighted avg over lum per pixels = 3.3203241184731893 \n",
      "Ended: 2024-04-19T03:43:14.730875\n",
      "Time Used: 0:00:02.942425\n",
      "\n",
      "Start: 2024-04-19T03:43:14.730927\n",
      "\tWorking on 4md_01200_xzy...\n",
      "    Debug  :    run_code() ==> <module>() ==> integrate_along_ray_gridxy_ind():\n",
      "\t3402 particles actually participated calculation (0.24% of all particles, average 13.28 per ray.)\n",
      "    Debug  :    main():\n",
      "\tlum = 1086805.7247528106 solLum\n",
      "\tarea (from <1>) = ( 18.6%) 2021.5428310781508 AU2\n",
      "\tarea (from tau) = ( 17.6%) 1905.5430684856904 AU2\n",
      "\tsize (from <1>) = 44.961570602884315 AU\n",
      "\tsize (from tau) = 43.65252648456548 AU\n",
      "\ttotal possible area = 10840.42278960748 AU2\n",
      "\tlower bound of the # of particles at photosphere, weighted avg over lum per pixels = 7.775293923145811 \n",
      "Ended: 2024-04-19T03:43:17.639175\n",
      "Time Used: 0:00:02.908248\n",
      "\n",
      "    Note: Density column rho already exist in self.time = 240000.0.\n",
      "Start: 2024-04-19T03:43:19.716640\n",
      "\tWorking on 4md_04800_xyz...\n",
      "    Debug  :    run_code() ==> <module>() ==> integrate_along_ray_gridxy_ind():\n",
      "\t17969 particles actually participated calculation (1.3% of all particles, average 70.19 per ray.)\n",
      "    Debug  :    main():\n",
      "\tlum = 289182.18977401714 solLum\n",
      "\tarea (from <1>) = ( 17.0%) 94729.2848051899 AU2\n",
      "\tarea (from tau) = ( 16.4%) 91591.07391501154 AU2\n",
      "\tsize (from <1>) = 307.78122880577024 AU\n",
      "\tsize (from tau) = 302.64017234169614 AU\n",
      "\ttotal possible area = 558269.4029105465 AU2\n",
      "\tlower bound of the # of particles at photosphere, weighted avg over lum per pixels = 4.870942518272718 \n",
      "Ended: 2024-04-19T03:43:22.778983\n",
      "Time Used: 0:00:03.062343\n",
      "\n",
      "Start: 2024-04-19T03:43:22.779018\n",
      "\tWorking on 4md_04800_xzy...\n",
      "    Debug  :    run_code() ==> <module>() ==> integrate_along_ray_gridxy_ind():\n",
      "\t20573 particles actually participated calculation (1.49% of all particles, average 80.36 per ray.)\n",
      "    Debug  :    main():\n",
      "\tlum = 224398.4201705077 solLum\n",
      "\tarea (from <1>) = ( 18.0%) 84458.29545489527 AU2\n",
      "\tarea (from tau) = ( 17.6%) 82499.14720989199 AU2\n",
      "\tsize (from <1>) = 290.6170942234735 AU\n",
      "\tsize (from tau) = 287.22664780603486 AU\n",
      "\ttotal possible area = 469328.4819051635 AU2\n",
      "\tlower bound of the # of particles at photosphere, weighted avg over lum per pixels = 5.147694371458111 \n",
      "Ended: 2024-04-19T03:43:25.523286\n",
      "Time Used: 0:00:02.744268\n",
      "\n",
      "    Note: Density column rho already exist in self.time = 880000.0.\n",
      "Start: 2024-04-19T03:43:27.462344\n",
      "\tWorking on 4md_17600_xyz...\n",
      "    Debug  :    run_code() ==> <module>() ==> integrate_along_ray_gridxy_ind():\n",
      "\t26500 particles actually participated calculation (1.93% of all particles, average 103.51 per ray.)\n",
      "    Debug  :    main():\n",
      "\tlum = 247368.86817041642 solLum\n",
      "\tarea (from <1>) = ( 13.7%) 1318778.209481543 AU2\n",
      "\tarea (from tau) = ( 13.3%) 1278443.6863982785 AU2\n",
      "\tsize (from <1>) = 1148.3806901378753 AU\n",
      "\tsize (from tau) = 1130.6828407640573 AU\n",
      "\ttotal possible area = 9625928.932881158 AU2\n",
      "\tlower bound of the # of particles at photosphere, weighted avg over lum per pixels = 19.016005872462248 \n",
      "Ended: 2024-04-19T03:43:30.265972\n",
      "Time Used: 0:00:02.803628\n",
      "\n",
      "Start: 2024-04-19T03:43:30.266005\n",
      "\tWorking on 4md_17600_xzy...\n",
      "    Debug  :    run_code() ==> <module>() ==> integrate_along_ray_gridxy_ind():\n",
      "\t33452 particles actually participated calculation (2.43% of all particles, average 130.67 per ray.)\n",
      "    Debug  :    main():\n",
      "\tlum = 272910.98471990373 solLum\n",
      "\tarea (from <1>) = ( 17.8%) 1358497.4848742313 AU2\n",
      "\tarea (from tau) = ( 17.2%) 1310595.6538386706 AU2\n",
      "\tsize (from <1>) = 1165.5460028991697 AU\n",
      "\tsize (from tau) = 1144.8124972407797 AU\n",
      "\ttotal possible area = 7625283.804152266 AU2\n",
      "\tlower bound of the # of particles at photosphere, weighted avg over lum per pixels = 16.368965437138634 \n",
      "Ended: 2024-04-19T03:43:33.728424\n",
      "Time Used: 0:00:03.462419\n",
      "\n",
      "*   Note   :    run_ast_nodes() ==> run_code() ==> <module>():\n",
      "\tFig saved to ../fig/20240222_LCGen/16x16/test_LC_4md_16x16.pdf.\n",
      "*   Note   :    run_ast_nodes() ==> run_code() ==> <module>():\n",
      "\tFig saved to ../fig/20240222_LCGen/16x16/test_LC_4md_16x16.png.\n",
      "*   Note   :    run_code() ==> <module>() ==> hdf5_dump():\n",
      "\tWriting to ../interm/test_lcgen.16x16.hdf5 (will OVERWRITE if file already exist.)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    # init combined data\n",
    "    comb = {}\n",
    "    \n",
    "    for job_nickname in job_nicknames: #['2md', ]:\n",
    "        job_profile = JOB_PROFILES_DICT[job_nickname]\n",
    "        job_name    = job_profile['job_name']\n",
    "        file_indexes= job_profile['file_indexes']\n",
    "        params      = job_profile['params']\n",
    "        eos_opacity = get_eos_opacity(ieos=10, params=params)    #EoS_MESA_opacity(params, settings)\n",
    "        \n",
    "        comb[job_nickname] = {\n",
    "            xyzs: {\n",
    "                'times': np.full(len(file_indexes), np.nan) * units.yr,\n",
    "                'lums' : np.full(len(file_indexes), np.nan) * units.Lsun,\n",
    "                'areas': np.full(len(file_indexes), np.nan) * units.au**2,\n",
    "                # no of particles at the photosphere - lower bound (weighted average per pixel, weighted by lums contribution)\n",
    "                # i.e. how resolved the photosphere is\n",
    "                'npt_at_ph_lb': np.full(len(file_indexes), -1) * units.dimensionless_unscaled,\n",
    "                '_meta_': {\n",
    "                    'lums' : { 'Description': \"Luminosity.\", },\n",
    "                    'areas': { 'Description': (\n",
    "                        \"Visible size of the simulated object.\" +\n",
    "                        \"(i.e. pixel * (area per pixel) * (tau if tau<1 else 1)\"), },\n",
    "                    'npt_at_ph_lb': { 'Description': (\n",
    "                            \"no of particles at the photosphere - lower bound\" +\n",
    "                            \"(weighted average per pixel, weighted by lums contribution per pixel)\"), },\n",
    "                },\n",
    "            } for xyzs in xyzs_list\n",
    "        }\n",
    "\n",
    "            \n",
    "        for ifile, file_index in enumerate(file_indexes):\n",
    "            # init\n",
    "    \n",
    "            mpdf = mpdf_read(job_name, file_index, eos_opacity, reset_xyz_by='R1', verbose=1)\n",
    "            mpdf.calc_sdf_params(['R1'])\n",
    "            sdf  = mpdf.data['gas']\n",
    "            srcfuncs = mpdf.const['sigma_sb'] * sdf['T']**4 / pi\n",
    "            sdf['srcfunc'] = srcfuncs\n",
    "\n",
    "            with mupl.hdf5_open(f\"{interm_dir}{job_nickname}_{file_index:05d}.lcgen.{no_xy_txt}.hdf5\", 'a', metadata) as out_interm_grp1:\n",
    "                #out_interm_grp1 = mupl.hdf5_subgroup(out_interm_file, f\"{file_index:05d}\", {})\n",
    "\n",
    "                for xyzs in xyzs_list:\n",
    "                    xyzs_names_list = [x for x in xyzs]\n",
    "        \n",
    "                    \n",
    "                    # record time used\n",
    "                    python_time_start = datetime.utcnow()\n",
    "                    print(f\"Start: {python_time_start.isoformat()}\")\n",
    "                    print(f\"\\tWorking on {job_nickname}_{file_index:05d}_{xyzs}...\")\n",
    "        \n",
    "                    \n",
    "                    # get rays\n",
    "                    rays, areas, dXs = get_xy_grids_of_rays(\n",
    "                        sdf, no_xy=no_xy, frac_contained=100., use_adaptive_grid=False, xyzs_names_list=xyzs_names_list)\n",
    "                    pts    = np.array(sdf[xyzs_names_list])\n",
    "                    hs     = np.array(sdf[ 'h' ])    # npart-shaped array\n",
    "                    kernel = sdf.kernel\n",
    "                    kernel_rad = float(kernel.get_radius())\n",
    "                    col_kernel = kernel.get_column_kernel_func(samples=1000)\n",
    "                    \n",
    "                    rays_u = (rays * mpdf.units['dist']).to(units.au)\n",
    "                    areas_u = (areas * mpdf.units['dist']**2).to(units.au**2)\n",
    "        \n",
    "                    \n",
    "                    # do integration without error estimation\n",
    "                    ans   = integrate_along_ray_gridxy_ind(\n",
    "                        sdf, srcfuncs, rays, xyzs_names_list=xyzs_names_list, parallel=True, verbose=verbose)\n",
    "                    rads, areas_p, taus, inds, contr, pts_order_used = ans\n",
    "                    rads  = (rads * mpdf.units['sigma_sb'] * mpdf.units['temp']**4 / units.sr).cgs\n",
    "                    inds *= units.dimensionless_unscaled\n",
    "                    contr = 100 * contr * units.percent\n",
    "                    npt_at_ph_lb = np.average(\n",
    "                        np.where(np.isnan(contr), 0., 1. / contr),\n",
    "                        weights=(rads * areas_u).value,\n",
    "                    ).to(units.dimensionless_unscaled)\n",
    "                    lum   = ((4 * pi * units.sr) * (rads * areas_u)).sum().to(units.solLum)\n",
    "                    area  = (areas_p * areas_u).sum()\n",
    "                    area_2= (np.where(\n",
    "                        np.isnan(taus),\n",
    "                        1.,\n",
    "                        np.where(\n",
    "                            taus > 0.636, #PHOTOSPHERE_TAU,\n",
    "                            1.0,\n",
    "                            0.0,\n",
    "                        )) * areas_u).sum()\n",
    "                    #anses_fft = fft.fft2(rads.reshape(no_xy).value)\n",
    "    \n",
    "                    if is_verbose(verbose, 'info'):\n",
    "                        say('info', 'main()', verbose,\n",
    "                            f\"lum = {lum}\",\n",
    "                            f\"area (from <1>) = ({area  /areas_u.sum()*100: 5.1f}%) {area}\",\n",
    "                            f\"area (from tau) = ({area_2/areas_u.sum()*100: 5.1f}%) {area_2}\",\n",
    "                            f\"size (from <1>) = {area**0.5}\",\n",
    "                            f\"size (from tau) = {area_2**0.5}\",\n",
    "                            f\"total possible area = {areas_u.sum()}\",\n",
    "                            f\"lower bound of the # of particles at photosphere, weighted avg over lum per pixels = {npt_at_ph_lb} \",\n",
    "                        )\n",
    "                \n",
    "                    # save interm data\n",
    "                    data = {}\n",
    "                    data['lum'  ] = lum\n",
    "                    data['area_one'] = area\n",
    "                    data['area_tau'] = area_2\n",
    "                    data['npt_at_ph_lb'] = npt_at_ph_lb\n",
    "                    data['xyzs' ] = xyzs\n",
    "                    data['time' ] = mpdf.get_time()\n",
    "                    data['mpdf_params'] = mpdf.params\n",
    "                    data['rays' ] = rays_u[:, 0, :2]\n",
    "                    data['ray_unit_vec'] = get_ray_unit_vec(rays_u[0].value)\n",
    "                    data['area_per_ray'] = areas_u[0] #areas_u\n",
    "                    data['rads' ] = rads\n",
    "                    data['contr'] = contr\n",
    "                    \n",
    "                    data['_meta_'] = {\n",
    "                        'npt_at_ph_lb': comb[job_nickname][xyzs]['_meta_']['npt_at_ph_lb'],\n",
    "                        'rays' : { 'Description': \"Pixel centers on the 2D plane defined by xyzs.\", },\n",
    "                        'rads' : { 'Description': \"Specific intensity per pixel.\", },\n",
    "                        'contr': {\n",
    "                            'Description': \"Maximum contributed particle's contribution towards the specific intensity, per pixel.\", },\n",
    "                    }\n",
    "    \n",
    "                    \n",
    "                    mupl.hdf5_dump(data, mupl.hdf5_subgroup(out_interm_grp1, xyzs, overwrite=True), {})\n",
    "        \n",
    "                    comb[job_nickname][xyzs]['times'][ifile] = data['time']\n",
    "                    comb[job_nickname][xyzs]['lums' ][ifile] = data['lum' ]\n",
    "                    comb[job_nickname][xyzs]['areas'][ifile] = data['area_one']\n",
    "                    comb[job_nickname][xyzs]['npt_at_ph_lb'][ifile] = data['npt_at_ph_lb']\n",
    "        \n",
    "                    \n",
    "                    # plotting\n",
    "                    if False:\n",
    "                        plt.close('all')\n",
    "                        fig, ax, outfilenames = plot_imshow(\n",
    "                            no_xy, rays_u, rads, data_label=\"$I$\",\n",
    "                            xyzs=xyzs, save_label=f\"I\",\n",
    "                            job_profile=job_profile, file_index=file_index, notes=data,\n",
    "                            output_dir=output_dir, verbose=verbose_loop)\n",
    "                        fig, ax, outfilenames = plot_imshow(\n",
    "                            no_xy, rays_u, inds%20, data_label=\"index % 20 of the most contributed\",\n",
    "                            xyzs=xyzs, save_label=f\"dinds\",\n",
    "                            job_profile=job_profile, file_index=file_index, cmap='turbo', notes=data,\n",
    "                            output_dir=output_dir, verbose=verbose_loop)\n",
    "                        fig, ax, outfilenames = plot_imshow(\n",
    "                            no_xy, rays_u, contr, data_label=\"contribution fraction of the most contributed\",\n",
    "                            xyzs=xyzs, save_label=f\"contr\",\n",
    "                            job_profile=job_profile, file_index=file_index, cmap='seismic', notes=data,\n",
    "                            output_dir=output_dir, verbose=verbose_loop)\n",
    "                        #fig, ax, outfilenames = plot_imshow(\n",
    "                        #    no_xy, rays_u, np.abs(anses_fft), data_label=\"FFt of $I$\", xyzs=xyzs, save_label=f\"I-fft\",\n",
    "                        #    norm=mpl.colors.LogNorm(),\n",
    "                        #    job_profile=job_profile, file_index=file_index, notes=data, output_dir=output_dir, verbose=verbose_loop)\n",
    "        \n",
    "        \n",
    "                    # record time used\n",
    "                    python_time_ended = datetime.utcnow()\n",
    "                    python_time__used  = python_time_ended - python_time_start\n",
    "                    print(f\"Ended: {python_time_ended.isoformat()}\\nTime Used: {python_time__used}\\n\")\n",
    "\n",
    "        \n",
    "        # save data for now\n",
    "        with open(f\"{interm_dir}lcgen.{no_xy_txt}.json\", 'w') as f:\n",
    "            mupl.json_dump(comb, f, metadata)\n",
    "\n",
    "        \n",
    "        #plotting\n",
    "        plt.close('all')\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        for xyzs in xyzs_list:\n",
    "            ax.semilogy(\n",
    "                comb[job_nickname][xyzs]['times'].to_value(units.yr), comb[job_nickname][xyzs]['lums'].to_value(units.Lsun),\n",
    "                'o--', label=f\"Viewed from +{xyzs[2]}\")\n",
    "        ax.legend()\n",
    "        ax.set_xlabel('Time / yr')\n",
    "        ax.set_ylabel('Luminosity / Lsun')\n",
    "        ax.set_xlim(0., 45.)\n",
    "        ax.set_ylim(1e4, 5e6)\n",
    "        outfilename_noext = f\"{output_dir}LC_{job_nickname}_{no_xy_txt}\"\n",
    "        \n",
    "        # write pdf\n",
    "        outfilename = f\"{outfilename_noext}.pdf\"\n",
    "        fig.savefig(outfilename)\n",
    "        if is_verbose(verbose, 'note'):\n",
    "            say('note', None, verbose, f\"Fig saved to {outfilename}.\")\n",
    "        \n",
    "        # write png (with plot title)\n",
    "        ax.set_title(f\"Light curve ({job_nickname}, {no_xy_txt} rays)\")\n",
    "        outfilename = f\"{outfilename_noext}.png\"\n",
    "        fig.savefig(outfilename)\n",
    "        if is_verbose(verbose, 'note'):\n",
    "            say('note', None, verbose, f\"Fig saved to {outfilename}.\")\n",
    "                \n",
    "    \n",
    "    plt.close('all')\n",
    "    mupl.hdf5_dump(comb, f\"{interm_dir}lcgen.{no_xy_txt}.hdf5.gz\", metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dfe709-c9a0-4008-a213-3b98d31b3c37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
