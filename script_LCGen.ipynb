{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa711f22-c313-4343-a584-e3efdecd3ad4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Scripts for analyzing of phantom outputs.\\n\\nThis script generate lightcurves (LC) by doing radiative transfer on a grid.\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Scripts for analyzing of phantom outputs.\n",
    "\n",
    "This script generate lightcurves (LC) by doing radiative transfer on a grid.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b19be2-030c-4fa9-ade5-f4bb38a0c13a",
   "metadata": {},
   "source": [
    ".\n",
    "\n",
    "# Def\n",
    "\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035afbe0-6134-4121-8416-f0dbbab169fb",
   "metadata": {},
   "source": [
    "## Imports & Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "093b4271-dab0-4b64-8b31-1588fc264966",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import pi\n",
    "from astropy import units\n",
    "from astropy import constants as const\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from numba import jit\n",
    "import sarracen\n",
    "import itertools\n",
    "from scipy import integrate, fft\n",
    "from scipy.spatial import kdtree\n",
    "from datetime import datetime\n",
    "#from moviepy.editor import ImageSequenceClip\n",
    "#from os import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c75c48c-a738-4783-a6af-560603d629a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import my modules listed in ./main/\n",
    "\n",
    "import clmuphantomlib as mupl\n",
    "from clmuphantomlib            import MyPhantomDataFrames, get_eos\n",
    "from clmuphantomlib.log        import is_verbose, say\n",
    "from clmuphantomlib.settings   import DEFAULT_SETTINGS as settings\n",
    "from clmuphantomlib.units_util import get_val_in_unit, set_as_quantity #, get_units_field_name, get_units_cgs\n",
    "from clmuphantomlib.readwrite  import json_dump, json_load\n",
    "from clmuphantomlib.eos_mesa   import EoS_MESA_opacity\n",
    "from clmuphantomlib.light      import get_optical_depth_by_ray_tracing_3D, get_photosphere_on_ray\n",
    "\n",
    "from multiprocessing import cpu_count, Pool #Process, Queue\n",
    "NPROCESSES = 1 if cpu_count() is None else max(cpu_count(), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "859f4f11-9307-4f36-b896-3778f252d65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*   Note   :    script:\n",
      "\tWill use 8 processes for parallelization\n"
     ]
    }
   ],
   "source": [
    "# settings\n",
    "#\n",
    "#   imported from script_input.py file\n",
    "\n",
    "\n",
    "from script_LCGen__input import verbose, interm_dir, output_dir, unitsOut, PHOTOSPHERE_TAU, JOB_PROFILES_DICT\n",
    "from script_LCGen__input import job_nicknames, xyzs_list, no_xy, no_xy_txt, verbose_loop\n",
    "from _sharedFuncs import mpdf_read\n",
    "\n",
    "unitsOutTxt = {  key  : unitsOut[key].to_string('latex_inline') for key in unitsOut.keys() }\n",
    "\n",
    "\n",
    "# set metadata\n",
    "with open(\"_metadata__input.json\", 'r') as f:\n",
    "    metadata = json_load(f)\n",
    "metadata['Title'] = \"Getting light curves by intergrating across a grid of rays\"\n",
    "metadata['Description'] = f\"\"\"Getting light curves by intergrating across a grid of rays with the same directions\n",
    "for dump file data generated by phantom\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "if __name__ == '__main__' and is_verbose(verbose, 'note'):\n",
    "    # remember to check if name is '__main__' if you wanna say anything\n",
    "    #    so when you do multiprocessing the program doesn't freak out\n",
    "    say('note', \"script\", verbose, f\"Will use {NPROCESSES} processes for parallelization\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4853c017-d945-4a40-9098-74dd24e60180",
   "metadata": {},
   "outputs": [],
   "source": [
    "from clmuphantomlib.log import say, is_verbose\n",
    "from clmuphantomlib.geometry import get_dist2_between_2pt, get_closest_pt_on_line\n",
    "from clmuphantomlib.sph_interp import get_sph_interp, get_h_from_rho, get_no_neigh, _get_sph_interp_phantom_np\n",
    "from clmuphantomlib.units_util import set_as_quantity, set_as_quantity_temperature, get_units_field_name\n",
    "from clmuphantomlib.eos_base import EoS_Base\n",
    "from clmuphantomlib.light import integrate_along_ray_grid, integrate_along_ray_gridxy\n",
    "\n",
    "#  import (general)\n",
    "import numpy as np\n",
    "import numba\n",
    "from numba import jit, prange\n",
    "import sarracen\n",
    "\n",
    "from clmuphantomlib.geometry import get_dist2_from_pts_to_line, get_dist2_from_pt_to_line_nb, get_ray_unit_vec, get_rays_unit_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645cc989-5868-4956-9009-e3fd293c8736",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fba0ff6-57ab-4dc7-88d8-874c885a5e45",
   "metadata": {},
   "source": [
    "### LC integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3e20ab-9650-46fc-898f-b676d35540b8",
   "metadata": {},
   "source": [
    "#### Backup codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8551e10-e248-4947-bd7a-1671298482db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_along_ray(\n",
    "    sdf, ray, srcfuncs, photosphere_tau=1.,\n",
    "    verbose: int = 3,\n",
    "):\n",
    "    pts_on_ray, dtaus, pts_order = get_optical_depth_by_ray_tracing_3D(sdf=sdf, ray=ray)\n",
    "    if is_verbose(verbose, 'debug'):\n",
    "        say('debug', None, verbose, 'optical depth got.')\n",
    "\n",
    "    dtaus_ordered = dtaus[pts_order]\n",
    "    if is_verbose(verbose, 'debug'):\n",
    "        say('debug', None, verbose, 'ordered.')\n",
    "    srcfuncs_ordered = srcfuncs[pts_order]\n",
    "    if is_verbose(verbose, 'debug'):\n",
    "        say('debug', None, verbose, 'srcfuncs_ordered.')\n",
    "    dat_steps = np.full_like(dtaus_ordered, np.nan)\n",
    "\n",
    "    if True:\n",
    "    #if backwards:\n",
    "        # closest to observer to furtherest\n",
    "        dat = 0.\n",
    "        taus_ordered = np.array([0., *np.cumsum(dtaus_ordered[:-1])])\n",
    "        # dat_bwd_inc: dat_backward_contributions\n",
    "        dat_bwd_inc = np.exp(-taus_ordered) * (1 - np.exp(-dtaus_ordered)) * srcfuncs_ordered\n",
    "        dat = np.sum(dat_bwd_inc)\n",
    "        if is_verbose(verbose, 'info'):\n",
    "            say('info', None, verbose,\n",
    "                f\"backward dat = {dat}\")    # debug\n",
    "        if False:\n",
    "            # commented\n",
    "            # get the percentage of contribution to lum from photosphere outwards\n",
    "            photosphere_loc_index = np.searchsorted(taus_ordered, photosphere_tau) - 1\n",
    "            photosphere_contri_percent = np.sum(dat_bwd_inc[:photosphere_loc_index+2]) / dat * 100\n",
    "            dat_percent_index = np.where(np.cumsum(dat_bwd_inc) / dat<0.5)[0][-1]\n",
    "            if is_verbose(verbose, 'info'):\n",
    "                say('info', None, verbose,\n",
    "                    f\"\\tContribution to L from photosphere and outwards is: {photosphere_contri_percent} %\",\n",
    "                    f\"\\t50% Contributed correspond to tau = {taus_ordered[dat_percent_index]} \")\n",
    "        taus_ordered = taus_ordered[::-1]\n",
    "\n",
    "    else:\n",
    "        # furtherest to observer to closest\n",
    "        dat = 0.\n",
    "        #  pts_order[::-1]: reverse pts_order so that the furtherest particles comes first\n",
    "        taus_ordered = np.array([0., *np.cumsum(dtaus_ordered[:-1])])[::-1]\n",
    "        exp_mdtaus_r = np.exp(-dtaus_ordered[::-1])\n",
    "        srcfuncs_ordered_r = srcfuncs_ordered[::-1]\n",
    "        for index, srcfunc in enumerate(srcfuncs_ordered_r):\n",
    "            exp_mdtau = exp_mdtaus_r[index]\n",
    "            dat = exp_mdtau * dat + (1-exp_mdtau) * srcfunc\n",
    "            dat_steps[index] = dat\n",
    "        if is_verbose(verbose, 'info'):\n",
    "            say('info', None, verbose,\n",
    "                f\"forward dat = {dat}\")    # debug\n",
    "        \n",
    "    dtaus_ordered = dtaus_ordered[::-1]\n",
    "    pts_order = pts_order[::-1]  # furtherest to observer to closest\n",
    "    pts_on_ray_ordered = pts_on_ray[pts_order]\n",
    "    \n",
    "    \n",
    "    return  pts_order, pts_on_ray, dtaus_ordered, taus_ordered, \\\n",
    "            dat, dat_steps, dat_bwd_inc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc0e7a4e-ec6f-42a0-b0df-9786ca451f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_error_along_ray(\n",
    "    dtaus_ordered, # furtherest to closest\n",
    "    srcfuncs_ordered, srcfuncs_ordered_errp, srcfuncs_ordered_errm,\n",
    "    photosphere_tau=1.,\n",
    "):\n",
    "    #if backwards:\n",
    "    if True:\n",
    "        \n",
    "        # closest to observer to furtherest\n",
    "        dtaus_ordered = dtaus_ordered[::-1]\n",
    "        \n",
    "        # calc data + error\n",
    "        taus_ordered = np.array([0., *np.cumsum(dtaus_ordered[:-1])])\n",
    "        srcfuncs_errs = np.stack([srcfuncs_ordered, srcfuncs_ordered_errp, srcfuncs_ordered_errm])\n",
    "        srcfuncs_errs = np.flip(srcfuncs_errs, axis=-1) # flip it since we are working backwards\n",
    "        dat_bwd_inc_errs = np.exp(-taus_ordered) * (1 - np.exp(-dtaus_ordered)) * srcfuncs_errs\n",
    "        dat_errs = np.sum(dat_bwd_inc_errs, axis=-1)\n",
    "        \n",
    "        dat_bwd_inc_errs = np.flip(dat_bwd_inc_errs, axis=-1)\n",
    "        \n",
    "        if False:\n",
    "            # get data\n",
    "            dat_bwd_inc = dat_bwd_inc_errs[0]\n",
    "            dat = dat_errs[0]\n",
    "            dat_errp = dat_errs[1]\n",
    "            dat_errm = dat_errs[2]\n",
    "        \n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "        # furtherest to observer to closest\n",
    "        dat = 0.\n",
    "        dat_steps = np.full_like(dtaus_ordered, np.nan)\n",
    "        #  pts_order[::-1]: reverse pts_order so that the furtherest particles comes first\n",
    "        taus_ordered = np.array([0., *np.cumsum(dtaus_ordered[:-1])])[::-1]\n",
    "        exp_mdtaus_r = np.exp(-dtaus_ordered[::-1])\n",
    "        srcfuncs_ordered_r = srcfuncs_ordered[::-1]\n",
    "        for index, srcfunc in enumerate(srcfuncs_ordered_r):\n",
    "            exp_mdtau = exp_mdtaus_r[index]\n",
    "            dat = exp_mdtau * dat + (1-exp_mdtau) * srcfunc\n",
    "            dat_steps[index] = dat\n",
    "        print(\"forward dat = \", dat)    # debug\n",
    "        \n",
    "    #dtaus_ordered = dtaus_ordered[::-1]\n",
    "    return dat_errs, dat_bwd_inc_errs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394b11f2-040f-484e-9ed4-6132700848e1",
   "metadata": {},
   "source": [
    "#### Test codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2dc4b419-b471-4c9b-8405-2faf75fae819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test runs\n",
    "@jit(nopython=True, parallel=True)\n",
    "def _integrate_along_ray_gridxy_sub_parallel_analysis_test(\n",
    "    pts_ordered          : np.ndarray,    # (npart, 3)-shaped\n",
    "    hs_ordered           : np.ndarray,    # (npart,  )-shaped\n",
    "    mkappa_div_h2_ordered: np.ndarray,    # (npart,  )-shaped\n",
    "    srcfuncs_ordered     : np.ndarray,    # (npart,  )-shaped\n",
    "    rays                 : np.ndarray,    # (nray, 2, 3)-shaped\n",
    "    kernel_rad           : float,\n",
    "    col_kernel           : numba.core.registry.CPUDispatcher,\n",
    "    pts_order            : np.ndarray,    # (npart,  )-shaped\n",
    "    rel_tol              : float = 1e-15, # because float64 is only has only 16 digits accuracy\n",
    ") -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Sub process for integrate_along_ray_gridxy(). Numba parallel version (using prange).\n",
    "\n",
    "    Unit vec must be [0., 0., 1.] (i.e. all rays must point upwards towards +z).\n",
    "\n",
    "    Private function. Assumes specific input type. See source code comments.\n",
    "\n",
    "    \"\"\"\n",
    "    #raise NotImplementedError\n",
    "\n",
    "    nray  = len(rays)\n",
    "    npart = len(srcfuncs_ordered)\n",
    "    ndim  = pts_ordered.shape[-1]\n",
    "    anses = np.zeros(nray)\n",
    "    indes = np.zeros(nray, dtype=np.int64)    # indexes of max contribution particle\n",
    "    contr = np.zeros(nray)    # contribution of the max contribution particle\n",
    "    jused = np.full(npart, False)    # is j-th particle in the ordered list used for this calculation?\n",
    "\n",
    "    # error tolerance of tau (part 1)\n",
    "    tol_tau_base = np.log(srcfuncs_ordered.sum()) - np.log(rel_tol)\n",
    "\n",
    "    # hr = h * kernel_rad\n",
    "    hrs_ordered = hs_ordered * kernel_rad\n",
    "\n",
    "    # loop over ray\n",
    "    for i in prange(nray):\n",
    "        ray = rays[i]\n",
    "        tau = 0.\n",
    "        ans = 0.\n",
    "        dans= 0.\n",
    "        dans_max_tmp = 0.\n",
    "        ind = -1\n",
    "\n",
    "        #   xy-grid specific solution\n",
    "        ray_x = ray[0, 0]\n",
    "        ray_y = ray[0, 1]\n",
    "        \n",
    "        # loop over particles\n",
    "        #for pt, hr, mkappa_div_h2, srcfunc in zip(\n",
    "        #    pts_ordered, hrs_ordered, mkappa_div_h2_ordered, srcfuncs_ordered):\n",
    "        for j in range(npart):\n",
    "            pt = pts_ordered[j]\n",
    "            hr = hrs_ordered[j]\n",
    "            \n",
    "            # check if the particle is within range\n",
    "            #   general solution\n",
    "            #q = get_dist2_from_pt_to_line_nb(pt, ray)**0.5 / h\n",
    "            #if q < kernel_rad:\n",
    "            #   xy-grid specific solution\n",
    "            if ray_x - hr < pt[0] and pt[0] < ray_x + hr and ray_y - hr < pt[1] and pt[1] < ray_y + hr:\n",
    "                h = hs_ordered[ j]\n",
    "                q = ((pt[0] - ray_x)**2 + (pt[1] - ray_y)**2)**0.5 / h\n",
    "                if q < kernel_rad:\n",
    "\n",
    "                    jused[j] = True\n",
    "                    \n",
    "                    # now do radiative transfer\n",
    "                    \n",
    "                    mkappa_div_h2 = mkappa_div_h2_ordered[j]\n",
    "                    srcfunc = srcfuncs_ordered[j]\n",
    "                    \n",
    "                    dtau = mkappa_div_h2 * col_kernel(q, ndim-1)\n",
    "                    #tau += dtau/2.\n",
    "                    dans = np.exp(-tau) * dtau * srcfunc\n",
    "                    ans += dans\n",
    "                    tau += dtau#/2.\n",
    "\n",
    "                    # note down the largest contributor\n",
    "                    if dans > dans_max_tmp:\n",
    "                        dans_max_tmp = dans\n",
    "                        ind = pts_order[j]\n",
    "    \n",
    "                    # terminate the calc for this ray if tau is sufficient large\n",
    "                    #    such that the relative error on ans is smaller than rel_tol\n",
    "                    # i.e. since when tau > np.log(srcfuncs_ordered.sum()) - np.log(rel_tol) - np.log(ans),\n",
    "                    #    we know that ans[i] - ans[i][k] < rel_tol * ans[i]\n",
    "                    # see my notes for derivation\n",
    "                    if tau > tol_tau_base - np.log(ans):\n",
    "                        break\n",
    "            \n",
    "        anses[i] = ans\n",
    "        indes[i] = ind\n",
    "        contr[i] = dans_max_tmp / ans\n",
    "    \n",
    "    return anses, indes, contr, jused\n",
    "\n",
    "\n",
    "#_integrate_along_ray_gridxy_sub_parallel_analysis = _integrate_along_ray_gridxy_sub_parallel_analysis_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4246b679-eef6-4b44-bbb8-039c050202f9",
   "metadata": {},
   "source": [
    "#### Actual code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30983d75-44d0-4eec-9ae8-efb05e6cc93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True, parallel=True)\n",
    "def _integrate_along_ray_gridxy_sub_parallel_analysis(\n",
    "    pts_ordered          : np.ndarray,    # (npart, 3)-shaped\n",
    "    hs_ordered           : np.ndarray,    # (npart,  )-shaped\n",
    "    mkappa_div_h2_ordered: np.ndarray,    # (npart,  )-shaped\n",
    "    srcfuncs_ordered     : np.ndarray,    # (npart,  )-shaped\n",
    "    rays                 : np.ndarray,    # (nray, 2, 3)-shaped\n",
    "    kernel_rad           : float,\n",
    "    col_kernel           : numba.core.registry.CPUDispatcher,\n",
    "    pts_order            : np.ndarray,    # (npart,  )-shaped\n",
    "    rel_tol              : float = 1e-15, # because float64 is only has only 16 digits accuracy\n",
    ") -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Sub process for integrate_along_ray_gridxy(). Numba parallel version (using prange).\n",
    "\n",
    "    Unit vec must be [0., 0., 1.] (i.e. all rays must point upwards towards +z).\n",
    "\n",
    "    Private function. Assumes specific input type. See source code comments.\n",
    "\n",
    "    \"\"\"\n",
    "    #raise NotImplementedError\n",
    "\n",
    "    nray  = len(rays)\n",
    "    npart = len(srcfuncs_ordered)\n",
    "    ndim  = pts_ordered.shape[-1]\n",
    "    anses = np.zeros(nray)\n",
    "    indes = np.zeros(nray, dtype=np.int64)    # indexes of max contribution particle\n",
    "    contr = np.zeros(nray)    # contribution of the max contribution particle\n",
    "    jused = np.full(npart, False)    # is j-th particle in the ordered list used for this calculation?\n",
    "\n",
    "    # error tolerance of tau (part 1)\n",
    "    tol_tau_base = np.log(srcfuncs_ordered.sum()) - np.log(rel_tol)\n",
    "\n",
    "    # hr = h * kernel_rad\n",
    "    hrs_ordered = hs_ordered * kernel_rad\n",
    "\n",
    "    # loop over ray\n",
    "    for i in prange(nray):\n",
    "        ray = rays[i]\n",
    "        tau = 0.\n",
    "        ans = 0.\n",
    "        dans= 0.\n",
    "        dans_max_tmp = 0.\n",
    "        ind = -1\n",
    "\n",
    "        #   xy-grid specific solution\n",
    "        ray_x = ray[0, 0]\n",
    "        ray_y = ray[0, 1]\n",
    "        \n",
    "        # loop over particles\n",
    "        #for pt, hr, mkappa_div_h2, srcfunc in zip(\n",
    "        #    pts_ordered, hrs_ordered, mkappa_div_h2_ordered, srcfuncs_ordered):\n",
    "        for j in range(npart):\n",
    "            pt = pts_ordered[j]\n",
    "            hr = hrs_ordered[j]\n",
    "            \n",
    "            # check if the particle is within range\n",
    "            #   general solution\n",
    "            #q = get_dist2_from_pt_to_line_nb(pt, ray)**0.5 / h\n",
    "            #if q < kernel_rad:\n",
    "            #   xy-grid specific solution\n",
    "            if ray_x - hr < pt[0] and pt[0] < ray_x + hr and ray_y - hr < pt[1] and pt[1] < ray_y + hr:\n",
    "                h = hs_ordered[ j]\n",
    "                q = ((pt[0] - ray_x)**2 + (pt[1] - ray_y)**2)**0.5 / h\n",
    "                if q < kernel_rad:\n",
    "\n",
    "                    jused[j] = True\n",
    "                    \n",
    "                    # now do radiative transfer\n",
    "                    \n",
    "                    mkappa_div_h2 = mkappa_div_h2_ordered[j]\n",
    "                    srcfunc = srcfuncs_ordered[j]\n",
    "                    \n",
    "                    dtau = mkappa_div_h2 * col_kernel(q, ndim-1)\n",
    "                    #tau += dtau/2.\n",
    "                    dans = np.exp(-tau) * (1. - np.exp(-dtau)) * srcfunc\n",
    "                    ans += dans\n",
    "                    tau += dtau#/2.\n",
    "\n",
    "                    # note down the largest contributor\n",
    "                    if dans > dans_max_tmp:\n",
    "                        dans_max_tmp = dans\n",
    "                        ind = pts_order[j]\n",
    "    \n",
    "                    # terminate the calc for this ray if tau is sufficient large\n",
    "                    #    such that the relative error on ans is smaller than rel_tol\n",
    "                    # i.e. since when tau > np.log(srcfuncs_ordered.sum()) - np.log(rel_tol) - np.log(ans),\n",
    "                    #    we know that ans[i] - ans[i][k] < rel_tol * ans[i]\n",
    "                    # see my notes for derivation\n",
    "                    if tau > tol_tau_base - np.log(ans):\n",
    "                        break\n",
    "            \n",
    "        anses[i] = ans\n",
    "        indes[i] = ind\n",
    "        contr[i] = dans_max_tmp / ans\n",
    "    \n",
    "    return anses, indes, contr, jused\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9aea1f60-0d2b-456f-b542-b87ad2b73105",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True, parallel=True)\n",
    "def _integrate_along_ray_gridxy_sub_parallel_err_ind(\n",
    "    pts_ordered          : np.ndarray,    # (npart, 3)-shaped\n",
    "    hs_ordered           : np.ndarray,    # (npart,  )-shaped\n",
    "    mkappa_div_h2_ordered: np.ndarray,    # (npart,  )-shaped\n",
    "    srcfuncs_ordered     : np.ndarray,    # (npart,  )-shaped\n",
    "    srcfuncs_err_ordered : np.ndarray,    # (npart,  )-shaped\n",
    "    rays                 : np.ndarray,    # (nray, 2, 3)-shaped\n",
    "    kernel_rad           : float,\n",
    "    col_kernel           : numba.core.registry.CPUDispatcher,\n",
    "    pts_order            : np.ndarray,    # (npart,  )-shaped\n",
    "    rel_tol              : float = 1e-15, # because float64 is only has only 16 digits accuracy\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Sub process for integrate_along_ray_gridxy(). Numba parallel version (using prange).\n",
    "\n",
    "    Unit vec must be [0., 0., 1.] (i.e. all rays must point upwards towards +z).\n",
    "\n",
    "    Private function. Assumes specific input type. See source code comments.\n",
    "\n",
    "    \"\"\"\n",
    "    #raise NotImplementedError\n",
    "\n",
    "    nray  = len(rays)\n",
    "    npart = len(srcfuncs_ordered)\n",
    "    ndim  = pts_ordered.shape[-1]\n",
    "    anses = np.zeros(nray)\n",
    "    erres = np.zeros(nray)\n",
    "\n",
    "    # error tolerance of tau (part 1)\n",
    "    tol_tau_base = np.log(srcfuncs_ordered.sum()) - np.log(rel_tol)\n",
    "\n",
    "    # hr = h * kernel_rad\n",
    "    hrs_ordered = hs_ordered * kernel_rad\n",
    "\n",
    "    # loop over ray\n",
    "    for i in prange(nray):\n",
    "        ray = rays[i]\n",
    "        tau = 0.\n",
    "        ans = 0.\n",
    "        err = 0.\n",
    "        dans= 0.\n",
    "\n",
    "        #   xy-grid specific solution\n",
    "        ray_x = ray[0, 0]\n",
    "        ray_y = ray[0, 1]\n",
    "        \n",
    "        # loop over particles\n",
    "        #for pt, hr, mkappa_div_h2, srcfunc in zip(\n",
    "        #    pts_ordered, hrs_ordered, mkappa_div_h2_ordered, srcfuncs_ordered):\n",
    "        for j in range(npart):\n",
    "            pt = pts_ordered[j]\n",
    "            hr = hrs_ordered[j]\n",
    "            \n",
    "            # check if the particle is within range\n",
    "            #   general solution\n",
    "            #q = get_dist2_from_pt_to_line_nb(pt, ray)**0.5 / h\n",
    "            #if q < kernel_rad:\n",
    "            #   xy-grid specific solution\n",
    "            if ray_x - hr < pt[0] and pt[0] < ray_x + hr and ray_y - hr < pt[1] and pt[1] < ray_y + hr:\n",
    "                h = hs_ordered[ j]\n",
    "                q = ((pt[0] - ray_x)**2 + (pt[1] - ray_y)**2)**0.5 / h\n",
    "                if q < kernel_rad:\n",
    "                    \n",
    "                    # now do radiative transfer\n",
    "                    \n",
    "                    mkappa_div_h2 = mkappa_div_h2_ordered[j]\n",
    "                    srcfunc = srcfuncs_ordered[j]\n",
    "                    srcfunc_err = srcfuncs_err_ordered[j]\n",
    "\n",
    "                    dtau = mkappa_div_h2 * col_kernel(q, ndim-1)\n",
    "                    #tau += dtau/2.\n",
    "                    dans = np.exp(-tau) * (1. - np.exp(-dtau)) * srcfunc\n",
    "                    err += np.exp(-tau) * (1. - np.exp(-dtau)) * srcfunc_err\n",
    "                    ans += dans\n",
    "                    tau += dtau#/2.\n",
    "    \n",
    "                    # terminate the calc for this ray if tau is sufficient large\n",
    "                    #    such that the relative error on ans is smaller than rel_tol\n",
    "                    # i.e. since when tau > np.log(srcfuncs_ordered.sum()) - np.log(rel_tol) - np.log(ans),\n",
    "                    #    we know that ans[i] - ans[i][k] < rel_tol * ans[i]\n",
    "                    # see my notes for derivation\n",
    "                    if tau > tol_tau_base - np.log(ans):\n",
    "                        break\n",
    "            \n",
    "        anses[i] = ans\n",
    "        erres[i] = err\n",
    "    \n",
    "    return anses, erres\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882b95a0-7c74-4249-a863-c7612ad7316a",
   "metadata": {},
   "source": [
    "#### integrate only, no error estiamtes (faster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a391db2-a336-4e5b-96be-d3c0692e96ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# integrate only, no error estiamtes\n",
    "\n",
    "def integrate_along_ray_gridxy_ind(\n",
    "    sdf     : sarracen.SarracenDataFrame,\n",
    "    srcfuncs: np.ndarray,\n",
    "    rays    : np.ndarray,\n",
    "    ray_unit_vec: np.ndarray|None = None,\n",
    "    kernel  : sarracen.kernels.BaseKernel = None,\n",
    "    parallel: bool = True,\n",
    "    err_h   : float = 1.0,\n",
    "    rel_tol : float = 1e-15,\n",
    "    sdf_kdtree : kdtree.KDTree = None,\n",
    "    xyzs_names_list : list = ['x', 'y', 'z'],\n",
    "    verbose : int = 3,\n",
    ") -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Backward integration of source functions along a grided ray (traced backwards), weighted by optical depth.\n",
    "    \n",
    "    Assuming all rays facing +z direction. (with the same ray_unit_vec [0., 0., 1.])\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sdf: sarracen.SarracenDataFrame\n",
    "        Must contain columns: x, y, z, h, m, kappa\n",
    "        \n",
    "    rays: (nray, 2, 3)-shaped array\n",
    "        Representing the ray trajectory. Currently only straight infinite lines are supported.\n",
    "        each ray is of the format:\n",
    "        [[begin point], [end point]]\n",
    "        where the end point is closer to the observer.\n",
    "\n",
    "    srcfuncs: 1D array\n",
    "        arrays describing the source function for every particle\n",
    "        \n",
    "    kernel: sarracen.kernels.base_kernel\n",
    "        Smoothing kernel for SPH data interpolation.\n",
    "        If None, will use the one in sdf.\n",
    "\n",
    "    parallel: bool\n",
    "        If to use the numba parallel function\n",
    "\n",
    "    err_h: float ( > 0. )\n",
    "        determine confidence level.\n",
    "        e.g.,\n",
    "            1.0 will give error assuming error range is +/-1.0 smoothing length h;\n",
    "            0.5 will give error assuming error range is +/-0.5 smoothing length h;\n",
    "            etc. etc.\n",
    "            \n",
    "    rel_tol : float\n",
    "        maximum relative error tolerence per ray.\n",
    "        Default 1e-15 because float64 is only accurate to ~16th digits.\n",
    "\n",
    "    sdf_kdtree : kdtree.KDTree\n",
    "        KDTree built from sdf[['x', 'y', 'z']], for fast neighbour search.\n",
    "        \n",
    "    xyzs_names_list: list\n",
    "        list of names of the columns that represents x, y, z axes (i.e. coord axes names)\n",
    "        MUST INCLUDE ALL THREE AXES LABELS.\n",
    "        If only 2 is included, WILL ASSUME IT IS 2D CACULATIONS.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    anses, indes, contr, pts_order_used\n",
    "    \n",
    "    anses: np.ndarray\n",
    "        Radiance (i.e. specific intensities) for each ray.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # init\n",
    "    npart : int = len(sdf)\n",
    "    nray  : int = len(rays)\n",
    "    if kernel is None: kernel = sdf.kernel\n",
    "    kernel_rad = float(kernel.get_radius())\n",
    "    col_kernel = kernel.get_column_kernel_func(samples=1000) # w integrated from z\n",
    "    if ray_unit_vec is None: ray_unit_vec = get_ray_unit_vec(rays[0])\n",
    "    \n",
    "    pts    = np.array(sdf[xyzs_names_list], order='C')    # (npart, 3)-shaped array (must be this shape for pts_order sorting below)\n",
    "    hs     = np.array(sdf[ 'h'           ], order='C')    # npart-shaped array\n",
    "    masses = np.array(sdf[ 'm'           ], order='C')\n",
    "    kappas = np.array(sdf[ 'kappa'       ], order='C')\n",
    "    srcfuncs = np.array(srcfuncs          , order='C')\n",
    "    ndim   = pts.shape[-1]\n",
    "    mkappa_div_h2_arr = masses * kappas / hs**(ndim-1)\n",
    "    \n",
    "    # sanity check\n",
    "    if is_verbose(verbose, 'err') and not np.allclose(ray_unit_vec, get_rays_unit_vec(rays)):\n",
    "        raise ValueError(f\"Inconsistent ray_unit_vec {ray_unit_vec} with the rays.\")\n",
    "\n",
    "    if is_verbose(verbose, 'warn') and ndim != 3:\n",
    "        say('warn', 'integrate_along_ray_gridxy_err_ind()', verbose, f\"ndim == {ndim} is not 3.\")\n",
    "\n",
    "    # (npart-shaped array of the indices of the particles from closest to the observer to the furthest)\n",
    "    pts_order             = np.argsort( np.sum(pts * ray_unit_vec, axis=-1) )[::-1]\n",
    "    pts_ordered           = pts[     pts_order]\n",
    "    hs_ordered            = hs[      pts_order]\n",
    "    mkappa_div_h2_ordered = mkappa_div_h2_arr[pts_order]\n",
    "    srcfuncs_ordered      = srcfuncs[pts_order]\n",
    "\n",
    "    # get used particles indexes\n",
    "    anses, indes, contr, jused = _integrate_along_ray_gridxy_sub_parallel_analysis(\n",
    "        pts_ordered, hs_ordered, mkappa_div_h2_ordered, srcfuncs_ordered, rays, kernel_rad, col_kernel, pts_order, rel_tol=rel_tol)\n",
    "\n",
    "    pts_order_used = pts_order[jused]\n",
    "    if is_verbose(verbose, 'info'):\n",
    "        nused = len(pts_order_used)\n",
    "        say('info', 'integrate_along_ray_gridxy_err_ind()', verbose,\n",
    "            f\"{nused} particles actually participated calculation\",\n",
    "            f\"({int(nused/npart*10000)/100.}% of all particles,\",\n",
    "            f\"average {int(nused/nray*100)/100.} per ray.)\", sep=' ')\n",
    "\n",
    "    \n",
    "    return anses, indes, contr, pts_order_used\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e03222-3349-40cc-ac4d-fab3dcdef35b",
   "metadata": {},
   "source": [
    "#### integrate with error estiamtes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3517015f-7833-48fd-ae31-31678d6c50df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# integrate and integrate error\n",
    "\n",
    "#@jit(nopython=False)\n",
    "def integrate_along_ray_gridxy_err_ind(\n",
    "    sdf     : sarracen.SarracenDataFrame,\n",
    "    srcfuncs: np.ndarray,\n",
    "    rays    : np.ndarray,\n",
    "    ray_unit_vec: np.ndarray|None = None,\n",
    "    kernel  : sarracen.kernels.BaseKernel = None,\n",
    "    parallel: bool = True,\n",
    "    err_h   : float = 1.0,\n",
    "    rel_tol : float = 1e-15,\n",
    "    sdf_kdtree : kdtree.KDTree = None,\n",
    "    xyzs_names_list : list = ['x', 'y', 'z'],\n",
    "    verbose : int = 3,\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Backward integration of source functions along a grided ray (traced backwards), weighted by optical depth.\n",
    "    \n",
    "    Assuming all rays facing +z direction. (with the same ray_unit_vec [0., 0., 1.])\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sdf: sarracen.SarracenDataFrame\n",
    "        Must contain columns: x, y, z, h, m, kappa\n",
    "        \n",
    "    rays: (nray, 2, 3)-shaped array\n",
    "        Representing the ray trajectory. Currently only straight infinite lines are supported.\n",
    "        each ray is of the format:\n",
    "        [[begin point], [end point]]\n",
    "        where the end point is closer to the observer.\n",
    "\n",
    "    srcfuncs: 1D array\n",
    "        arrays describing the source function for every particle\n",
    "        \n",
    "    kernel: sarracen.kernels.base_kernel\n",
    "        Smoothing kernel for SPH data interpolation.\n",
    "        If None, will use the one in sdf.\n",
    "\n",
    "    parallel: bool\n",
    "        If to use the numba parallel function\n",
    "\n",
    "    err_h: float ( > 0. )\n",
    "        determine confidence level.\n",
    "        e.g.,\n",
    "            1.0 will give error assuming error range is +/-1.0 smoothing length h;\n",
    "            0.5 will give error assuming error range is +/-0.5 smoothing length h;\n",
    "            etc. etc.\n",
    "            \n",
    "    rel_tol : float\n",
    "        maximum relative error tolerence per ray.\n",
    "        Default 1e-15 because float64 is only accurate to ~16th digits.\n",
    "\n",
    "    sdf_kdtree : kdtree.KDTree\n",
    "        KDTree built from sdf[['x', 'y', 'z']], for fast neighbour search.\n",
    "        \n",
    "    xyzs_names_list: list\n",
    "        list of names of the columns that represents x, y, z axes (i.e. coord axes names)\n",
    "        MUST INCLUDE ALL THREE AXES LABELS.\n",
    "        If only 2 is included, WILL ASSUME IT IS 2D CACULATIONS.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    rads, errs\n",
    "    \n",
    "    rads: np.ndarray\n",
    "        Radiance (i.e. specific intensities) for each ray.\n",
    "\n",
    "    errs: np.ndarray\n",
    "        Uncertainties of Radiance (i.e. specific intensities) for each ray.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # init\n",
    "    npart : int = len(sdf)\n",
    "    nray  : int = len(rays)\n",
    "    if kernel is None: kernel = sdf.kernel\n",
    "    kernel_rad = float(kernel.get_radius())\n",
    "    col_kernel = kernel.get_column_kernel_func(samples=1000) # w integrated from z\n",
    "    if ray_unit_vec is None: ray_unit_vec = get_ray_unit_vec(rays[0])\n",
    "    \n",
    "    pts    = np.array(sdf[xyzs_names_list], order='C')    # (npart, 3)-shaped array (must be this shape for pts_order sorting below)\n",
    "    hs     = np.array(sdf[ 'h'           ], order='C')    # npart-shaped array\n",
    "    masses = np.array(sdf[ 'm'           ], order='C')\n",
    "    kappas = np.array(sdf[ 'kappa'       ], order='C')\n",
    "    srcfuncs = np.array(srcfuncs          , order='C')\n",
    "    ndim   = pts.shape[-1]\n",
    "    mkappa_div_h2_arr = masses * kappas / hs**(ndim-1)\n",
    "    \n",
    "    # sanity check\n",
    "    if is_verbose(verbose, 'err') and not np.allclose(ray_unit_vec, get_rays_unit_vec(rays)):\n",
    "        raise ValueError(f\"Inconsistent ray_unit_vec {ray_unit_vec} with the rays.\")\n",
    "\n",
    "    if is_verbose(verbose, 'warn') and ndim != 3:\n",
    "        say('warn', 'integrate_along_ray_gridxy_err_ind()', verbose, f\"ndim == {ndim} is not 3.\")\n",
    "\n",
    "    # (npart-shaped array of the indices of the particles from closest to the observer to the furthest)\n",
    "    pts_order             = np.argsort( np.sum(pts * ray_unit_vec, axis=-1) )[::-1]\n",
    "    pts_ordered           = pts[     pts_order]\n",
    "    hs_ordered            = hs[      pts_order]\n",
    "    mkappa_div_h2_ordered = mkappa_div_h2_arr[pts_order]\n",
    "    srcfuncs_ordered      = srcfuncs[pts_order]\n",
    "\n",
    "    # get used particles indexes\n",
    "    anses, indes, contr, jused = _integrate_along_ray_gridxy_sub_parallel_analysis(\n",
    "        pts_ordered, hs_ordered, mkappa_div_h2_ordered, srcfuncs_ordered, rays, kernel_rad, col_kernel, pts_order, rel_tol=rel_tol)\n",
    "\n",
    "    pts_order_used = pts_order[jused]\n",
    "    if is_verbose(verbose, 'info'):\n",
    "        nused = len(pts_order_used)\n",
    "        say('info', 'integrate_along_ray_gridxy_err_ind()', verbose,\n",
    "            f\"{nused} particles actually participated in calculation\",\n",
    "            f\"({int(nused/npart*10000)/100.}% of all particles,\",\n",
    "            f\"average {int(nused/nray*100)/100.} per ray.)\", sep=' ')\n",
    "\n",
    "\n",
    "    # calculate error in those particles\n",
    "    sdf['_srcfunc'] = srcfuncs\n",
    "    srcfuncs_err_orderedu = get_sph_error(sdf, '_srcfunc', pts_order_used, err_h=err_h, sdf_kdtree=sdf_kdtree,\n",
    "                                 kernel=kernel, xyzs_names_list=xyzs_names_list, verbose=verbose)[:, 0]\n",
    "    sdf['_srcfunc_err'] = np.nan\n",
    "    sdf['_srcfunc_err'].iloc[pts_order_used] = srcfuncs_err_orderedu\n",
    "\n",
    "    if is_verbose(verbose, 'debug'):\n",
    "        nused = len(pts_order_used)\n",
    "        say('debug', 'integrate_along_ray_gridxy_err_ind()', verbose,\n",
    "            f\"error calculation completed.\",\n",
    "            f\"average relative error of srcfunc\",\n",
    "            f\"{int((srcfuncs_err_orderedu/srcfuncs_ordered[jused]).sum()/len(pts_order_used)*10000)/100.}%\",\n",
    "            sep=' ')\n",
    "\n",
    "    rads, errs = _integrate_along_ray_gridxy_sub_parallel_err_ind(\n",
    "        pts_ordered[jused], hs_ordered[jused], mkappa_div_h2_ordered[jused], srcfuncs_ordered[jused],\n",
    "        srcfuncs_err_orderedu, rays, kernel_rad, col_kernel, pts_order, rel_tol=rel_tol)\n",
    "    #raise NotImplementedError\n",
    "    \n",
    "    return rads, errs\n",
    "\n",
    "#integrate_along_ray_gridxy_ind = integrate_along_ray_gridxy_err_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ea50ff-b1e8-4239-9172-0982ef13ede3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### rays grid generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5aa6efe-41e2-4f0a-be7f-b5307c1b7708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xy_grids_of_rays(\n",
    "    sdf  : None|sarracen.SarracenDataFrame = None,\n",
    "    #dXs  : None|list[list[float], list[float]]|np.ndarray= None,\n",
    "    no_xy: tuple[int, int] = (32, 32),\n",
    "    #orig_vec: np.ndarray = np.zeros(3),\n",
    "    frac_contained: float = 100., #99.73,\n",
    "    use_adaptive_grid: bool = False,\n",
    "    xyzs_names_list : list = ['x', 'y', 'z'],\n",
    "    verbose: int = 3,\n",
    ") -> tuple[np.ndarray, np.ndarray, list[np.ndarray]]:\n",
    "    \"\"\"Get a grid of rays (must pointing at z direction (i.e. xyzs_names_list[-1] direction) for now).\n",
    "\n",
    "    Supply either sdf or both dx and dy.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sdf: sarracen.SarracenDataFrame\n",
    "\n",
    "    no_xy: tuple[int, int]\n",
    "        number of the rays per axes.\n",
    "        \n",
    "    frac_contained : float\n",
    "        Suggested percentage of the particle that are contained within the grid. in (0, 100]\n",
    "\n",
    "    use_adaptive_grid : bool\n",
    "        if True,\n",
    "            will scale dXs according to particle distribution instead of even intervals,\n",
    "            if dXs is None or (None, None).\n",
    "\n",
    "    xyzs_names_list: list\n",
    "        list of names of the columns that represents x, y, z axes (i.e. coord axes names)\n",
    "        WARNING: since ndim==len(xyzs_names_list), if len(xyzs_names_list) !=3 will resulting non-3D results.\n",
    "            In which case you will need to change no_xy as well.\n",
    "\n",
    "\n",
    "    Returns: rays, areas, dXs\n",
    "    -------\n",
    "    rays: (no_ray, 2, 3)-shaped np.ndarray\n",
    "\n",
    "    areas: (no_ray)-shaped np.ndarray\n",
    "        areas corresponding to each ray in the grid\n",
    "        \n",
    "    dXs: list of no_xy[i]-shaped np.ndarray\n",
    "        width of the grid cells. in sdf units['dist'].\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    unit_vec = np.zeros(len(xyzs_names_list))\n",
    "    unit_vec[-1] = 1.\n",
    "    #x0, y0, z0 = orig_vec\n",
    "    z0 = 0.    # z value for rays\n",
    "\n",
    "    # sanity checks\n",
    "    if is_verbose(verbose, 'warn') and len(xyzs_names_list) != 3:\n",
    "        say('warn', 'get_xy_grids_of_rays()', verbose,\n",
    "            f\"xyzs_names_list being {xyzs_names_list}, its len = {len(xyzs_names_list)} is not 3.\",\n",
    "            f\"This means we are assuming {len(xyzs_names_list)}D.\")\n",
    "    if is_verbose(verbose, 'error') and len(xyzs_names_list) != len(no_xy) + 1:\n",
    "        say('error', 'get_xy_grids_of_rays()', verbose,\n",
    "            f\"ndim (=={len(xyzs_names_list)}) != len(no_xy) (=={len(no_xy)}) + 1\",\n",
    "            f\"i.e. asked ray grid dimension {no_xy} does not makes sense.\",\n",
    "            \"This will likely cause error in the next steps.\")\n",
    "    \n",
    "    # get dx & dy\n",
    "    frac_contained_m = 50. - frac_contained / 2.\n",
    "    frac_contained_p = 50. + frac_contained / 2.\n",
    "\n",
    "    Xs_edges = []\n",
    "    for i, label in enumerate(xyzs_names_list[:-1]):\n",
    "        #i0 = orig_vec[i]\n",
    "        if use_adaptive_grid:\n",
    "            # fraction points for the adaptive grid\n",
    "            fracs = np.linspace(frac_contained_m, frac_contained_p, no_xy[i]+1)\n",
    "            # edge points for the grid\n",
    "            Xs_edges.append(\n",
    "                np.percentile(np.concatenate((sdf[label] - sdf['h'], sdf[label] + sdf['h'])), fracs))\n",
    "        else:\n",
    "            Xs_edges.append(\n",
    "                np.linspace(\n",
    "                    *np.percentile(\n",
    "                        np.concatenate(\n",
    "                            (sdf[label] - sdf['h'], sdf[label] + sdf['h']),\n",
    "                        ), (frac_contained_m, frac_contained_p),\n",
    "                    ), no_xy[i]+1,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    dXs = np.array([np.diff(Xi_edges) for Xi_edges in Xs_edges])    # each item is (no_xy[i]+1,)-shaped\n",
    "    Xs_centers = np.array([Xi_edges[:-1] + dXi/2. for dXi, Xi_edges in zip(dXs, Xs_edges)])    # each item is (no_xy[i],)-shaped\n",
    "\n",
    "    # Note: orig_vecs must be 2D (i.e. in shape of (no_ray, 3))\n",
    "    orig_vecs = np.array([[*xy, z0] for xy in itertools.product(*Xs_centers)])\n",
    "    #orig_vecs = [[[x, y, z0] for x, y in zip(xs, ys)] for xs, ys in zip(*np.meshgrid(*xys))]\n",
    "    areas = np.array([dx*dy for dy in dXs[1] for dx in dXs[0]])\n",
    "\n",
    "    rays = mupl.geometry.get_rays(orig_vecs=orig_vecs, unit_vecs=unit_vec)\n",
    "    \n",
    "    return rays, areas, dXs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d16cde-9d77-4113-844d-c50637f3a07a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "97c973f4-9f35-4dd6-ba34-7836d5a16625",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_imshow(\n",
    "    no_xy: tuple[int, int],\n",
    "    rays: units.Quantity|np.ndarray,\n",
    "    data: units.Quantity|np.ndarray,\n",
    "    job_profile : dict= None,\n",
    "    file_index  : int = -1,\n",
    "    title_suffix: str =\"\",\n",
    "    notes       : dict= None,\n",
    "    data_label  : str =\"\",\n",
    "    save_label  : str =\"\",\n",
    "    xyzs:str|list = 'xyz',\n",
    "    norm=None,\n",
    "    cmap=None,\n",
    "    output_dir:str|None=None,\n",
    "    verbose = 3,\n",
    "):\n",
    "    \"\"\"Plotting a heatmap (contourf) of 1D data located at rays\"\"\"\n",
    "\n",
    "\n",
    "    if not isinstance(data, units.Quantity):\n",
    "        data = set_as_quantity(data, units.dimensionless_unscaled)\n",
    "\n",
    "    if not isinstance(rays, units.Quantity):\n",
    "        rays = set_as_quantity(rays, units.dimensionless_unscaled)\n",
    "\n",
    "    if job_profile is None:\n",
    "        job_profile = {\n",
    "            'plot_title_suffix': '',\n",
    "            'nickname'         : '',\n",
    "        }\n",
    "\n",
    "    #Xs = rays[:, 0, 0]\n",
    "    #Ys = rays[:, 0, 1]\n",
    "    rays_val = rays.reshape(*no_xy, *rays.shape[1:]).value\n",
    "    extent = (\n",
    "        rays_val[ 0, 0, 0, 0] - (rays_val[ 1, 0, 0, 0] - rays_val[ 0, 0, 0, 0])/2,\n",
    "        rays_val[-1,-1, 0, 0] + (rays_val[-1,-1, 0, 0] - rays_val[-2,-1, 0, 0])/2,\n",
    "        rays_val[ 0, 0, 0, 1] - (rays_val[ 0, 1, 0, 1] - rays_val[ 0, 0, 0, 1])/2,\n",
    "        rays_val[-1,-1, 0, 1] + (rays_val[-1,-1, 0, 1] - rays_val[-1,-2, 0, 1])/2,\n",
    "    )\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    \n",
    "    cax = ax.imshow(data.reshape(no_xy).T.value, norm=norm, cmap=cmap, origin='lower', extent=extent)\n",
    "    #cax = ax.contourf(Xs.reshape(no_xy), Ys.reshape(no_xy), data.reshape(no_xy), cmap=cmap)\n",
    "    fig.colorbar(cax, label=f\"{data_label} / {data.unit.to_string('latex_inline')}\")\n",
    "    ax.set_xlabel(f\"${xyzs[0]}$ / {rays.unit.to_string('latex_inline')}\")\n",
    "    ax.set_ylabel(f\"${xyzs[1]}$ / {rays.unit.to_string('latex_inline')}\")\n",
    "    if notes is not None:\n",
    "        ax.text(\n",
    "            0.98, 0.98,\n",
    "            f\"Time = {notes['time']:.1f}\\n\" + \\\n",
    "            f\" $L$ = {notes['lum']:.0f}\",\n",
    "            #color = \"black\",\n",
    "            ha = 'right', va = 'top',\n",
    "            transform=ax.transAxes,\n",
    "        )\n",
    "\n",
    "    no_xy_txt = 'x'.join([f'{i}' for i in no_xy])\n",
    "    outfilename_noext = f\"{output_dir}heat_{job_profile['nickname']}_{file_index:05d}_{''.join(xyzs)}_{save_label}_{no_xy_txt}\"\n",
    "    outfilenames = []\n",
    "\n",
    "    # write pdf\n",
    "    outfilename = f\"{outfilename_noext}.pdf\"\n",
    "    fig.savefig(outfilename)\n",
    "    outfilenames.append(outfilename)\n",
    "    if is_verbose(verbose, 'note'):\n",
    "        say('note', None, verbose, f\"Fig saved to {outfilename}.\")\n",
    "\n",
    "    # write png (with plot title)\n",
    "    ax.set_title(f\"Heatmap of {data_label}\\n{job_profile['plot_title_suffix']}\")\n",
    "    outfilename = f\"{outfilename_noext}.png\"\n",
    "    fig.savefig(outfilename)\n",
    "    outfilenames.append(outfilename)\n",
    "    if is_verbose(verbose, 'note'):\n",
    "        say('note', None, verbose, f\"Fig saved to {outfilename}.\")\n",
    "        \n",
    "    return fig, ax, outfilenames\n",
    "\n",
    "## example\n",
    "#fig, ax = plot_imshow(\n",
    "#    no_xy, rays * units.Rsun, anses, data_label=\"$I$\", save_label=\"I_xyz\",\n",
    "#    job_profile=job_profile, file_index=file_index, output_dir=output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8daf8b-0b46-4deb-80a1-f6d4ad05bc06",
   "metadata": {},
   "source": [
    "### Error estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9cd2dae-0051-430f-b6f4-ee4d84316ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sph_neighbours(\n",
    "    sdf_kdtree : kdtree.KDTree,\n",
    "    xyz_i      : np.ndarray,\n",
    "    h_i        : float,\n",
    "    w_rad      : float,\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Find neighbours of xyz_i within (w_rad*h_i) distance, using k-d tree.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sdf_kdtree : kdtree.KDTree\n",
    "        KDTree built from sdf[['x', 'y', 'z']], for fast neighbour search.\n",
    "    xyz_i : (ndim,)-shaped numpy array\n",
    "        position of the querying point\n",
    "    h_i : float\n",
    "        Smoothing length\n",
    "    w_rad: float\n",
    "        radius of the smoothing kernel w.\n",
    "    \n",
    "    Returns: dists, indices\n",
    "    -------\n",
    "    dists : np.ndarray\n",
    "        distances of the neighbouring points to the querying point\n",
    "    indices : np.ndarray\n",
    "        indices of the neighbouring points\n",
    "    \"\"\"\n",
    "    npart = sdf_kdtree.n\n",
    "    dists, indices = sdf_kdtree.query(xyz_i, k=npart, distance_upper_bound=w_rad*h_i)\n",
    "    indices_indices = indices<npart\n",
    "    return dists[indices_indices], indices[indices_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6cc574e5-6910-40f0-b845-66e99c39055b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sph error estimation\n",
    "\n",
    "# found this func in my old codes ../photosphere/Analysis_PhLoc.ipynb\n",
    "\n",
    "def get_sph_error(\n",
    "    sdf        : sarracen.SarracenDataFrame,\n",
    "    target_labels   : str|list[str],\n",
    "    target_indicies : int|list[int]|np.ndarray = [],\n",
    "    err_h      : float    = 1.0,\n",
    "    sdf_kdtree : kdtree.KDTree = None,\n",
    "    kernel     : sarracen.kernels.BaseKernel = None,\n",
    "    xyzs_names_list : list = ['x', 'y', 'z'],\n",
    "    verbose: int = 3,\n",
    ") -> np.ndarray:    # (ntarget, nval)-shaped\n",
    "    \"\"\"Calculate error bar for sarracen data frame.\n",
    "    \n",
    "    Assuming 3D.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sdf: sarracen.SarracenDataFrame\n",
    "        Need to contain columns: x, y, z, m, h, rho.\n",
    "        If density (rho) is not in sdf, will compute rho.\n",
    "        \n",
    "    target_labels: str or list of str (len>2)\n",
    "        Column label of the target data in sdf for error computing\n",
    "        \n",
    "    target_indicies: int or list of int or np.ndarray\n",
    "        indices for particles in sdf for error calculating\n",
    "        \n",
    "    err_h: float ( > 0. )\n",
    "        determine confidence level.\n",
    "        e.g.,\n",
    "            1.0 will give error assuming error range is +/-1.0 smoothing length h;\n",
    "            0.5 will give error assuming error range is +/-0.5 smoothing length h;\n",
    "            etc. etc.\n",
    "            \n",
    "    sdf_kdtree: kdtree.KDTree\n",
    "        KDTree built from sdf[['x', 'y', 'z']], for fast neighbour search.\n",
    "        If None, will build one.\n",
    "        \n",
    "    kernel: sarracen.kernels.base_kernel\n",
    "        Smoothing kernel for SPH data interpolation.\n",
    "        If None, will use the one in sdf.\n",
    "\n",
    "    xyzs_names_list: list\n",
    "        list of names of the columns that represents x, y, z axes (i.e. coord axes names)\n",
    "        MUST INCLUDE ALL THREE AXES LABELS.\n",
    "        If only 2 is included, WILL ASSUME IT IS 2D CACULATIONS.\n",
    "    \n",
    "            \n",
    "    Returns: dvals\n",
    "    -------\n",
    "    dvalsp: (ntarget, nval)-shaped ndarray\n",
    "        error.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # init\n",
    "    \n",
    "    xyzs = sdf[xyzs_names_list].to_numpy()\n",
    "    ms   = sdf['m'   ].to_numpy()\n",
    "    hs   = sdf['h'   ].to_numpy()\n",
    "    rhos = sdf['rho' ].to_numpy()\n",
    "    target_indicies = np.atleast_1d(target_indicies)\n",
    "    dxes = err_h * hs[target_indicies]\n",
    "    # assuming 3D in the following calc\n",
    "    locs = xyzs[target_indicies]\n",
    "    vals =  sdf[ target_labels ].to_numpy()[target_indicies]\n",
    "    if vals.ndim == 2: nval = vals.shape[1]\n",
    "    else:              nval = 1\n",
    "    ntarget = len(target_indicies)\n",
    "    ndim = len(xyzs_names_list)\n",
    "    \n",
    "    if sdf_kdtree is None:\n",
    "        sdf_kdtree = kdtree.KDTree(xyzs)\n",
    "    if kernel is None:\n",
    "        kernel = sdf.kernel\n",
    "    kernel_rad = float(kernel.get_radius())\n",
    "    kernel_w   = kernel.w\n",
    "        \n",
    "    neigh_rad = kernel_rad + err_h\n",
    "    \n",
    "    # ans array\n",
    "    dvals = np.full((ntarget, nval), np.nan)\n",
    "    \n",
    "    \n",
    "    for i in range(ntarget):\n",
    "        loc = locs[i]\n",
    "        val = vals[i]\n",
    "        dx  = dxes[i]\n",
    "        h   = hs[target_indicies[i]]\n",
    "        # find all neighbours within 3h, this includes all points needed for the calc of error for this particle\n",
    "        _, neigh_inds = get_sph_neighbours(sdf_kdtree, loc, h, neigh_rad)\n",
    "\n",
    "        # prepare data\n",
    "        sdf_temp = sdf.iloc[neigh_inds]\n",
    "        \n",
    "        loc_plus_dx = [loc for i in range(ndim*2)]\n",
    "        for j in range(ndim):\n",
    "            loc_plus_dx[j][j] += dx\n",
    "            loc_plus_dx[ndim+j][j] -= dx\n",
    "\n",
    "        dval_xyz = get_sph_interp(sdf_temp, target_labels, loc_plus_dx, kernel=kernel, verbose=0) - val\n",
    "        dvals[i] = ((dval_xyz**2).sum(axis=0)/len(dval_xyz))**0.5\n",
    "    \n",
    "    #dvals = dvals.squeeze()\n",
    "    return dvals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b89f446-18c0-4b53-a8c6-6a208b05a055",
   "metadata": {},
   "source": [
    ".\n",
    "\n",
    "## Main\n",
    "\n",
    ".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efa53829-5cf5-4874-92ec-ad7a4c9658ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yay\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'__main__'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "781a4c3a-af5e-4354-aece-629b1ebe4077",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Note: Density column rho already exist in self.time = 0.0.\n",
      "Start: 2024-04-04T01:28:24.917374\n",
      "\tWorking on 2md_00000_xyz...\n",
      "    Debug  :    integrate_along_ray_gridxy_err_ind():\n",
      "\t1858 particles actually participated calculation (0.13% of all particles, average 0.02 per ray.)\n",
      "Ended: 2024-04-04T01:29:05.232427\n",
      "Time Used: 0:00:40.315053\n",
      "\n",
      "Start: 2024-04-04T01:29:05.232551\n",
      "\tWorking on 2md_00000_xzy...\n",
      "    Debug  :    integrate_along_ray_gridxy_err_ind():\n",
      "\t1867 particles actually participated calculation (0.13% of all particles, average 0.02 per ray.)\n",
      "Ended: 2024-04-04T01:29:46.810683\n",
      "Time Used: 0:00:41.578132\n",
      "\n",
      "    Note: Density column rho already exist in self.time = 5000.0.\n",
      "Start: 2024-04-04T01:29:48.431293\n",
      "\tWorking on 2md_00100_xyz...\n",
      "    Debug  :    integrate_along_ray_gridxy_err_ind():\n",
      "\t2138 particles actually participated calculation (0.15% of all particles, average 0.03 per ray.)\n",
      "Ended: 2024-04-04T01:30:39.305732\n",
      "Time Used: 0:00:50.874439\n",
      "\n",
      "Start: 2024-04-04T01:30:39.305843\n",
      "\tWorking on 2md_00100_xzy...\n",
      "    Debug  :    integrate_along_ray_gridxy_err_ind():\n",
      "\t2109 particles actually participated calculation (0.15% of all particles, average 0.03 per ray.)\n",
      "Ended: 2024-04-04T01:31:28.697706\n",
      "Time Used: 0:00:49.391863\n",
      "\n",
      "    Note: Density column rho already exist in self.time = 10000.0.\n",
      "Start: 2024-04-04T01:31:30.402689\n",
      "\tWorking on 2md_00200_xyz...\n",
      "    Debug  :    integrate_along_ray_gridxy_err_ind():\n",
      "\t2234 particles actually participated calculation (0.16% of all particles, average 0.03 per ray.)\n",
      "Ended: 2024-04-04T01:32:24.947883\n",
      "Time Used: 0:00:54.545194\n",
      "\n",
      "Start: 2024-04-04T01:32:24.947992\n",
      "\tWorking on 2md_00200_xzy...\n",
      "    Debug  :    integrate_along_ray_gridxy_err_ind():\n",
      "\t2267 particles actually participated calculation (0.16% of all particles, average 0.03 per ray.)\n",
      "Ended: 2024-04-04T01:33:16.375782\n",
      "Time Used: 0:00:51.427790\n",
      "\n",
      "    Note: Density column rho already exist in self.time = 15000.0.\n",
      "Start: 2024-04-04T01:33:18.053372\n",
      "\tWorking on 2md_00300_xyz...\n",
      "    Debug  :    integrate_along_ray_gridxy_err_ind():\n",
      "\t3473 particles actually participated calculation (0.25% of all particles, average 0.05 per ray.)\n",
      "Ended: 2024-04-04T01:34:18.818364\n",
      "Time Used: 0:01:00.764992\n",
      "\n",
      "Start: 2024-04-04T01:34:18.818586\n",
      "\tWorking on 2md_00300_xzy...\n",
      "    Debug  :    integrate_along_ray_gridxy_err_ind():\n",
      "\t3717 particles actually participated calculation (0.27% of all particles, average 0.05 per ray.)\n",
      "Ended: 2024-04-04T01:35:08.097038\n",
      "Time Used: 0:00:49.278452\n",
      "\n",
      "    Note: Density column rho already exist in self.time = 20000.0.\n",
      "Start: 2024-04-04T01:35:10.317115\n",
      "\tWorking on 2md_00400_xyz...\n",
      "    Debug  :    integrate_along_ray_gridxy_err_ind():\n",
      "\t6907 particles actually participated calculation (0.5% of all particles, average 0.1 per ray.)\n",
      "Ended: 2024-04-04T01:36:14.399905\n",
      "Time Used: 0:01:04.082790\n",
      "\n",
      "Start: 2024-04-04T01:36:14.400157\n",
      "\tWorking on 2md_00400_xzy...\n",
      "    Debug  :    integrate_along_ray_gridxy_err_ind():\n",
      "\t6909 particles actually participated calculation (0.5% of all particles, average 0.1 per ray.)\n",
      "Ended: 2024-04-04T01:37:16.484832\n",
      "Time Used: 0:01:02.084675\n",
      "\n",
      "    Note: Density column rho already exist in self.time = 25000.0.\n",
      "Start: 2024-04-04T01:37:18.457160\n",
      "\tWorking on 2md_00500_xyz...\n",
      "    Debug  :    integrate_along_ray_gridxy_err_ind():\n",
      "\t12384 particles actually participated calculation (0.9% of all particles, average 0.18 per ray.)\n",
      "Ended: 2024-04-04T01:38:27.439151\n",
      "Time Used: 0:01:08.981991\n",
      "\n",
      "Start: 2024-04-04T01:38:27.439253\n",
      "\tWorking on 2md_00500_xzy...\n",
      "    Debug  :    integrate_along_ray_gridxy_err_ind():\n",
      "\t12360 particles actually participated calculation (0.9% of all particles, average 0.18 per ray.)\n",
      "Ended: 2024-04-04T01:39:33.991238\n",
      "Time Used: 0:01:06.551985\n",
      "\n",
      "    Note: Density column rho already exist in self.time = 30000.0.\n",
      "Start: 2024-04-04T01:39:35.761884\n",
      "\tWorking on 2md_00600_xyz...\n",
      "    Debug  :    integrate_along_ray_gridxy_err_ind():\n",
      "\t19082 particles actually participated calculation (1.39% of all particles, average 0.29 per ray.)\n",
      "Ended: 2024-04-04T01:40:41.120000\n",
      "Time Used: 0:01:05.358116\n",
      "\n",
      "Start: 2024-04-04T01:40:41.120107\n",
      "\tWorking on 2md_00600_xzy...\n",
      "    Debug  :    integrate_along_ray_gridxy_err_ind():\n",
      "\t19198 particles actually participated calculation (1.39% of all particles, average 0.29 per ray.)\n",
      "Ended: 2024-04-04T01:41:45.392596\n",
      "Time Used: 0:01:04.272489\n",
      "\n",
      "    Note: Density column rho already exist in self.time = 35000.0.\n",
      "Start: 2024-04-04T01:41:47.167394\n",
      "\tWorking on 2md_00700_xyz...\n",
      "    Debug  :    integrate_along_ray_gridxy_err_ind():\n",
      "\t26185 particles actually participated calculation (1.9% of all particles, average 0.39 per ray.)\n",
      "Ended: 2024-04-04T01:42:50.689111\n",
      "Time Used: 0:01:03.521717\n",
      "\n",
      "Start: 2024-04-04T01:42:50.689267\n",
      "\tWorking on 2md_00700_xzy...\n",
      "    Debug  :    integrate_along_ray_gridxy_err_ind():\n",
      "\t26508 particles actually participated calculation (1.93% of all particles, average 0.4 per ray.)\n",
      "Ended: 2024-04-04T01:44:02.567769\n",
      "Time Used: 0:01:11.878502\n",
      "\n",
      "    Note: Density column rho already exist in self.time = 40000.0.\n",
      "Start: 2024-04-04T01:44:04.941061\n",
      "\tWorking on 2md_00800_xyz...\n",
      "    Debug  :    integrate_along_ray_gridxy_err_ind():\n",
      "\t31840 particles actually participated calculation (2.32% of all particles, average 0.48 per ray.)\n",
      "Ended: 2024-04-04T01:45:19.962360\n",
      "Time Used: 0:01:15.021299\n",
      "\n",
      "Start: 2024-04-04T01:45:19.962532\n",
      "\tWorking on 2md_00800_xzy...\n",
      "    Debug  :    integrate_along_ray_gridxy_err_ind():\n",
      "\t32574 particles actually participated calculation (2.37% of all particles, average 0.49 per ray.)\n",
      "Ended: 2024-04-04T01:46:38.853479\n",
      "Time Used: 0:01:18.890947\n",
      "\n",
      "    Note: Density column rho already exist in self.time = 45000.0.\n",
      "Start: 2024-04-04T01:46:40.911415\n",
      "\tWorking on 2md_00900_xyz...\n",
      "    Debug  :    integrate_along_ray_gridxy_err_ind():\n",
      "\t35861 particles actually participated calculation (2.61% of all particles, average 0.54 per ray.)\n",
      "Ended: 2024-04-04T01:47:49.701870\n",
      "Time Used: 0:01:08.790455\n",
      "\n",
      "Start: 2024-04-04T01:47:49.701980\n",
      "\tWorking on 2md_00900_xzy...\n",
      "    Debug  :    integrate_along_ray_gridxy_err_ind():\n",
      "\t36570 particles actually participated calculation (2.66% of all particles, average 0.55 per ray.)\n",
      "Ended: 2024-04-04T01:49:05.243215\n",
      "Time Used: 0:01:15.541235\n",
      "\n",
      "    Note: Density column rho already exist in self.time = 50000.0.\n",
      "Start: 2024-04-04T01:49:07.249499\n",
      "\tWorking on 2md_01000_xyz...\n",
      "    Debug  :    integrate_along_ray_gridxy_err_ind():\n",
      "\t39444 particles actually participated calculation (2.87% of all particles, average 0.6 per ray.)\n",
      "Ended: 2024-04-04T01:50:19.806999\n",
      "Time Used: 0:01:12.557500\n",
      "\n",
      "Start: 2024-04-04T01:50:19.807106\n",
      "\tWorking on 2md_01000_xzy...\n",
      "    Debug  :    integrate_along_ray_gridxy_err_ind():\n",
      "\t39716 particles actually participated calculation (2.89% of all particles, average 0.6 per ray.)\n",
      "Ended: 2024-04-04T01:51:36.180876\n",
      "Time Used: 0:01:16.373770\n",
      "\n",
      "    Note: Density column rho already exist in self.time = 55000.0.\n",
      "Start: 2024-04-04T01:51:38.025563\n",
      "\tWorking on 2md_01100_xyz...\n",
      "    Debug  :    integrate_along_ray_gridxy_err_ind():\n",
      "\t44104 particles actually participated calculation (3.21% of all particles, average 0.67 per ray.)\n",
      "Ended: 2024-04-04T01:52:50.607336\n",
      "Time Used: 0:01:12.581773\n",
      "\n",
      "Start: 2024-04-04T01:52:50.607498\n",
      "\tWorking on 2md_01100_xzy...\n",
      "    Debug  :    integrate_along_ray_gridxy_err_ind():\n",
      "\t41394 particles actually participated calculation (3.01% of all particles, average 0.63 per ray.)\n",
      "Ended: 2024-04-04T01:54:04.919899\n",
      "Time Used: 0:01:14.312401\n",
      "\n",
      "    Note: Density column rho already exist in self.time = 60000.0.\n",
      "Start: 2024-04-04T01:54:07.110425\n",
      "\tWorking on 2md_01200_xyz...\n",
      "    Debug  :    integrate_along_ray_gridxy_err_ind():\n",
      "\t51304 particles actually participated calculation (3.73% of all particles, average 0.78 per ray.)\n",
      "Ended: 2024-04-04T01:55:20.360192\n",
      "Time Used: 0:01:13.249767\n",
      "\n",
      "Start: 2024-04-04T01:55:20.360345\n",
      "\tWorking on 2md_01200_xzy...\n",
      "    Debug  :    integrate_along_ray_gridxy_err_ind():\n",
      "\t45118 particles actually participated calculation (3.28% of all particles, average 0.68 per ray.)\n",
      "Ended: 2024-04-04T01:56:33.836214\n",
      "Time Used: 0:01:13.475869\n",
      "\n",
      "    Note: Density column rho already exist in self.time = 65000.0.\n",
      "Start: 2024-04-04T01:56:35.621507\n",
      "\tWorking on 2md_01300_xyz...\n",
      "    Debug  :    integrate_along_ray_gridxy_err_ind():\n",
      "\t61559 particles actually participated calculation (4.48% of all particles, average 0.93 per ray.)\n",
      "Ended: 2024-04-04T01:57:47.089383\n",
      "Time Used: 0:01:11.467876\n",
      "\n",
      "Start: 2024-04-04T01:57:47.089759\n",
      "\tWorking on 2md_01300_xzy...\n",
      "    Debug  :    integrate_along_ray_gridxy_err_ind():\n",
      "\t52754 particles actually participated calculation (3.84% of all particles, average 0.8 per ray.)\n",
      "Ended: 2024-04-04T01:59:00.614870\n",
      "Time Used: 0:01:13.525111\n",
      "\n",
      "    Note: Density column rho already exist in self.time = 70000.0.\n",
      "Start: 2024-04-04T01:59:02.394542\n",
      "\tWorking on 2md_01400_xyz...\n",
      "    Debug  :    integrate_along_ray_gridxy_err_ind():\n",
      "\t71453 particles actually participated calculation (5.2% of all particles, average 1.09 per ray.)\n",
      "Ended: 2024-04-04T02:00:20.018346\n",
      "Time Used: 0:01:17.623804\n",
      "\n",
      "Start: 2024-04-04T02:00:20.018500\n",
      "\tWorking on 2md_01400_xzy...\n",
      "    Debug  :    integrate_along_ray_gridxy_err_ind():\n",
      "\t64369 particles actually participated calculation (4.69% of all particles, average 0.98 per ray.)\n",
      "Ended: 2024-04-04T02:01:35.731721\n",
      "Time Used: 0:01:15.713221\n",
      "\n",
      "    Note: Density column rho already exist in self.time = 75000.0.\n",
      "Start: 2024-04-04T02:01:37.860824\n",
      "\tWorking on 2md_01500_xyz...\n",
      "    Debug  :    integrate_along_ray_gridxy_err_ind():\n",
      "\t82529 particles actually participated calculation (6.01% of all particles, average 1.25 per ray.)\n",
      "Ended: 2024-04-04T02:03:01.096867\n",
      "Time Used: 0:01:23.236043\n",
      "\n",
      "Start: 2024-04-04T02:03:01.097008\n",
      "\tWorking on 2md_01500_xzy...\n",
      "    Debug  :    integrate_along_ray_gridxy_err_ind():\n",
      "\t76966 particles actually participated calculation (5.6% of all particles, average 1.17 per ray.)\n",
      "Ended: 2024-04-04T02:04:14.383028\n",
      "Time Used: 0:01:13.286020\n",
      "\n",
      "    Note: Density column rho already exist in self.time = 80000.0.\n",
      "Start: 2024-04-04T02:04:17.181033\n",
      "\tWorking on 2md_01600_xyz...\n",
      "    Debug  :    integrate_along_ray_gridxy_err_ind():\n",
      "\t94541 particles actually participated calculation (6.89% of all particles, average 1.44 per ray.)\n",
      "Ended: 2024-04-04T02:05:36.738523\n",
      "Time Used: 0:01:19.557490\n",
      "\n",
      "Start: 2024-04-04T02:05:36.738647\n",
      "\tWorking on 2md_01600_xzy...\n",
      "    Debug  :    integrate_along_ray_gridxy_err_ind():\n",
      "\t90308 particles actually participated calculation (6.58% of all particles, average 1.37 per ray.)\n",
      "Ended: 2024-04-04T02:06:56.395648\n",
      "Time Used: 0:01:19.657001\n",
      "\n",
      "    Note: Density column rho already exist in self.time = 85000.0.\n",
      "Start: 2024-04-04T02:06:58.342253\n",
      "\tWorking on 2md_01700_xyz...\n",
      "    Debug  :    integrate_along_ray_gridxy_err_ind():\n",
      "\t107914 particles actually participated calculation (7.86% of all particles, average 1.64 per ray.)\n",
      "Ended: 2024-04-04T02:08:26.474940\n",
      "Time Used: 0:01:28.132687\n",
      "\n",
      "Start: 2024-04-04T02:08:26.475065\n",
      "\tWorking on 2md_01700_xzy...\n",
      "    Debug  :    integrate_along_ray_gridxy_err_ind():\n",
      "\t105554 particles actually participated calculation (7.69% of all particles, average 1.61 per ray.)\n",
      "Ended: 2024-04-04T02:09:44.632502\n",
      "Time Used: 0:01:18.157437\n",
      "\n",
      "    Note: Density column rho already exist in self.time = 90000.0.\n",
      "Start: 2024-04-04T02:09:46.669105\n",
      "\tWorking on 2md_01800_xyz...\n",
      "    Debug  :    integrate_along_ray_gridxy_err_ind():\n",
      "\t121354 particles actually participated calculation (8.84% of all particles, average 1.85 per ray.)\n",
      "Ended: 2024-04-04T02:11:08.290564\n",
      "Time Used: 0:01:21.621459\n",
      "\n",
      "Start: 2024-04-04T02:11:08.290745\n",
      "\tWorking on 2md_01800_xzy...\n",
      "    Debug  :    integrate_along_ray_gridxy_err_ind():\n",
      "\t119635 particles actually participated calculation (8.71% of all particles, average 1.82 per ray.)\n",
      "Ended: 2024-04-04T02:12:30.958039\n",
      "Time Used: 0:01:22.667294\n",
      "\n",
      "    Note: Density column rho already exist in self.time = 95000.0.\n",
      "Start: 2024-04-04T02:12:34.005660\n",
      "\tWorking on 2md_01900_xyz...\n",
      "    Debug  :    integrate_along_ray_gridxy_err_ind():\n",
      "\t135120 particles actually participated calculation (9.84% of all particles, average 2.06 per ray.)\n",
      "Ended: 2024-04-04T02:13:59.835075\n",
      "Time Used: 0:01:25.829415\n",
      "\n",
      "Start: 2024-04-04T02:13:59.835203\n",
      "\tWorking on 2md_01900_xzy...\n",
      "    Debug  :    integrate_along_ray_gridxy_err_ind():\n",
      "\t131554 particles actually participated calculation (9.58% of all particles, average 2.0 per ray.)\n",
      "Ended: 2024-04-04T02:15:23.032028\n",
      "Time Used: 0:01:23.196825\n",
      "\n",
      "    Note: Density column rho already exist in self.time = 100000.0.\n",
      "Start: 2024-04-04T02:15:26.190219\n",
      "\tWorking on 2md_02000_xyz...\n",
      "    Debug  :    integrate_along_ray_gridxy_err_ind():\n",
      "\t151923 particles actually participated calculation (11.07% of all particles, average 2.31 per ray.)\n",
      "Ended: 2024-04-04T02:16:48.541170\n",
      "Time Used: 0:01:22.350951\n",
      "\n",
      "Start: 2024-04-04T02:16:48.541283\n",
      "\tWorking on 2md_02000_xzy...\n",
      "    Debug  :    integrate_along_ray_gridxy_err_ind():\n",
      "\t140396 particles actually participated calculation (10.23% of all particles, average 2.14 per ray.)\n",
      "Ended: 2024-04-04T02:18:11.154404\n",
      "Time Used: 0:01:22.613121\n",
      "\n",
      "    Note: Density column rho already exist in self.time = 240000.0.\n",
      "Start: 2024-04-04T02:18:13.630345\n",
      "\tWorking on 2md_04800_xyz...\n",
      "    Debug  :    integrate_along_ray_gridxy_err_ind():\n",
      "\t1145509 particles actually participated calculation (84.45% of all particles, average 17.47 per ray.)\n",
      "Ended: 2024-04-04T02:19:35.498778\n",
      "Time Used: 0:01:21.868433\n",
      "\n",
      "Start: 2024-04-04T02:19:35.498884\n",
      "\tWorking on 2md_04800_xzy...\n",
      "    Debug  :    integrate_along_ray_gridxy_err_ind():\n",
      "\t1118235 particles actually participated calculation (82.44% of all particles, average 17.06 per ray.)\n",
      "Ended: 2024-04-04T02:20:58.820818\n",
      "Time Used: 0:01:23.321934\n",
      "\n",
      "    Note: Density column rho already exist in self.time = 320000.0.\n",
      "Start: 2024-04-04T02:21:01.246463\n",
      "\tWorking on 2md_06400_xyz...\n",
      "    Debug  :    integrate_along_ray_gridxy_err_ind():\n",
      "\t674483 particles actually participated calculation (49.85% of all particles, average 10.29 per ray.)\n",
      "Ended: 2024-04-04T02:22:12.491240\n",
      "Time Used: 0:01:11.244777\n",
      "\n",
      "Start: 2024-04-04T02:22:12.491404\n",
      "\tWorking on 2md_06400_xzy...\n",
      "    Debug  :    integrate_along_ray_gridxy_err_ind():\n",
      "\t76868 particles actually participated calculation (5.68% of all particles, average 1.17 per ray.)\n",
      "Ended: 2024-04-04T02:23:22.090050\n",
      "Time Used: 0:01:09.598646\n",
      "\n",
      "    Note: Density column rho already exist in self.time = 400000.0.\n",
      "Start: 2024-04-04T02:23:24.292436\n",
      "\tWorking on 2md_08000_xyz...\n",
      "    Debug  :    integrate_along_ray_gridxy_err_ind():\n",
      "\t302552 particles actually participated calculation (22.37% of all particles, average 4.61 per ray.)\n",
      "Ended: 2024-04-04T02:24:37.684999\n",
      "Time Used: 0:01:13.392563\n",
      "\n",
      "Start: 2024-04-04T02:24:37.685160\n",
      "\tWorking on 2md_08000_xzy...\n",
      "    Debug  :    integrate_along_ray_gridxy_err_ind():\n",
      "\t122604 particles actually participated calculation (9.06% of all particles, average 1.87 per ray.)\n",
      "Ended: 2024-04-04T02:25:48.269694\n",
      "Time Used: 0:01:10.584534\n",
      "\n",
      "    Note: Density column rho already exist in self.time = 880000.0.\n",
      "Start: 2024-04-04T02:25:50.441452\n",
      "\tWorking on 2md_17600_xyz...\n",
      "    Debug  :    integrate_along_ray_gridxy_err_ind():\n",
      "\t595761 particles actually participated calculation (44.05% of all particles, average 9.09 per ray.)\n",
      "Ended: 2024-04-04T02:27:00.505240\n",
      "Time Used: 0:01:10.063788\n",
      "\n",
      "Start: 2024-04-04T02:27:00.505357\n",
      "\tWorking on 2md_17600_xzy...\n",
      "    Debug  :    integrate_along_ray_gridxy_err_ind():\n",
      "\t626846 particles actually participated calculation (46.35% of all particles, average 9.56 per ray.)\n",
      "Ended: 2024-04-04T02:28:22.300466\n",
      "Time Used: 0:01:21.795109\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    # init combined data\n",
    "    comb = {}\n",
    "    \n",
    "    for job_nickname in job_nicknames: #['2md', ]:\n",
    "        comb[job_nickname] = {\n",
    "            xyzs: {\n",
    "                'times_yr' : [],\n",
    "                'lums_Lsun': [],\n",
    "            } for xyzs in xyzs_list\n",
    "        }\n",
    "        job_profile = JOB_PROFILES_DICT[job_nickname]\n",
    "        job_name    = job_profile['job_name']\n",
    "        file_indexes= job_profile['file_indexes']\n",
    "        params      = job_profile['params']\n",
    "        eos_opacity = EoS_MESA_opacity(params, settings)\n",
    "        \n",
    "        for file_index in file_indexes:\n",
    "            # init\n",
    "    \n",
    "            mpdf = mpdf_read(job_name, file_index, eos_opacity, reset_xyz_by='R1', verbose=1)\n",
    "            mpdf.calc_sdf_params(['R1'])\n",
    "            sdf  = mpdf.data['gas']\n",
    "            srcfuncs = mpdf.const['sigma_sb'] * sdf['T']**4 / pi\n",
    "            sdf['srcfunc'] = srcfuncs\n",
    "            for xyzs in xyzs_list:\n",
    "                xyzs_names_list = [x for x in xyzs]\n",
    "    \n",
    "                \n",
    "                # record time used\n",
    "                python_time_start = datetime.utcnow()\n",
    "                print(f\"Start: {python_time_start.isoformat()}\")\n",
    "                print(f\"\\tWorking on {job_nickname}_{file_index:05d}_{xyzs}...\")\n",
    "    \n",
    "                \n",
    "                # get rays\n",
    "                rays, areas, dXs = get_xy_grids_of_rays(\n",
    "                    sdf, no_xy=no_xy, frac_contained=100., use_adaptive_grid=False, xyzs_names_list=xyzs_names_list)\n",
    "                pts    = np.array(sdf[xyzs_names_list])\n",
    "                hs     = np.array(sdf[ 'h' ])    # npart-shaped array\n",
    "                kernel = sdf.kernel\n",
    "                kernel_rad = float(kernel.get_radius())\n",
    "                col_kernel = kernel.get_column_kernel_func(samples=1000)\n",
    "                \n",
    "                rays_u = (rays * mpdf.units['dist']).to(units.au)\n",
    "                areas_u = (areas * mpdf.units['dist']**2).to(units.au**2)\n",
    "    \n",
    "                \n",
    "                # do integration without error estimation\n",
    "                ans = integrate_along_ray_gridxy_ind(sdf, srcfuncs, rays, xyzs_names_list=xyzs_names_list, verbose=99)\n",
    "                rads, inds, contr, pts_order_used = ans\n",
    "                rads = (rads * mpdf.units['sigma_sb'] * mpdf.units['temp']**4 / units.sr).cgs\n",
    "                inds *= units.dimensionless_unscaled\n",
    "                contr = 100 * contr * units.percent\n",
    "                lum = ((4 * pi * units.sr) * (rads * areas_u)).sum().to(units.solLum)\n",
    "                #anses_fft = fft.fft2(rads.reshape(no_xy).value)\n",
    "            \n",
    "                # save interm data\n",
    "                data = {}\n",
    "                data['lum'  ] = lum\n",
    "                data['rays' ] = rays_u[:, 0, :2]\n",
    "                data['ray_unit_vec'] = get_ray_unit_vec(rays_u[0].value)\n",
    "                data['area_per_ray'] = areas_u[0] #areas_u\n",
    "                data['rads' ] = rads\n",
    "                data['time' ] = mpdf.get_time()\n",
    "                data['mpdf_params'] = mpdf.params\n",
    "                \n",
    "                with open(f\"{interm_dir}{job_nickname}_{file_index:05d}.lcgen.{xyzs}.{no_xy_txt}.json\", 'w') as f:\n",
    "                    mupl.json_dump(data, f, metadata)\n",
    "    \n",
    "                comb[job_nickname][xyzs]['times_yr' ].append(data['time'].to_value(units.yr))\n",
    "                comb[job_nickname][xyzs]['lums_Lsun'].append(data['lum' ].to_value(units.Lsun))\n",
    "    \n",
    "                \n",
    "                # plotting\n",
    "                if False:\n",
    "                    plt.close('all')\n",
    "                    fig, ax, outfilenames = plot_imshow(\n",
    "                        no_xy, rays_u, rads, data_label=\"$I$\",\n",
    "                        xyzs=xyzs, save_label=f\"I\",\n",
    "                        job_profile=job_profile, file_index=file_index, notes=data,\n",
    "                        output_dir=output_dir, verbose=verbose_loop)\n",
    "                    fig, ax, outfilenames = plot_imshow(\n",
    "                        no_xy, rays_u, inds%20, data_label=\"index % 20 of the most contributed\",\n",
    "                        xyzs=xyzs, save_label=f\"dinds\",\n",
    "                        job_profile=job_profile, file_index=file_index, cmap='turbo', notes=data,\n",
    "                        output_dir=output_dir, verbose=verbose_loop)\n",
    "                    fig, ax, outfilenames = plot_imshow(\n",
    "                        no_xy, rays_u, contr, data_label=\"contribution fraction of the most contributed\",\n",
    "                        xyzs=xyzs, save_label=f\"contr\",\n",
    "                        job_profile=job_profile, file_index=file_index, cmap='seismic', notes=data,\n",
    "                        output_dir=output_dir, verbose=verbose_loop)\n",
    "                    #fig, ax, outfilenames = plot_imshow(\n",
    "                    #    no_xy, rays_u, np.abs(anses_fft), data_label=\"FFt of $I$\", xyzs=xyzs, save_label=f\"I-fft\",\n",
    "                    #    norm=mpl.colors.LogNorm(),\n",
    "                    #    job_profile=job_profile, file_index=file_index, notes=data, output_dir=output_dir, verbose=verbose_loop)\n",
    "    \n",
    "    \n",
    "                # record time used\n",
    "                python_time_ended = datetime.utcnow()\n",
    "                python_time__used  = python_time_ended - python_time_start\n",
    "                print(f\"Ended: {python_time_ended.isoformat()}\\nTime Used: {python_time__used}\\n\")\n",
    "\n",
    "        \n",
    "        # save data for now\n",
    "        with open(f\"{interm_dir}lcgen.{no_xy_txt}.json.part\", 'w') as f:\n",
    "            mupl.json_dump(comb, f, metadata)\n",
    "\n",
    "        \n",
    "        #plotting\n",
    "        plt.close('all')\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        for xyzs in xyzs_list:\n",
    "            ax.semilogy(comb[job_nickname][xyzs]['times_yr'], comb[job_nickname][xyzs]['lums_Lsun'], 'o--', label=f\"Viewed from +{xyzs[2]}\")\n",
    "        ax.legend()\n",
    "        ax.set_xlabel('Time / yr')\n",
    "        ax.set_ylabel('Luminosity / Lsun')\n",
    "        ax.set_xlim(0., 45.)\n",
    "        ax.set_ylim(1e4, 5e6)\n",
    "        outfilename_noext = f\"{output_dir}LC_{job_nickname}_{no_xy_txt}\"\n",
    "        \n",
    "        # write pdf\n",
    "        outfilename = f\"{outfilename_noext}.pdf\"\n",
    "        fig.savefig(outfilename)\n",
    "        if is_verbose(verbose, 'note'):\n",
    "            say('note', None, verbose, f\"Fig saved to {outfilename}.\")\n",
    "        \n",
    "        # write png (with plot title)\n",
    "        ax.set_title(f\"Light curve ({job_nickname}, {no_xy_txt} rays)\")\n",
    "        outfilename = f\"{outfilename_noext}.png\"\n",
    "        fig.savefig(outfilename)\n",
    "        if is_verbose(verbose, 'note'):\n",
    "            say('note', None, verbose, f\"Fig saved to {outfilename}.\")\n",
    "                \n",
    "    \n",
    "    plt.close('all')\n",
    "    with open(f\"{interm_dir}lcgen.{no_xy_txt}.json\", 'w') as f:\n",
    "        mupl.json_dump(comb, f, metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3777a8-6e31-4190-902e-7ba8af8b78ac",
   "metadata": {},
   "source": [
    "    for job_nickname in job_nicknames:\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        for xyzs in xyzs_list:\n",
    "            ax.semilogy(comb[job_nickname][xyzs]['times_yr'], comb[job_nickname][xyzs]['lums_Lsun'], 'o--', label=f\"Viewed from +{xyzs[2]}\")\n",
    "        ax.legend()\n",
    "        ax.set_xlabel('Time / yr')\n",
    "        ax.set_ylabel('Luminosity / Lsun')\n",
    "        ax.set_xlim(0., 45.)\n",
    "        ax.set_ylim(1e4, 5e6)\n",
    "        outfilename_noext = f\"{output_dir}LC_{job_profile['nickname']}_{no_xy_txt}\"\n",
    "        \n",
    "        # write pdf\n",
    "        outfilename = f\"{outfilename_noext}.pdf\"\n",
    "        fig.savefig(outfilename)\n",
    "        if is_verbose(verbose, 'note'):\n",
    "            say('note', None, verbose, f\"Fig saved to {outfilename}.\")\n",
    "        \n",
    "        # write png (with plot title)\n",
    "        ax.set_title(f\"Light curve ({job_nickname}, {no_xy_txt} rays)\")\n",
    "        outfilename = f\"{outfilename_noext}.png\"\n",
    "        fig.savefig(outfilename)\n",
    "        if is_verbose(verbose, 'note'):\n",
    "            say('note', None, verbose, f\"Fig saved to {outfilename}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985c0289-7144-4160-9783-0c584eb489d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
