{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa711f22-c313-4343-a584-e3efdecd3ad4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Scripts for analyzing of phantom outputs.\\n\\nThis script generate lightcurves (LC) by doing radiative transfer on a grid.\\n\\nAuthor: Chunliang Mu (at Macquarie University, expected duration 2023-2026)\\n\\n\\n-------------------------------------------------------------------------------\\n\\nSide note: Remember to limit line length to 79 characters according to PEP-8\\n    https://peps.python.org/pep-0008/#maximum-line-length    \\nwhich is the length of below line of '-' characters.\\n\\n-------------------------------------------------------------------------------\\n\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Scripts for analyzing of phantom outputs.\n",
    "\n",
    "This script generate lightcurves (LC) by doing radiative transfer on a grid.\n",
    "\n",
    "Author: Chunliang Mu (at Macquarie University, expected duration 2023-2026)\n",
    "\n",
    "\n",
    "-------------------------------------------------------------------------------\n",
    "\n",
    "Side note: Remember to limit line length to 79 characters according to PEP-8\n",
    "    https://peps.python.org/pep-0008/#maximum-line-length    \n",
    "which is the length of below line of '-' characters.\n",
    "\n",
    "-------------------------------------------------------------------------------\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b19be2-030c-4fa9-ade5-f4bb38a0c13a",
   "metadata": {},
   "source": [
    ".\n",
    "\n",
    "# Def\n",
    "\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035afbe0-6134-4121-8416-f0dbbab169fb",
   "metadata": {},
   "source": [
    "## Imports & Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "093b4271-dab0-4b64-8b31-1588fc264966",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from numpy import pi\n",
    "from astropy import units\n",
    "from astropy import constants as const\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from numba import jit\n",
    "import sarracen\n",
    "import itertools\n",
    "from scipy import integrate, fft\n",
    "from scipy.spatial import kdtree\n",
    "# fix weird moviepy cannot find my ffmpeg exe error\n",
    "try: from moviepy import editor\n",
    "except RuntimeError: import os; os.environ[\"IMAGEIO_FFMPEG_EXE\"] = \"/usr/bin/ffmpeg\"\n",
    "#from moviepy.editor import ImageSequenceClip\n",
    "#from os import path\n",
    "\n",
    "from datetime import datetime, UTC\n",
    "now_utc = lambda: datetime.now(UTC)\n",
    "now = now_utc\n",
    "\n",
    "# fix numpy v1.* compatibility\n",
    "try: np.trapezoid\n",
    "except AttributeError: np.trapezoid = np.trapz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c75c48c-a738-4783-a6af-560603d629a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import my modules listed in ./main/\n",
    "\n",
    "import clmuphantomlib as mupl\n",
    "from clmuphantomlib            import MyPhantomDataFrames, get_eos, get_col_kernel_funcs\n",
    "from clmuphantomlib.log        import is_verbose, say\n",
    "#from clmuphantomlib.settings   import DEFAULT_SETTINGS as settings\n",
    "from clmuphantomlib.units_util import get_val_in_unit, set_as_quantity #, get_units_field_name, get_units_cgs\n",
    "from clmuphantomlib.io         import json_dump, json_load\n",
    "from clmuphantomlib.eos        import get_eos_opacity\n",
    "from clmuphantomlib.light      import get_optical_depth_by_ray_tracing_3D, get_photosphere_on_ray\n",
    "#from clmuphantomlib.sph_interp import get_col_kernel_funcs\n",
    "\n",
    "from multiprocessing import cpu_count, Pool #Process, Queue\n",
    "NPROCESSES = 1 if cpu_count() is None else max(cpu_count(), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "859f4f11-9307-4f36-b896-3778f252d65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*   Note   :    script:\n",
      "\tWill use 8 processes for parallelization\n"
     ]
    }
   ],
   "source": [
    "# settings\n",
    "#\n",
    "#   imported from script_input.py file\n",
    "\n",
    "\n",
    "from script_LCGen__input import (\n",
    "    verbose, verbose_loop,\n",
    "    interm_dir, output_dir, JOB_PROFILES_DICT, job_nicknames, xyzs_list, no_xy, no_xy_txt,\n",
    "    unitsOut, PHOTOSPHERE_TAU, AVG_KC_PP, wavlens, use_Tscales, nsample_pp, z_olim_kc,\n",
    ")\n",
    "from _sharedFuncs import mpdf_read\n",
    "\n",
    "unitsOutTxt = {  key  : unitsOut[key].to_string('latex_inline') for key in unitsOut.keys() }\n",
    "\n",
    "\n",
    "# set metadata\n",
    "with open(\"_metadata__input.json\", 'r') as f:\n",
    "    metadata = json_load(f)\n",
    "metadata['Title'] = \"Getting light curves by intergrating across a grid of rays\"\n",
    "metadata['Description'] = f\"\"\"Getting light curves by intergrating across a grid of rays with the same directions\n",
    "for dump file data generated by phantom\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "if __name__ == '__main__' and is_verbose(verbose, 'note'):\n",
    "    # remember to check if name is '__main__' if you wanna say anything\n",
    "    #    so when you do multiprocessing the program doesn't freak out\n",
    "    say('note', \"script\", verbose, f\"Will use {NPROCESSES} processes for parallelization\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4853c017-d945-4a40-9098-74dd24e60180",
   "metadata": {},
   "outputs": [],
   "source": [
    "from clmuphantomlib.log import say, is_verbose\n",
    "from clmuphantomlib.geometry import get_dist2_between_2pt, get_closest_pt_on_line\n",
    "from clmuphantomlib.sph_interp import get_sph_interp, get_sph_gradient, get_h_from_rho, get_no_neigh, _get_sph_interp_phantom_np\n",
    "from clmuphantomlib.units_util import set_as_quantity, set_as_quantity_temperature, get_units_field_name\n",
    "from clmuphantomlib.eos import EoS_Base\n",
    "#from clmuphantomlib.light import integrate_along_ray_grid, integrate_along_ray_gridxy\n",
    "\n",
    "#  import (general)\n",
    "import numpy as np\n",
    "from numpy import typing as npt\n",
    "import numba\n",
    "from numba import jit, prange\n",
    "import sarracen\n",
    "\n",
    "from clmuphantomlib.geometry import get_dist2_from_pts_to_line, get_dist2_from_pt_to_line_nb, get_ray_unit_vec, get_rays_unit_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645cc989-5868-4956-9009-e3fd293c8736",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fba0ff6-57ab-4dc7-88d8-874c885a5e45",
   "metadata": {},
   "source": [
    "### LC integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3e20ab-9650-46fc-898f-b676d35540b8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Backup codes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394b11f2-040f-484e-9ed4-6132700848e1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Test codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c6e0b1f-a96f-4f04-893f-c9cf4230eb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test runs\n",
    "@jit(nopython=True, parallel=True, fastmath=True)\n",
    "def _integrate_along_rays_gridxy_sub_parallel_olim(\n",
    "    pts_ordered          : npt.NDArray[np.float64],    # (npart, 3)-shaped\n",
    "    hs_ordered           : npt.NDArray[np.float64],    # (npart,  )-shaped\n",
    "    mkappa_div_h2_ordered: npt.NDArray[np.float64],    # (npart,  )-shaped\n",
    "    srcfuncs_ordered     : npt.NDArray[np.float64],    # (npart,  )-shaped\n",
    "    rays_xy              : npt.NDArray[np.float64],    # (nray,  2)-shaped\n",
    "    ray_areas            : npt.NDArray[np.float64],    # (nray,   )-shaped\n",
    "    kernel_rad           : float,\n",
    "    kernel_col           : numba.core.registry.CPUDispatcher,\n",
    "    kernel_csz           : numba.core.registry.CPUDispatcher,\n",
    "    kernel_w             : numba.core.registry.CPUDispatcher,\n",
    "    pts_order            : npt.NDArray[np.float64],    # (npart,  )-shaped\n",
    "    rel_tol              : float = 1e-16, # because float64 has only 16 digits accuracy\n",
    "    nsample_pp           : int   = 100,  # no of sample points for integration\n",
    "    z_olim_kc            : float = 1.152,  # col kernel limit for when srcfunc began to count\n",
    "    photosphere_tau      : float = 2./3.,\n",
    ") -> tuple[\n",
    "    npt.NDArray[np.float64],    # rads\n",
    "    npt.NDArray[np.float64],    # rads_olim\n",
    "    npt.NDArray[np.float64],    # phkcs\n",
    "    npt.NDArray[np.float64],    # pphzs\n",
    "    npt.NDArray[np.int64  ],    # indes\n",
    "    npt.NDArray[np.float64],    # contr\n",
    "    npt.NDArray[np.float64],    # jfact\n",
    "    npt.NDArray[np.float64],    # jfact_olim\n",
    "    npt.NDArray[np.float64],    # estis\n",
    "]:\n",
    "    \"\"\"Sub process for integrate_along_ray_gridxy(). Numba parallel version (using prange).\n",
    "\n",
    "    Note: in this version,\n",
    "    the source function\n",
    "        outside of the outermost particle (within one h of the ray)'s z\n",
    "        are ignored.\n",
    "    \n",
    "    ---------------------------------------------------------------------------\n",
    "\n",
    "    Calculating the luminosity using\n",
    "    $$\n",
    "    L \\approx\n",
    "        4 \\pi \\sum_i \\sum_j \\triangle A_i\n",
    "            S_j \\frac{\\kappa_j m_j}{h_j^2}\n",
    "            \\int\n",
    "                e^{\n",
    "                    -\\sum_k \\frac{\\kappa_k m_k}{h_k^2}\n",
    "                    w_\\mathrm{csz}(q_{xy, ik}, \\, -q_{z, k})\n",
    "                }\n",
    "                w(q_{ij}) d(q_{z, j})\n",
    "    $$\n",
    "\n",
    "    Unit vec must be [0., 0., 1.] (i.e. all rays must point upwards towards +z)\n",
    "\n",
    "    Private function. Assumes specific input type. See source code comments.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    rads, rads_olim, phkcs, pphzs, indes, contr, jfact, jfact_olim, estis\n",
    "    \n",
    "    rads: (nray,)-shaped np.ndarray[float]\n",
    "        Radiance (i.e. specific intensities) for each ray.\n",
    "\n",
    "    rads_olim: (nray,)-shaped np.ndarray[float]\n",
    "        Radiance (i.e. specific intensities) for each ray, excluding unresolved surface.\n",
    "\n",
    "    phkcs: (nray,)-shaped np.ndarray[float]\n",
    "        Summed column kernel values at the photosphere.\n",
    "        summed of all particle's columne kernel if photosphere not found\n",
    "\n",
    "    pphzs: (nray,)-shaped np.ndarray[float]\n",
    "        locations of the photosphere (i.e. z value).\n",
    "        np.nan if photosphere not found\n",
    "\n",
    "    indes: (nray,)-shaped np.ndarray[int]\n",
    "        indexes of the max contribution particle\n",
    "\n",
    "    contr: (nray,)-shaped np.ndarray[float]\n",
    "        relative contribution (in fractions) of the max contribution particle\n",
    "\n",
    "    jfact: (npart,)-shaped np.ndarray[float]\n",
    "        Contribution factor for j-th particle.\n",
    "        Cannot be used as a way to calc lum\n",
    "            because we are additionally assuming the outermost particles has srcfunc of zero.\n",
    "\n",
    "    jfact_olim: (npart,)-shaped np.ndarray[float]\n",
    "        Contribution factor for j-th particle.\n",
    "        Multiply it with 4 * pi * srcfuncs and sum it up as an alternative way to get the luminosity.\n",
    "        Will be zero if particle is not used.\n",
    "\n",
    "    estis: (nray,)-shaped np.ndarray[float]\n",
    "        Estimations of radiance (i.e. specific intensities) for each ray, using old method\n",
    "        (Also estis means 'was' in Esperanto, so it's a fitting name)\n",
    "\n",
    "    \"\"\"\n",
    "    #raise NotImplementedError\n",
    "\n",
    "    nray  = len(rays_xy)\n",
    "    npart = len(srcfuncs_ordered)\n",
    "    ndim  = pts_ordered.shape[-1]\n",
    "    rads  = np.zeros(nray)\n",
    "    rads_olim = np.zeros(nray)\n",
    "    indes = np.zeros(nray, dtype=np.int64)    # indexes of max contribution particle\n",
    "    contr = np.zeros(nray)     # relative contribution of the max contribution particle\n",
    "    # ifact = np.zeros(nray)     # effective xsec for i-th ray  # NOTE: ifact = pones * ray_areas\n",
    "    jfact = np.zeros(npart)    # effective xsec for j-th particle\n",
    "    jfact_olim = np.zeros(npart)    # effective xsec for j-th particle\n",
    "    phkcs = np.zeros(nray)    # photosphere column kernels\n",
    "    pphzs = np.full(nray, np.nan)    # photosphere z\n",
    "    # pones = np.zeros(nray)\n",
    "    # ptaus = np.full(nray, np.nan)    # lower bound of the optical depth\n",
    "    estis = np.zeros(nray)\n",
    "    \n",
    "    # error tolerance of tau (part 1)\n",
    "    # #    this is aonly here for reference- value will be updated later\n",
    "    # tol_tau_base = np.log(srcfuncs_ordered.sum()) - np.log(rel_tol)\n",
    "\n",
    "\n",
    "    # cache calcs\n",
    "    hrs_ordered = hs_ordered * kernel_rad\n",
    "    # #   limits (approx)\n",
    "    # z_min = pts_ordered[-1, 2] - hrs_ordered[-1]\n",
    "    # z_max = pts_ordered[ 0, 2] + hrs_ordered[ 0]\n",
    "\n",
    "    # loop over ray\n",
    "    for i in prange(nray):\n",
    "        ray_xy  = rays_xy[i]\n",
    "        ray_area= ray_areas[i]\n",
    "        rad     = 0.\n",
    "        rad_olim= 0.\n",
    "        rad_est = 0.   # estimation of radiance (i.e. rad)\n",
    "        # dans_max_tmp = 0.\n",
    "        dfac_max_tmp = 0.\n",
    "        ind     = -1\n",
    "        fac     = 0. # effectively <1>\n",
    "        dfac    = 0. # factor\n",
    "        used_j  = np.int64(0)    # j = used_indexes[used_j]\n",
    "        ji      = np.int64(0)\n",
    "        ji_r    = np.int64(0)\n",
    "        z_olim = 0.\n",
    "\n",
    "        #   xy-grid specific solution\n",
    "        ray_x = ray_xy[0]\n",
    "        ray_y = ray_xy[1]\n",
    "\n",
    "        \n",
    "\n",
    "        # First, try to find out how many relevant particles are there\n",
    "\n",
    "        nused_i = 0    # no of used particles for i-th ray\n",
    "        srcfuncs_tot_relevant = 0.\n",
    "        for j in range(npart):\n",
    "            x_j = pts_ordered[j, 0]\n",
    "            y_j = pts_ordered[j, 1]\n",
    "            hr = hrs_ordered[j]\n",
    "            # check if the particle is within range\n",
    "            #   xy-grid specific solution\n",
    "            if ray_x - hr < x_j and x_j < ray_x + hr and ray_y - hr < y_j and y_j < ray_y + hr:\n",
    "                nused_i += 1\n",
    "                srcfuncs_tot_relevant += srcfuncs_ordered[j]\n",
    "\n",
    "        \n",
    "        if nused_i <= 0:\n",
    "            continue    # skip\n",
    "\n",
    "        \n",
    "        # Then, get the indexes etc info of these relevant particles\n",
    "\n",
    "        \n",
    "        used_indexes = np.full(nused_i, -1, dtype=np.int64)\n",
    "        used_dtaus   = np.full(nused_i, np.nan)\n",
    "        used_qs_xy   = np.full(nused_i, np.nan)\n",
    "        used_dzs     = np.full(nused_i, np.nan)\n",
    "        tol_tau_base_i = np.log(srcfuncs_tot_relevant) - np.log(rel_tol)    # error tolerance of tau (part 1)\n",
    "\n",
    "        used_j = 0    # j = used_indexes[used_j]\n",
    "        tau    = 0.\n",
    "        rad_est= 0.   # estimation of radiance (i.e. rad)\n",
    "        kc     = 0.\n",
    "        kcs    = 0.   # col kernel cummulative sum\n",
    "        for j in range(npart):\n",
    "            x_j = pts_ordered[j, 0]\n",
    "            y_j = pts_ordered[j, 1]\n",
    "            hr  = hrs_ordered[j]\n",
    "            \n",
    "            # check if the particle is within range\n",
    "            #   xy-grid specific solution\n",
    "            if ray_x - hr < x_j and x_j < ray_x + hr and ray_y - hr < y_j and y_j < ray_y + hr:\n",
    "                r2_xy = (x_j - ray_x)**2 + (y_j - ray_y)**2\n",
    "                dz2 = hr**2 - r2_xy\n",
    "                if dz2 > 0:\n",
    "                    z_j = pts_ordered[j, 2]\n",
    "                    h = hs_ordered[ j]\n",
    "                    q_xy = np.sqrt(r2_xy)/h\n",
    "\n",
    "                    # log\n",
    "                    dz = np.sqrt(dz2)\n",
    "                    mkappa_div_h2 = mkappa_div_h2_ordered[j]\n",
    "                    srcfunc = srcfuncs_ordered[j]\n",
    "                    kc = kernel_col(q_xy, ndim)\n",
    "                    dtau = mkappa_div_h2 * kc\n",
    "                    kcs += kc\n",
    "                            \n",
    "                    used_indexes[used_j] = j\n",
    "                    used_dtaus[  used_j] = dtau\n",
    "                    used_qs_xy[  used_j] = q_xy\n",
    "                    used_dzs[    used_j] = dz\n",
    "                    used_j += 1\n",
    "\n",
    "                    dfac = np.exp(-tau) * (1. - np.exp(-dtau))\n",
    "                    rad_est += dfac * srcfunc\n",
    "                    tau += dtau\n",
    "                    \n",
    "                    # terminate the calc for this ray if tau is sufficient large\n",
    "                    #    such that the relative error on rad is smaller than rel_tol\n",
    "                    # i.e. since when tau > np.log(srcfuncs_ordered.sum()) - np.log(rel_tol) - np.log(rad),\n",
    "                    #    we know that rad[i] - rad[i][k] < rel_tol * rad[i]\n",
    "                    # see my notes for derivation\n",
    "                    #    This should mean tau > photosphere_tau, but let's check it anyway to make sure\n",
    "                    if tau > tol_tau_base_i - np.log(rad_est) and tau > photosphere_tau:\n",
    "                        break\n",
    "        else:\n",
    "            # ptaus[i] = tau\n",
    "            phkcs[i] = kcs    # photosphere not found if didn't break\n",
    "        nused_i = used_j     # update used indexes size\n",
    "        used_dtaus = used_dtaus[:nused_i]\n",
    "        estis[i] = rad_est\n",
    "        \n",
    "        \n",
    "        if nused_i <= 0:\n",
    "            continue    # skip\n",
    "\n",
    "        \n",
    "\n",
    "        # Now, do radiative transfer\n",
    "\n",
    "        z_olim_not_found = True\n",
    "        photosphere_not_found = True\n",
    "        kcs_j  = np.zeros(nsample_pp)\n",
    "        for used_j in range(nused_i):\n",
    "            j    = used_indexes[used_j]\n",
    "            q_xy = used_qs_xy[used_j]\n",
    "            q2_xy= q_xy**2\n",
    "            h    = hs_ordered[ j]\n",
    "            hr   = h * kernel_rad\n",
    "            mkappa_div_h2 = mkappa_div_h2_ordered[j]\n",
    "            srcfunc = srcfuncs_ordered[j]\n",
    "            \n",
    "            z       = pts_ordered[j, 2]\n",
    "            dz      = used_dzs[used_j]\n",
    "            zmdz    = z - dz\n",
    "            zpdz    = z + dz\n",
    "            dzs_ji  = (zpdz - zmdz) / nsample_pp\n",
    "            dqz_j   = dzs_ji / h\n",
    "            # note zs_j is from away from observer to closer to observer\n",
    "            zs_j    = np.linspace(zmdz + dzs_ji/2., zpdz - dzs_ji/2., nsample_pp)\n",
    "            taus_j  = np.zeros(nsample_pp)\n",
    "            # used_pr = 0    # index of the particle index that is 'fully' ahead of j-th particle\n",
    "            tau_pr  = 0.   # \\tau'_j - the summed optical depth for particles that are 'fully' ahead of j-th particle\n",
    "            \n",
    "            dfac  = 0.\n",
    "            dfac_olim = 0.    # same as dfac, except that discounts the part outside z_olim (outmost particle loc) to zero\n",
    "            ddfac = 0.    # temp storage\n",
    "            if z_olim_not_found:\n",
    "                kcs_j[:] = 0\n",
    "            # get optical depth\n",
    "            #assert np.isclose(dzs_ji, zs_j[1] - zs_j[0])\n",
    "            for used_k in range(nused_i):\n",
    "                k = used_indexes[used_k]\n",
    "                z_k  = pts_ordered[k, 2]\n",
    "                dz_k = used_dzs[used_k]\n",
    "                if zmdz - dz_k < z_k:\n",
    "                    # particle being relevant\n",
    "                    if           z_k < zpdz + dz_k:\n",
    "                        # particle range intersects\n",
    "                        h_k = hs_ordered[k]\n",
    "                        mkappa_div_h2_k = mkappa_div_h2_ordered[k]\n",
    "                        q_xy_k = used_qs_xy[used_k] #((x_k - ray_x)**2 + (y_k - ray_y)**2)**0.5 / h_k\n",
    "                        for ji in range(nsample_pp):\n",
    "                            q_z_k  = (zs_j[ji] - z_k) / h_k\n",
    "                            kc     = kernel_csz(q_xy_k, -q_z_k, ndim)\n",
    "                            taus_j[ji] += mkappa_div_h2_k * kc\n",
    "                            if z_olim_not_found:\n",
    "                                kcs_j[ji] += kc\n",
    "                    else:\n",
    "                        # particle is fully ahead\n",
    "                        tau_pr += used_dtaus[used_k]\n",
    "                        if z_olim_not_found:\n",
    "                            q_xy_k = used_qs_xy[used_k]\n",
    "                            kcs_j += kernel_col(q_xy_k, ndim)\n",
    "            # find z_olim\n",
    "            if z_olim_not_found:\n",
    "                if kcs_j[0] > z_olim_kc:    # found\n",
    "                    for ji_r in range(nsample_pp):\n",
    "                        ji = nsample_pp - ji_r - 1\n",
    "                        if kcs_j[ji] > z_olim_kc:\n",
    "                            z_olim = zs_j[ji]\n",
    "                            z_olim_not_found = False\n",
    "                            break\n",
    "\n",
    "            # find photosphere\n",
    "            if photosphere_not_found:\n",
    "                if taus_j[0] > photosphere_tau:    # found\n",
    "                    for ji_r in range(nsample_pp):\n",
    "                        ji = nsample_pp - ji_r - 1\n",
    "                        if taus_j[ji] > photosphere_tau:\n",
    "                            pphzs[i] = zs_j[ji]\n",
    "                            phkcs[i] = kcs_j[ji]\n",
    "                            photosphere_not_found = False\n",
    "                            break\n",
    "            \n",
    "            # integrate through opitcal depth for j\n",
    "            for ji in range(nsample_pp):\n",
    "                # ****** Integration pending improvement ******\n",
    "                # ****** same for the integration in w_csz ******\n",
    "                q_ji = np.sqrt(q2_xy + ((zs_j[ji] - z)/h)**2)\n",
    "                ddfac = np.exp(-taus_j[ji]) * kernel_w(q_ji, ndim) * dqz_j\n",
    "                dfac += ddfac\n",
    "                if not z_olim_not_found and zs_j[ji] < z_olim:\n",
    "                    dfac_olim += ddfac\n",
    "\n",
    "            tmp = mkappa_div_h2 * np.exp(-tau_pr)\n",
    "            dfac *= tmp\n",
    "            dfac_olim *= tmp\n",
    "\n",
    "            # get radiance contribution\n",
    "            fac      += dfac                # for getting <1>\n",
    "            rad      += dfac      * srcfunc # for getting <S>\n",
    "            rad_olim += dfac_olim * srcfunc\n",
    "            jfact[      j] += dfac      * ray_area\n",
    "            jfact_olim[j] += dfac_olim * ray_area\n",
    "            # note down the largest contributor\n",
    "            if dfac > dfac_max_tmp:\n",
    "                dfac_max_tmp = dfac\n",
    "                ind = pts_order[j]\n",
    "        \n",
    "        rads[ i] = rad\n",
    "        rads_olim[i] = rad_olim\n",
    "        indes[i] = ind\n",
    "        if rad > 0: contr[i] = dfac_max_tmp / fac  # dans_max_tmp / rad\n",
    "        # pones[i] = fac\n",
    "    \n",
    "    return rads, rads_olim, phkcs, pphzs, indes, contr, jfact, jfact_olim, estis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a391db2-a336-4e5b-96be-d3c0692e96ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# integrate with error estiamtes\n",
    "\n",
    "def integrate_along_rays_gridxy(\n",
    "    sdf         : sarracen.SarracenDataFrame,\n",
    "    srcfuncs    : npt.NDArray[np.float64],    # (npart,  )-shaped\n",
    "    srcfuncs_err: None|npt.NDArray[np.float64],    # (npart,  )-shaped\n",
    "    rays        : npt.NDArray[np.float64],    # (nray, 2, 3)-shaped\n",
    "    ray_areas   : npt.NDArray[np.float64],    # (nray,     )-shaped\n",
    "    ray_unit_vec: None|npt.NDArray[np.float64] = None,    # (nray, 3)-shaped\n",
    "    kernel      : None|sarracen.kernels.BaseKernel = None,\n",
    "    kernel_col  : None|numba.core.registry.CPUDispatcher = None,\n",
    "    kernel_csz  : None|numba.core.registry.CPUDispatcher = None,\n",
    "    hfact       : None|float = None,\n",
    "    parallel    : bool = False,\n",
    "    err_h       : float = 1.0,\n",
    "    rel_tol     : float = 1e-16,\n",
    "    nsample_pp  : int   = 1000,\n",
    "    z_olim_kc   : float = 1.152,\n",
    "    photosphere_tau: float = 2./3.,\n",
    "    sdf_kdtree  : None|kdtree.KDTree = None,\n",
    "    xyzs_names_list : list = ['x', 'y', 'z'],\n",
    "    verbose     : int = 3,\n",
    "):\n",
    "    \"\"\"Backward integration of source functions along a grided ray (traced backwards), weighted by optical depth.\n",
    "    ---------------------------------------------------------------------------\n",
    "    \n",
    "    Assuming all rays facing +z direction\n",
    "    (with the same ray_unit_vec [0., 0., 1.])\n",
    "\n",
    "    WARNING: will overwrite sdf['srcfunc'] if srcfuncs_err is None.\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sdf: sarracen.SarracenDataFrame\n",
    "        Must contain columns: x, y, z, h, m, kappa\n",
    "        \n",
    "    rays: (nray, 2, 3)-shaped array\n",
    "        Representing the ray trajectory. Currently only straight infinite lines are supported.\n",
    "        each ray is of the format:\n",
    "        [[begin point], [end point]]\n",
    "        where the end point is closer to the observer.\n",
    "\n",
    "    srcfuncs: 1D array\n",
    "        arrays describing the source function for every particle\n",
    "        \n",
    "    kernel: sarracen.kernels.base_kernel\n",
    "        Smoothing kernel for SPH data interpolation.\n",
    "        If None, will use the one in sdf.\n",
    "\n",
    "    kernel_csz: func\n",
    "        Cumulative summed kernel along z axis:\n",
    "        $ \\int_{-w_\\mathrm{rad}}^{q_z} w(\\sqrt{q_{xy}^2 + q_z^2}) dq_z $\n",
    "\n",
    "    hfact : None|float\n",
    "\n",
    "    parallel: bool\n",
    "        If to use the numba parallel function\n",
    "\n",
    "    err_h: float ( > 0. )\n",
    "        determine confidence level.\n",
    "        e.g.,\n",
    "            1.0 will give error assuming error range is +/-1.0 smoothing length h;\n",
    "            0.5 will give error assuming error range is +/-0.5 smoothing length h;\n",
    "            etc. etc.\n",
    "            \n",
    "    rel_tol : float\n",
    "        maximum relative error tolerence per ray.\n",
    "        Default 1e-15 because float64 is only accurate to ~16th digits.\n",
    "        \n",
    "    nsample_pp : int\n",
    "        no of sample points per particle for integration\n",
    "\n",
    "    z_olim_kc : float\n",
    "        col kernel limit for when srcfunc began to count\n",
    "        1.152 because it is where there's an average of 14.5 particles (1/4 N_neigh) on the outer side, as\n",
    "            quad(lambda q_xy: N_neigh/4*w_col(q_xy,3)*q_xy, 0., 2.,) gives 2.304\n",
    "\n",
    "    sdf_kdtree : kdtree.KDTree\n",
    "        KDTree built from sdf[['x', 'y', 'z']], for fast neighbour search.\n",
    "        if None, will build one.\n",
    "        \n",
    "    xyzs_names_list: list\n",
    "        list of names of the columns that represents x, y, z axes (i.e. coord axes names)\n",
    "        MUST INCLUDE ALL THREE AXES LABELS.\n",
    "        If only 2 is included, WILL ASSUME IT IS 2D CACULATIONS.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    lum, lum_err, rads, jfact_used,\n",
    "    lum_olim, lum_err_olim, rads_olim, jfact_olim_used,\n",
    "    estis, phkcs, pphzs, indes, contr, pts_order_used\n",
    "    \n",
    "    rads: np.ndarray\n",
    "        Radiance (i.e. specific intensities) for each ray.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # init\n",
    "    npart : int = len(sdf)\n",
    "    nray  : int = len(rays)\n",
    "    if kernel is None: kernel = sdf.kernel\n",
    "    kernel_rad = float(kernel.get_radius())\n",
    "    #kernel_col = kernel.get_column_kernel_func(samples=1000) # w integrated from z\n",
    "    if kernel_col is None or kernel_csz is None:\n",
    "        kernel_col, kernel_csz, _, _ = get_col_kernel_funcs(kernel)\n",
    "    if ray_unit_vec is None: ray_unit_vec = get_ray_unit_vec(rays[0])\n",
    "    \n",
    "    pts    = np.array(sdf[xyzs_names_list], order='C')    # (npart, 3)-shaped array (must be this shape for pts_order sorting below)\n",
    "    hs     = np.array(sdf[ 'h'           ], order='C')    # npart-shaped array\n",
    "    masses = np.array(sdf[ 'm'           ], order='C')\n",
    "    kappas = np.array(sdf[ 'kappa'       ], order='C')\n",
    "    srcfuncs = np.array(srcfuncs          , order='C')\n",
    "    ndim   = pts.shape[-1]\n",
    "    mkappa_div_h2_arr = masses * kappas / hs**(ndim-1)\n",
    "    \n",
    "    # sanity check\n",
    "    if is_verbose(verbose, 'err') and not np.allclose(ray_unit_vec, get_rays_unit_vec(rays)):\n",
    "        raise ValueError(f\"Inconsistent ray_unit_vec {ray_unit_vec} with the rays.\")\n",
    "\n",
    "    if is_verbose(verbose, 'warn') and ndim != 3:\n",
    "        say('warn', None, verbose, f\"ndim == {ndim} is not 3.\")\n",
    "\n",
    "    if is_verbose(verbose, 'fatal') and not np.allclose(ray_unit_vec, np.array([0., 0., 1.])):\n",
    "        raise NotImplementedError(\n",
    "            f\"Unsupported {ray_unit_vec=}:\"+\n",
    "            \"currently all rays must point towards +z direction (ray_unit_vec = np.array([0., 0., 1.])) \")\n",
    "    # *** warning: the following line only works with +z point rays ***\n",
    "    rays_xy = rays[:, 0, 0:2]\n",
    "\n",
    "    # (npart-shaped array of the indices of the particles from closest to the observer to the furthest)\n",
    "    pts_order             = np.argsort( np.sum(pts * ray_unit_vec, axis=-1) )[::-1]\n",
    "    pts_ordered           = pts[     pts_order]\n",
    "    hs_ordered            = hs[      pts_order]\n",
    "    mkappa_div_h2_ordered = mkappa_div_h2_arr[pts_order]\n",
    "    srcfuncs_ordered      = srcfuncs[pts_order]\n",
    "\n",
    "    # get used particles indexes\n",
    "    if parallel:\n",
    "        rads, rads_olim, phkcs, pphzs, indes, contr, jfact, jfact_olim, estis = _integrate_along_rays_gridxy_sub_parallel_olim(\n",
    "            pts_ordered, hs_ordered, mkappa_div_h2_ordered, srcfuncs_ordered,\n",
    "            rays_xy, ray_areas, kernel_rad, kernel_col, kernel_csz, kernel.w, pts_order,\n",
    "            rel_tol=rel_tol, nsample_pp=nsample_pp, z_olim_kc=z_olim_kc, photosphere_tau=photosphere_tau)\n",
    "    else:\n",
    "        raise NotImplementedError(\"parallel=False version of this function not yet implemented.\")\n",
    "\n",
    "    jused = np.where(jfact)\n",
    "    jfact_used = jfact[jused]\n",
    "    jfact_olim_used = jfact_olim[jused]\n",
    "    pts_order_used = pts_order[jused]\n",
    "    \n",
    "    lum  = 4 * pi * (rads * ray_areas).sum()\n",
    "    lum2 = 4 * pi * (srcfuncs_ordered[jused] * jfact_used).sum()\n",
    "    lum_olim  = 4 * pi * (rads_olim * ray_areas).sum()\n",
    "    lum_olim2 = 4 * pi * (srcfuncs_ordered[jused] * jfact_olim_used).sum()\n",
    "    say('debug', None, verbose,\n",
    "        f\"{lum = }, {lum2 = }    (code unit)\",\n",
    "        f\"{lum_olim = }, {lum_olim2 = }    (code unit)\",\n",
    "        f\"{rads.shape=}, {ray_areas.shape=}\",\n",
    "        f\"{srcfuncs[jused].shape=}, {jfact_used.shape=}\",\n",
    "    )\n",
    "    #assert np.isclose(lum, lum2)\n",
    "\n",
    "    if srcfuncs_err is None:\n",
    "        # calc error of source function now\n",
    "        sdf['srcfunc'] = srcfuncs\n",
    "        srcfuncs_grad_used, nneighs_z_used = get_sph_gradient(\n",
    "            sdf,\n",
    "            val_names   ='srcfunc',\n",
    "            locs        = pts_ordered[     jused],\n",
    "            vals_at_locs= srcfuncs_ordered[jused],\n",
    "            hs_at_locs  = hs_ordered[      jused],\n",
    "            kernel      = kernel,\n",
    "            hfact       = hfact,\n",
    "            sdf_kdtree  = sdf_kdtree,\n",
    "            ndim        = ndim,\n",
    "            xyzs_names_list=xyzs_names_list,\n",
    "            parallel    = parallel,\n",
    "            return_nneighs_z = True,\n",
    "            verbose     = verbose,\n",
    "        )\n",
    "        srcfuncs_grad_used = srcfuncs_grad_used[:, :, 0]    # get_sph_gradient returns a (nlocs, ndim, nvals)-shaped np.ndarray\n",
    "        # srcfuncs_err_used: (nlocs,)-shaped np.ndarray\n",
    "        srcfuncs_err_used = np.sum(srcfuncs_grad_used**2, axis=1)**0.5 * hs_ordered[jused] * err_h\n",
    "        # # put a floor on error range if particle position is poorly resolved\n",
    "        # #    pres_used = poorly_resolved_used\n",
    "        # #    *** this bit only works if ray is pointing towards +z ***\n",
    "        # pres_used = nneighs_z_used[:, 0] < 8\n",
    "        # srcfuncs_err_used = np.where(\n",
    "        #     np.logical_and(pres_used, srcfuncs_ordered[jused] > srcfuncs_err_used),\n",
    "        #     srcfuncs_ordered[jused],    # no need for abs since srcfuncs are already positive\n",
    "        #     srcfuncs_err_used,\n",
    "        # )\n",
    "    else:\n",
    "        srcfuncs_err_used = srcfuncs_err[pts_order_used]\n",
    "        # pres_used = np.zeros(len(jused), dtype=bool)\n",
    "\n",
    "    lum_err = 4 * pi * (((srcfuncs_err_used * jfact_used)**2).sum())**0.5\n",
    "    lum_err_olim = 4 * pi * (\n",
    "        ((srcfuncs_err_used * jfact_olim_used)**2).sum()    # uncertainty from normal inside particles\n",
    "        + (srcfuncs_ordered[jused] * (jfact_used - jfact_olim_used)).sum()**2    # uncertainty from outermost particles\n",
    "    )**0.5\n",
    "\n",
    "    # number of particles before photosphere (based on column kernel) per pixel\n",
    "    pphns = phkcs / AVG_KC_PP\n",
    "\n",
    "    if is_verbose(verbose, 'info'):\n",
    "        nused = len(jfact_used)\n",
    "        say('info', None, verbose,\n",
    "            f\"{nused} particles actually participated calculation\",\n",
    "            f\"({int(nused/npart*10000)/100.}% of all particles,\",\n",
    "            f\"average {int(nused/nray*100)/100.} per ray.)\\n\",\n",
    "            f\"Photosphere resolution {np.average(pphns) = }, {np.std(pphns) = }\\n\",\n",
    "            f\"overall resolution {jfact_used.sum() / np.max(jfact_used) = :.1f}\\n\" if nused else \"\",\n",
    "            f\"overall resolution (olim) {jfact_olim_used.sum() / np.max(jfact_olim_used) = :.1f}\\n\" if nused else \"\",\n",
    "            # f\"Among which, {np.count_nonzero(pres_used)} are poorly resolved (less than 8 neighbours with higher z))\\n\",\n",
    "            sep=' ')\n",
    "\n",
    "    \n",
    "    return (\n",
    "        lum, lum_err, rads, jfact_used,\n",
    "        lum_olim, lum_err_olim, rads_olim, jfact_olim_used,\n",
    "        estis, pphns, pphzs, indes, contr, pts_order_used\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ea50ff-b1e8-4239-9172-0982ef13ede3",
   "metadata": {},
   "source": [
    "### rays grid generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5aa6efe-41e2-4f0a-be7f-b5307c1b7708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xy_grids_of_rays(\n",
    "    sdf  : None|sarracen.SarracenDataFrame = None,\n",
    "    #dXs  : None|list[list[float], list[float]]|np.ndarray= None,\n",
    "    no_xy: tuple[int, int] = (32, 32),\n",
    "    #orig_vec: np.ndarray = np.zeros(3),\n",
    "    frac_contained: float = 100., #99.73,\n",
    "    use_adaptive_grid: bool = False,\n",
    "    xyzs_names_list : list = ['x', 'y', 'z'],\n",
    "    w_rad: None|float = None,\n",
    "    verbose: int = 3,\n",
    ") -> tuple[np.ndarray, np.ndarray, list[np.ndarray]]:\n",
    "    \"\"\"Get a grid of rays (must pointing at z direction (i.e. xyzs_names_list[-1] direction) for now).\n",
    "\n",
    "    Supply either sdf or both dx and dy.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sdf: sarracen.SarracenDataFrame\n",
    "\n",
    "    no_xy: tuple[int, int]\n",
    "        number of the rays per axes.\n",
    "        \n",
    "    frac_contained : float\n",
    "        Suggested percentage of the particle that are contained within the grid. in (0, 100]\n",
    "\n",
    "    use_adaptive_grid : bool\n",
    "        if True,\n",
    "            will scale dXs according to particle distribution instead of even intervals,\n",
    "            if dXs is None or (None, None).\n",
    "\n",
    "    xyzs_names_list: list\n",
    "        list of names of the columns that represents x, y, z axes (i.e. coord axes names)\n",
    "        WARNING: since ndim==len(xyzs_names_list), if len(xyzs_names_list) !=3 will resulting non-3D results.\n",
    "            In which case you will need to change no_xy as well.\n",
    "\n",
    "\n",
    "    Returns: rays, areas, dXs\n",
    "    -------\n",
    "    rays: (no_ray, 2, 3)-shaped np.ndarray\n",
    "\n",
    "    areas: (no_ray)-shaped np.ndarray\n",
    "        areas corresponding to each ray in the grid\n",
    "        \n",
    "    dXs: list of no_xy[i]-shaped np.ndarray\n",
    "        width of the grid cells. in sdf units['dist'].\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    unit_vec = np.zeros(len(xyzs_names_list))\n",
    "    unit_vec[-1] = 1.\n",
    "    #x0, y0, z0 = orig_vec\n",
    "    z0 = 0.    # z value for rays\n",
    "    if w_rad is None:\n",
    "        w_rad = sdf.kernel.get_radius()\n",
    "\n",
    "    # sanity checks\n",
    "    if is_verbose(verbose, 'warn') and len(xyzs_names_list) != 3:\n",
    "        say('warn', 'get_xy_grids_of_rays()', verbose,\n",
    "            f\"xyzs_names_list being {xyzs_names_list}, its len = {len(xyzs_names_list)} is not 3.\",\n",
    "            f\"This means we are assuming {len(xyzs_names_list)}D.\")\n",
    "    if is_verbose(verbose, 'error') and len(xyzs_names_list) != len(no_xy) + 1:\n",
    "        say('error', 'get_xy_grids_of_rays()', verbose,\n",
    "            f\"ndim (=={len(xyzs_names_list)}) != len(no_xy) (=={len(no_xy)}) + 1\",\n",
    "            f\"i.e. asked ray grid dimension {no_xy} does not makes sense.\",\n",
    "            \"This will likely cause error in the next steps.\")\n",
    "    \n",
    "    # get dx & dy\n",
    "    frac_contained_m = 50. - frac_contained / 2.\n",
    "    frac_contained_p = 50. + frac_contained / 2.\n",
    "\n",
    "    Xs_edges = []\n",
    "    for i, label in enumerate(xyzs_names_list[:-1]):\n",
    "        #i0 = orig_vec[i]\n",
    "        if use_adaptive_grid:\n",
    "            # fraction points for the adaptive grid\n",
    "            fracs = np.linspace(frac_contained_m, frac_contained_p, no_xy[i]+1)\n",
    "            # edge points for the grid\n",
    "            Xs_edges.append(\n",
    "                np.percentile(np.concatenate((sdf[label] - w_rad*sdf['h'], sdf[label] + w_rad*sdf['h'])), fracs))\n",
    "        else:\n",
    "            Xs_edges.append(\n",
    "                np.linspace(\n",
    "                    *np.percentile(\n",
    "                        np.concatenate(\n",
    "                            (sdf[label] - w_rad*sdf['h'], sdf[label] + w_rad*sdf['h']),\n",
    "                        ), (frac_contained_m, frac_contained_p),\n",
    "                    ), no_xy[i]+1,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    dXs = np.array([np.diff(Xi_edges) for Xi_edges in Xs_edges])    # each item is (no_xy[i]+1,)-shaped\n",
    "    Xs_centers = np.array([Xi_edges[:-1] + dXi/2. for dXi, Xi_edges in zip(dXs, Xs_edges)])    # each item is (no_xy[i],)-shaped\n",
    "\n",
    "    # Note: orig_vecs must be 2D (i.e. in shape of (no_ray, 3))\n",
    "    orig_vecs = np.array([[*xy, z0] for xy in itertools.product(*Xs_centers)])\n",
    "    #orig_vecs = [[[x, y, z0] for x, y in zip(xs, ys)] for xs, ys in zip(*np.meshgrid(*xys))]\n",
    "    areas = np.array([dx*dy for dy in dXs[1] for dx in dXs[0]])\n",
    "\n",
    "    rays = mupl.geometry.get_rays(orig_vecs=orig_vecs, unit_vecs=unit_vec)\n",
    "    \n",
    "    return rays, areas, dXs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d16cde-9d77-4113-844d-c50637f3a07a",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97c973f4-9f35-4dd6-ba34-7836d5a16625",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_imshow(\n",
    "    no_xy: tuple[int, int],\n",
    "    rays: units.Quantity|np.ndarray,\n",
    "    data: units.Quantity|np.ndarray,\n",
    "    job_profile  : dict= None,\n",
    "    file_index   : int = -1,\n",
    "    title_suffix : str =\"\",\n",
    "    notes        : dict= None,\n",
    "    data_label   : str =\"\",\n",
    "    save_label   : str =\"\",\n",
    "    xyzs         : str|list[str] = 'xyz',\n",
    "    out_exts     : list[str] = ['pdf', 'png'],\n",
    "    norm=None,\n",
    "    cmap=None,\n",
    "    output_dir:str|None=None,\n",
    "    verbose = 4,\n",
    "):\n",
    "    \"\"\"Plotting a heatmap (contourf) of 1D data located at rays\"\"\"\n",
    "\n",
    "\n",
    "    if not isinstance(data, units.Quantity):\n",
    "        data = set_as_quantity(data, units.dimensionless_unscaled)\n",
    "\n",
    "    if not isinstance(rays, units.Quantity):\n",
    "        rays = set_as_quantity(rays, units.dimensionless_unscaled)\n",
    "\n",
    "    if job_profile is None:\n",
    "        job_profile = {\n",
    "            'plot_title_suffix': '',\n",
    "            'nickname'         : '',\n",
    "        }\n",
    "\n",
    "    #Xs = rays[:, 0, 0]\n",
    "    #Ys = rays[:, 0, 1]\n",
    "    rays_val = rays.reshape(*no_xy, *rays.shape[1:]).value\n",
    "    extent = (\n",
    "        rays_val[ 0, 0, 0, 0] - (rays_val[ 1, 0, 0, 0] - rays_val[ 0, 0, 0, 0])/2,\n",
    "        rays_val[-1,-1, 0, 0] + (rays_val[-1,-1, 0, 0] - rays_val[-2,-1, 0, 0])/2,\n",
    "        rays_val[ 0, 0, 0, 1] - (rays_val[ 0, 1, 0, 1] - rays_val[ 0, 0, 0, 1])/2,\n",
    "        rays_val[-1,-1, 0, 1] + (rays_val[-1,-1, 0, 1] - rays_val[-1,-2, 0, 1])/2,\n",
    "    )\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    \n",
    "    cax = ax.imshow(data.reshape(no_xy).T.value, norm=norm, cmap=cmap, origin='lower', extent=extent)\n",
    "    #cax = ax.contourf(Xs.reshape(no_xy), Ys.reshape(no_xy), data.reshape(no_xy), cmap=cmap)\n",
    "    fig.colorbar(cax, label=f\"{data_label} / {data.unit.to_string('latex_inline')}\")\n",
    "    ax.set_xlabel(f\"${xyzs[0]}$ / {rays.unit.to_string('latex_inline')}\")\n",
    "    ax.set_ylabel(f\"${xyzs[1]}$ / {rays.unit.to_string('latex_inline')}\")\n",
    "    if notes is not None:\n",
    "        ax.text(\n",
    "            0.98, 0.98,\n",
    "            f\"Time = {notes['time']:.1f}\\n\" + \\\n",
    "            f\" $L$ = {notes['lum']:.0f}\",\n",
    "            #f\" $L = {notes['lum'].value:.0f}$ {notes['lum'].unit.to_string('latex_inline')}\",\n",
    "            #color = \"black\",\n",
    "            ha = 'right', va = 'top',\n",
    "            transform=ax.transAxes,\n",
    "        )\n",
    "\n",
    "    \n",
    "    if output_dir is not None:\n",
    "        no_xy_txt = 'x'.join([f'{i}' for i in no_xy])\n",
    "        outfilename_noext = f\"{output_dir}heat_{job_profile['nickname']}_{file_index:05d}_{''.join(xyzs)}_{save_label}_{no_xy_txt}\"\n",
    "        outfilenames = []\n",
    "    \n",
    "        # write pdf\n",
    "        for out_ext in out_exts:\n",
    "            outfilename = f\"{outfilename_noext}.{out_ext}\"\n",
    "            if out_ext == 'pdf':\n",
    "                ax.set_title('')\n",
    "            else:\n",
    "                ax.set_title(f\"Heatmap of {data_label}\\n{job_profile['plot_title_suffix']}\")\n",
    "            fig.savefig(outfilename)\n",
    "            outfilenames.append(outfilename)\n",
    "            if is_verbose(verbose, 'note'):\n",
    "                say('note', None, verbose, f\"Fig saved to {outfilename}.\")\n",
    "        \n",
    "    return fig, ax, outfilenames\n",
    "\n",
    "## example\n",
    "#fig, ax = plot_imshow(\n",
    "#    no_xy, rays * units.Rsun, anses, data_label=\"$I$\", save_label=\"I_xyz\",\n",
    "#    job_profile=job_profile, file_index=file_index, output_dir=output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8daf8b-0b46-4deb-80a1-f6d4ad05bc06",
   "metadata": {},
   "source": [
    "### Error estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9cd2dae-0051-430f-b6f4-ee4d84316ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sph_neighbours(\n",
    "    sdf_kdtree : kdtree.KDTree,\n",
    "    xyz_i      : np.ndarray,\n",
    "    h_i        : float,\n",
    "    w_rad      : float,\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Find neighbours of xyz_i within (w_rad*h_i) distance, using k-d tree.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sdf_kdtree : kdtree.KDTree\n",
    "        KDTree built from sdf[['x', 'y', 'z']], for fast neighbour search.\n",
    "    xyz_i : (ndim,)-shaped numpy array\n",
    "        position of the querying point\n",
    "    h_i : float\n",
    "        Smoothing length\n",
    "    w_rad: float\n",
    "        radius of the smoothing kernel w.\n",
    "    \n",
    "    Returns: dists, indices\n",
    "    -------\n",
    "    dists : np.ndarray\n",
    "        distances of the neighbouring points to the querying point\n",
    "    indices : np.ndarray\n",
    "        indices of the neighbouring points\n",
    "    \"\"\"\n",
    "    npart = sdf_kdtree.n\n",
    "    dists, indices = sdf_kdtree.query(xyz_i, k=npart, distance_upper_bound=w_rad*h_i)\n",
    "    indices_indices = indices<npart\n",
    "    return dists[indices_indices], indices[indices_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cc574e5-6910-40f0-b845-66e99c39055b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sph error estimation\n",
    "\n",
    "# found this func in my old codes ../photosphere/Analysis_PhLoc.ipynb\n",
    "\n",
    "def get_sph_error(\n",
    "    sdf        : sarracen.SarracenDataFrame,\n",
    "    target_labels   : str|list[str],\n",
    "    target_indicies : int|list[int]|np.ndarray = [],\n",
    "    err_h      : float    = 1.0,\n",
    "    sdf_kdtree : kdtree.KDTree = None,\n",
    "    kernel     : sarracen.kernels.BaseKernel = None,\n",
    "    xyzs_names_list : list = ['x', 'y', 'z'],\n",
    "    verbose: int = 3,\n",
    ") -> np.ndarray:    # (ntarget, nval)-shaped\n",
    "    \"\"\"Calculate error bar for sarracen data frame.\n",
    "    \n",
    "    Assuming 3D.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sdf: sarracen.SarracenDataFrame\n",
    "        Need to contain columns: x, y, z, m, h, rho.\n",
    "        If density (rho) is not in sdf, will compute rho.\n",
    "        \n",
    "    target_labels: str or list of str (len>2)\n",
    "        Column label of the target data in sdf for error computing\n",
    "        \n",
    "    target_indicies: int or list of int or np.ndarray\n",
    "        indices for particles in sdf for error calculating\n",
    "        \n",
    "    err_h: float ( > 0. )\n",
    "        determine confidence level.\n",
    "        e.g.,\n",
    "            1.0 will give error assuming error range is +/-1.0 smoothing length h;\n",
    "            0.5 will give error assuming error range is +/-0.5 smoothing length h;\n",
    "            etc. etc.\n",
    "            \n",
    "    sdf_kdtree: kdtree.KDTree\n",
    "        KDTree built from sdf[['x', 'y', 'z']], for fast neighbour search.\n",
    "        If None, will build one.\n",
    "        \n",
    "    kernel: sarracen.kernels.base_kernel\n",
    "        Smoothing kernel for SPH data interpolation.\n",
    "        If None, will use the one in sdf.\n",
    "\n",
    "    xyzs_names_list: list\n",
    "        list of names of the columns that represents x, y, z axes (i.e. coord axes names)\n",
    "        MUST INCLUDE ALL THREE AXES LABELS.\n",
    "        If only 2 is included, WILL ASSUME IT IS 2D CACULATIONS.\n",
    "    \n",
    "            \n",
    "    Returns: dvals\n",
    "    -------\n",
    "    dvalsp: (ntarget, nval)-shaped ndarray\n",
    "        error.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # init\n",
    "    \n",
    "    xyzs = sdf[xyzs_names_list].to_numpy()\n",
    "    ms   = sdf['m'   ].to_numpy()\n",
    "    hs   = sdf['h'   ].to_numpy()\n",
    "    rhos = sdf['rho' ].to_numpy()\n",
    "    target_indicies = np.atleast_1d(target_indicies)\n",
    "    dxes = err_h * hs[target_indicies]\n",
    "    # assuming 3D in the following calc\n",
    "    locs = xyzs[target_indicies]\n",
    "    vals =  sdf[ target_labels ].to_numpy()[target_indicies]\n",
    "    if vals.ndim == 2: nval = vals.shape[1]\n",
    "    else:              nval = 1\n",
    "    ntarget = len(target_indicies)\n",
    "    ndim = len(xyzs_names_list)\n",
    "    \n",
    "    if sdf_kdtree is None:\n",
    "        sdf_kdtree = kdtree.KDTree(xyzs)\n",
    "    if kernel is None:\n",
    "        kernel = sdf.kernel\n",
    "    kernel_rad = float(kernel.get_radius())\n",
    "    kernel_w   = kernel.w\n",
    "        \n",
    "    neigh_rad = kernel_rad + err_h\n",
    "    \n",
    "    # ans array\n",
    "    dvals = np.full((ntarget, nval), np.nan)\n",
    "    \n",
    "    \n",
    "    for i in range(ntarget):\n",
    "        loc = locs[i]\n",
    "        val = vals[i]\n",
    "        dx  = dxes[i]\n",
    "        h   = hs[target_indicies[i]]\n",
    "        # find all neighbours within 3h, this includes all points needed for the calc of error for this particle\n",
    "        _, neigh_inds = get_sph_neighbours(sdf_kdtree, loc, h, neigh_rad)\n",
    "\n",
    "        # prepare data\n",
    "        sdf_temp = sdf.iloc[neigh_inds]\n",
    "        \n",
    "        loc_plus_dx = [loc for i in range(ndim*2)]\n",
    "        for j in range(ndim):\n",
    "            loc_plus_dx[j][j] += dx\n",
    "            loc_plus_dx[ndim+j][j] -= dx\n",
    "\n",
    "        dval_xyz = get_sph_interp(sdf_temp, target_labels, loc_plus_dx, kernel=kernel, verbose=0) - val\n",
    "        dvals[i] = ((dval_xyz**2).sum(axis=0)/len(dval_xyz))**0.5\n",
    "    \n",
    "    #dvals = dvals.squeeze()\n",
    "    return dvals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181371ab-d884-447f-a650-1c81b9210e04",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Spectrum Generation (Gray Opacity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e7d3b5a-be7e-47cc-a00c-98009ef03ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spectrum generation\n",
    "\n",
    "\n",
    "def B_vu(freqs: units.Quantity, T: units.Quantity) -> units.Quantity:\n",
    "    return 2 * const.h / const.c**2 * freqs**3 / (np.exp(const.h * freqs / (const.k_B * T)) - 1)\n",
    "\n",
    "def B_wav(wavlens: units.Quantity, T: units.Quantity) -> units.Quantity:\n",
    "    return 2 * const.h * const.c**2 / wavlens**5 / (np.exp(const.h * const.c / (wavlens * const.k_B * T)) - 1)\n",
    "\n",
    "\n",
    "\n",
    "CONST_H = const.h.cgs.value\n",
    "CONST_C = const.c.cgs.value\n",
    "CONST_K_B = const.k_B.cgs.value\n",
    "CONST_SIG = const.sigma_sb.cgs.value\n",
    "\n",
    "@jit(nopython=True, fastmath=True)\n",
    "def B_vu_nb(freqs_Hz: npt.NDArray[float], T_K: float) -> npt.NDArray[float]:\n",
    "    return 2 * CONST_H / CONST_C**2 * freqs**3 / (np.exp(CONST_H * freqs / (CONST_K_B * T_K)) - 1)\n",
    "\n",
    "@jit(nopython=True, fastmath=True)\n",
    "def B_wav_nb(wavlens_cm: npt.NDArray[float], T_K: float) -> npt.NDArray[float]:\n",
    "    return 2 * CONST_H * CONST_C**2 / wavlens_cm**5 / (np.exp(CONST_H * CONST_C / (wavlens_cm * (CONST_K_B * T_K))) - 1)\n",
    "    \n",
    "\n",
    "@jit(nopython=True, fastmath=True, parallel=True)\n",
    "def L_vu_nb(\n",
    "    freqs_Hz : npt.NDArray[float],\n",
    "    Ts_K     : npt.NDArray[float],\n",
    "    Aeffjs_cm2: npt.NDArray[float],\n",
    ") -> npt.NDArray[float]:\n",
    "    \n",
    "    L_vus = np.zeros_like(freqs_Hz)\n",
    "    nused = len(Ts_K)\n",
    "    #debug_fact = np.zeros(nused)\n",
    "    for i in prange(nused):\n",
    "        dL_vus = 4 * pi * B_vu_nb(freqs_Hz, Ts_K[i]) * Aeffjs_cm2[i]\n",
    "        L_vus += dL_vus\n",
    "        ## debug\n",
    "        #dL_sig = 4 * pi * CONST_SIG * Ts_K[i]**4 / pi * Aeffjs_cm2[i]\n",
    "        #dL_int = np.trapezoid(dL_vus, freqs_Hz)\n",
    "        #debug_fact[i] = dL_int/dL_sig\n",
    "        #print(dL_int/dL_sig, Ts_K[i], Aeffjs_cm2[i])\n",
    "    return L_vus#, debug_fact\n",
    "\n",
    "\n",
    "@jit(nopython=True, fastmath=True, parallel=True)\n",
    "def L_wav_nb(\n",
    "    wavlens_cm: npt.NDArray[float],\n",
    "    Ts_K      : npt.NDArray[float],\n",
    "    Aeffjs_cm2 : npt.NDArray[float],\n",
    ") -> npt.NDArray[float]:\n",
    "    \n",
    "    L_wavs= np.zeros_like(wavlens_cm)\n",
    "    nused = len(Ts_K)\n",
    "    for i in prange(nused):\n",
    "        L_wavs += 4 * pi * B_wav_nb(wavlens_cm, Ts_K[i]) * Aeffjs_cm2[i]\n",
    "    return L_wavs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7516fd21-fc56-49d9-80dd-4578615c0a7c",
   "metadata": {},
   "source": [
    ".\n",
    "\n",
    "## Debug\n",
    "\n",
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94abae9a-50dc-4217-bd02-380a42872e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_debug = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b89f446-18c0-4b53-a8c6-6a208b05a055",
   "metadata": {},
   "source": [
    ".\n",
    "\n",
    "## Main\n",
    "\n",
    ".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50dfe709-c9a0-4008-a213-3b98d31b3c37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*   Note   :    <module>() ==> mpdf_read() ==> read():\n",
      "\t\n",
      "\n",
      "\tReading filename='../raw/clmu_2mdd/lumo_00000'\n",
      "\n",
      "\n",
      "    Debug  :    <module>() ==> mpdf_read() ==> read():\n",
      "\tudist = 1.00036 solRad\n",
      "\tumass = 1.00035 solMass\n",
      "\tutime = 1.00053 unit_time\n",
      "\tself.time = np.float64(0.0)\n",
      "self.gamma = np.float64(1.6666666666666667)\n",
      "self.ieos = 10\n",
      "self.total_mass = np.float64(1.6920736802719674)\n",
      "\n",
      "\tCenter of mass location: self.loc_CoM = array([ 0.00323244, -0.00974289, -0.00362086])\n",
      "\n",
      "*   Note   :    mpdf_read() ==> read() ==> reset_xyz_by():\n",
      "\tReseting Origin to R1 ([0. 0. 0.])...\n",
      "*   Note   :    mpdf_read() ==> read() ==> reset_xyz_by():\n",
      "\tCoM location is now [ 0.00323244 -0.00974289 -0.00362086]\n",
      "**  Warning:    <module>() ==> mpdf_read() ==> read():\n",
      "\tkappa column exists.\n",
      "\tWe here assume kappa is in phantom units self.units['opacity']=Unit(\"udist2 / umass\") \n",
      "\tHowever in phantom kappa is assumed to be in cgs unit.\n",
      "\tIf so, please CONVERT KAPPA MANNUALLY into PHANTOM units BEFORE proceeding, e.g.:\n",
      "\t\tmpdf.data['gas']['kappa'] = mupl.units_util.get_val_in_unit(\n",
      "\t\tmpdf.data['gas']['kappa'], units.cm**2/units.g, mpdf.units['opacity'])\n",
      "    Debug  :    run_code() ==> <module>() ==> mpdf_read():\n",
      "\tnp.count_nonzero(mpdf.data['gas']['kappaDust']) = 0\n",
      "\tnp.count_nonzero(mpdf.data['gas']['kappa'])      = 1355745\n",
      "    Note: Density column rho already exist in self.time = np.float64(0.0).\n",
      "Start: 2025-07-30T05:33:57.455725+00:00\n",
      "\tWorking on 2mdd_00000_xyz...\n",
      "    Debug  :    run_code() ==> <module>() ==> integrate_along_rays_gridxy():\n",
      "\tlum = np.float64(3.628398356303982e-07), lum2 = np.float64(3.628398356303982e-07)    (code unit)\n",
      "\tlum_olim = np.float64(5.668346967112512e-41), lum_olim2 = np.float64(5.668346967112512e-41)    (code unit)\n",
      "\trads.shape=(256,), ray_areas.shape=(256,)\n",
      "\tsrcfuncs[jused].shape=(1889,), jfact_used.shape=(1889,)\n",
      "**  Warning:    <module>() ==> integrate_along_rays_gridxy() ==> get_sph_gradient_phantom():\n",
      "\tOnly half of the process is parallelized. Pending improvements.\n",
      "    Debug  :    run_code() ==> <module>() ==> integrate_along_rays_gridxy():\n",
      "\t1889 particles actually participated calculation (0.13% of all particles, average 7.37 per ray.)\n",
      " Photosphere resolution np.average(pphns) = np.float64(0.5380972027251307), np.std(pphns) = np.float64(0.6134368091467047)\n",
      " overall resolution jfact_used.sum() / np.max(jfact_used) = 182.9\n",
      " overall resolution (olim) jfact_olim_used.sum() / np.max(jfact_olim_used) = 1.2\n",
      "\n",
      "In Progress: 2025-07-30T05:34:08.059553+00:00\n",
      "Time Used: 0:00:10.603828\n",
      "\n",
      "\n",
      "2mdd_00000_xyz:\n",
      "\n",
      "\tlum = 225593.996 solLum, lum_err = 17135.883 solLum\n",
      "\n",
      "Time = 0.00 yr\n",
      "\n",
      "L_int      = 225594.094 solLum (rel err 0.000 %)\n",
      "\n",
      "L_int_olim = 0.000 solLum (rel err 0.000 %)\n",
      "\n",
      "\n",
      "Lum    : lum1    =    225594.00 solLum +/-     17135.88 solLum (  7.60 %)\n",
      "Low lim: lum_olim=         0.00 solLum +/-    225594.00 solLum ( 640115782847412621812594349994672128.00 %)\n",
      "Old est: lume    =    154519.02 solLum    (rel diff = +37.40 %)\n",
      "\n",
      "    Debug  :    main():\n",
      "\tlum = 225593.99625740873 solLum\n",
      "\tphotosphere T = 7462.073 K +/- 700.852 K\n",
      "\tphotosphere area = ( 70.7%) 5.217 AU2  (radius 1.289 AU)\n",
      "\tphotosphere radius (from z) = 1.273 AU +/- 0.011 AU\n",
      "\ttotal possible area = 7.378 AU2\n",
      "\tlower bound of the # of particles for each ray, weighted avg over lum per pixels = 1.605 \n",
      "\tavg particles above photosphere (based on summed column kernel) per pixels = 0.684 \n",
      "\n",
      "int(rads.size/2)-1 = 127\n",
      "rads[int(rads.size/2)-1].cgs = <Quantity 1.91517892e+10 g / (rad2 s3)>\n",
      "70.703125 % rays hit photosphere\n",
      "np.std(rads[inds]) / np.average(rads[inds]) = <Quantity 0.32697596>\n",
      "lum       = 225594.00 solLum\n",
      "lum_in_ph = 225175.72 solLum    (99.81 %)\n",
      "Ended: 2025-07-30T05:34:12.752994+00:00\n",
      "Time Used: 0:00:15.297269\n",
      "\n",
      "Start: 2025-07-30T05:34:12.753031+00:00\n",
      "\tWorking on 2mdd_00000_xzy...\n",
      "    Debug  :    run_code() ==> <module>() ==> integrate_along_rays_gridxy():\n",
      "\tlum = np.float64(3.730632134624791e-07), lum2 = np.float64(3.730632134624791e-07)    (code unit)\n",
      "\tlum_olim = np.float64(1.6153977346595854e-37), lum_olim2 = np.float64(1.615397734659585e-37)    (code unit)\n",
      "\trads.shape=(256,), ray_areas.shape=(256,)\n",
      "\tsrcfuncs[jused].shape=(1888,), jfact_used.shape=(1888,)\n",
      "**  Warning:    <module>() ==> integrate_along_rays_gridxy() ==> get_sph_gradient_phantom():\n",
      "\tOnly half of the process is parallelized. Pending improvements.\n",
      "    Debug  :    run_code() ==> <module>() ==> integrate_along_rays_gridxy():\n",
      "\t1888 particles actually participated calculation (0.13% of all particles, average 7.37 per ray.)\n",
      " Photosphere resolution np.average(pphns) = np.float64(0.5124673150686975), np.std(pphns) = np.float64(0.5507368299505415)\n",
      " overall resolution jfact_used.sum() / np.max(jfact_used) = 184.5\n",
      " overall resolution (olim) jfact_olim_used.sum() / np.max(jfact_olim_used) = 1.1\n",
      "\n",
      "In Progress: 2025-07-30T05:34:19.554910+00:00\n",
      "Time Used: 0:00:06.801879\n",
      "\n",
      "\n",
      "2mdd_00000_xzy:\n",
      "\n",
      "\tlum = 231950.334 solLum, lum_err = 17899.660 solLum\n",
      "\n",
      "Time = 0.00 yr\n",
      "\n",
      "L_int      = 231950.435 solLum (rel err 0.000 %)\n",
      "\n",
      "L_int_olim = 0.000 solLum (rel err 0.000 %)\n",
      "\n",
      "\n",
      "Lum    : lum1    =    231950.33 solLum +/-     17899.66 solLum (  7.72 %)\n",
      "Low lim: lum_olim=         0.00 solLum +/-    231950.33 solLum ( 230942018462775103373609233547264.00 %)\n",
      "Old est: lume    =    152627.93 solLum    (rel diff = +41.25 %)\n",
      "\n",
      "    Debug  :    main():\n",
      "\tlum = 231950.3343270187 solLum\n",
      "\tphotosphere T = 7543.712 K +/- 592.266 K\n",
      "\tphotosphere area = ( 70.3%) 5.205 AU2  (radius 1.287 AU)\n",
      "\tphotosphere radius (from z) = 1.274 AU +/- 0.009 AU\n",
      "\ttotal possible area = 7.403 AU2\n",
      "\tlower bound of the # of particles for each ray, weighted avg over lum per pixels = 1.529 \n",
      "\tavg particles above photosphere (based on summed column kernel) per pixels = 0.680 \n",
      "\n",
      "int(rads.size/2)-1 = 127\n",
      "rads[int(rads.size/2)-1].cgs = <Quantity 4.34756424e+10 g / (rad2 s3)>\n",
      "70.3125 % rays hit photosphere\n",
      "np.std(rads[inds]) / np.average(rads[inds]) = <Quantity 0.29042008>\n",
      "lum       = 231950.33 solLum\n",
      "lum_in_ph = 231560.64 solLum    (99.83 %)\n",
      "Ended: 2025-07-30T05:34:19.758093+00:00\n",
      "Time Used: 0:00:07.005062\n",
      "\n",
      "Start: 2025-07-30T05:34:19.758124+00:00\n",
      "\tWorking on 2mdd_00000_yzx...\n",
      "    Debug  :    run_code() ==> <module>() ==> integrate_along_rays_gridxy():\n",
      "\tlum = np.float64(3.5185252800827305e-07), lum2 = np.float64(3.5185252800827305e-07)    (code unit)\n",
      "\tlum_olim = np.float64(5.0505967039656544e-26), lum_olim2 = np.float64(5.0505967039656544e-26)    (code unit)\n",
      "\trads.shape=(256,), ray_areas.shape=(256,)\n",
      "\tsrcfuncs[jused].shape=(1863,), jfact_used.shape=(1863,)\n",
      "**  Warning:    <module>() ==> integrate_along_rays_gridxy() ==> get_sph_gradient_phantom():\n",
      "\tOnly half of the process is parallelized. Pending improvements.\n",
      "    Debug  :    run_code() ==> <module>() ==> integrate_along_rays_gridxy():\n",
      "\t1863 particles actually participated calculation (0.13% of all particles, average 7.27 per ray.)\n",
      " Photosphere resolution np.average(pphns) = np.float64(0.551175059048503), np.std(pphns) = np.float64(0.6025666595347436)\n",
      " overall resolution jfact_used.sum() / np.max(jfact_used) = 185.0\n",
      " overall resolution (olim) jfact_olim_used.sum() / np.max(jfact_olim_used) = 1.1\n",
      "\n",
      "In Progress: 2025-07-30T05:34:27.514397+00:00\n",
      "Time Used: 0:00:07.756273\n",
      "\n",
      "\n",
      "2mdd_00000_yzx:\n",
      "\n",
      "\tlum = 218762.688 solLum, lum_err = 17081.472 solLum\n",
      "\n",
      "Time = 0.00 yr\n",
      "\n",
      "L_int      = 218762.783 solLum (rel err 0.000 %)\n",
      "\n",
      "L_int_olim = 0.000 solLum (rel err 0.000 %)\n",
      "\n",
      "\n",
      "Lum    : lum1    =    218762.69 solLum +/-     17081.47 solLum (  7.81 %)\n",
      "Low lim: lum_olim=         0.00 solLum +/-    218762.69 solLum ( 696655362983157301248.00 %)\n",
      "Old est: lume    =    152606.90 solLum    (rel diff = +35.63 %)\n",
      "\n",
      "    Debug  :    main():\n",
      "\tlum = 218762.68836014255 solLum\n",
      "\tphotosphere T = 7407.741 K +/- 653.399 K\n",
      "\tphotosphere area = ( 71.1%) 5.236 AU2  (radius 1.291 AU)\n",
      "\tphotosphere radius (from z) = 1.272 AU +/- 0.010 AU\n",
      "\ttotal possible area = 7.365 AU2\n",
      "\tlower bound of the # of particles for each ray, weighted avg over lum per pixels = 1.633 \n",
      "\tavg particles above photosphere (based on summed column kernel) per pixels = 0.718 \n",
      "\n",
      "int(rads.size/2)-1 = 127\n",
      "rads[int(rads.size/2)-1].cgs = <Quantity 2.8622839e+10 g / (rad2 s3)>\n",
      "71.09375 % rays hit photosphere\n",
      "np.std(rads[inds]) / np.average(rads[inds]) = <Quantity 0.31820322>\n",
      "lum       = 218762.69 solLum\n",
      "lum_in_ph = 218460.51 solLum    (99.86 %)\n",
      "Ended: 2025-07-30T05:34:27.768528+00:00\n",
      "Time Used: 0:00:08.010404\n",
      "\n",
      "*   Note   :    run_code() ==> <module>() ==> hdf5_dump():\n",
      "\tWriting to ../interm/olim_lcgen.16x16.2mdd.tmp.hdf5  (will OVERWRITE if file already exist.; compress=False)\n",
      "*   Note   :    <module>() ==> mpdf_read() ==> read():\n",
      "\t\n",
      "\n",
      "\tReading filename='../raw/clmu_2mdd/lumo_00010'\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../raw/clmu_2mdd/lumo_00010'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 53\u001b[39m\n\u001b[32m     13\u001b[39m comb[job_nickname] = {\n\u001b[32m     14\u001b[39m     xyzs: {\n\u001b[32m     15\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mtimes\u001b[39m\u001b[33m'\u001b[39m: np.full(\u001b[38;5;28mlen\u001b[39m(file_indexes), np.nan) * units.yr,\n\u001b[32m   (...)\u001b[39m\u001b[32m     46\u001b[39m     } \u001b[38;5;28;01mfor\u001b[39;00m xyzs \u001b[38;5;129;01min\u001b[39;00m xyzs_list\n\u001b[32m     47\u001b[39m }\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ifile, file_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(file_indexes):\n\u001b[32m     51\u001b[39m     \u001b[38;5;66;03m# init\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m     mpdf = \u001b[43mmpdf_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meos_opacity\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset_xyz_by\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mR1\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_Tscales\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_Tscales\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m     mpdf.calc_sdf_params([\u001b[33m'\u001b[39m\u001b[33mR1\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     55\u001b[39m     sdf  = mpdf.data[\u001b[33m'\u001b[39m\u001b[33mgas\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/d/Sync_OneDrive/Macquarie University/Project RT in CEE - Docs/Projects/20230201/scripts/_sharedFuncs.py:88\u001b[39m, in \u001b[36mmpdf_read\u001b[39m\u001b[34m(job_name, file_index, eos_opacity, mpdf, params, reset_xyz_by, calc_params, kappa_gas, kappa_tol, T_cond_oxy, do_extrap, use_Tscales, verbose)\u001b[39m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mpdf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     86\u001b[39m     mpdf = MyPhantomDataFrames()\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m \u001b[43mmpdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset_xyz_by\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset_xyz_by\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcalc_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcalc_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m eos_opacity \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     91\u001b[39m     eos_opacity = get_eos_opacity(ieos=mpdf.ieos, params=params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/d/Sync_OneDrive/Macquarie University/Project RT in CEE - Docs/Projects/20230201/scripts/clmuphantomlib/mpdf.py:160\u001b[39m, in \u001b[36mMyPhantomDataFrames.read\u001b[39m\u001b[34m(self, job_name, file_index, calc_params, calc_params_params, reset_xyz_by, verbose, reset_xyz_by_CoM)\u001b[39m\n\u001b[32m    157\u001b[39m     say(\u001b[33m'\u001b[39m\u001b[33mnote\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m, verbose, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33mReading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    159\u001b[39m \u001b[38;5;66;03m# read\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m \u001b[38;5;28mself\u001b[39m.sdfs = \u001b[43msarracen\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_phantom\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.sdfs, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[32m    162\u001b[39m     \u001b[38;5;28mself\u001b[39m.sdfs = \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mself\u001b[39m.sdfs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/d/Sync_OneDrive/Macquarie University/Project RT in CEE - Docs/Projects/20230201/scripts/sarracen/readers/read_phantom.py:276\u001b[39m, in \u001b[36mread_phantom\u001b[39m\u001b[34m(filename, separate_types, ignore_inactive)\u001b[39m\n\u001b[32m    226\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread_phantom\u001b[39m(filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    227\u001b[39m                  separate_types: \u001b[38;5;28mstr\u001b[39m = \u001b[33m'\u001b[39m\u001b[33msinks\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    228\u001b[39m                  ignore_inactive: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m    229\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    230\u001b[39m \u001b[33;03m    Read data from a Phantom dump file.\u001b[39;00m\n\u001b[32m    231\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    274\u001b[39m \u001b[33;03m    >>> sdf_gas, sdf_dust, sdf_sinks = sarracen.read_phantom('dumpfile_00000', separate_types='all')\u001b[39;00m\n\u001b[32m    275\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m276\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[32m    277\u001b[39m         def_int_dtype, def_real_dtype, iversion = _read_capture_pattern(fp)\n\u001b[32m    278\u001b[39m         file_identifier = _read_file_identifier(fp)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../raw/clmu_2mdd/lumo_00010'"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__' and not do_debug:\n",
    "\n",
    "    # init combined data\n",
    "    comb = {}\n",
    "    \n",
    "    for job_nickname in job_nicknames: #['2md', ]:  #\n",
    "        job_profile = JOB_PROFILES_DICT[job_nickname]\n",
    "        job_name    = job_profile['job_name']\n",
    "        file_indexes= job_profile['file_indexes']  #[17600,]  #\n",
    "        params      = job_profile['params']\n",
    "        eos_opacity = get_eos_opacity(ieos=10, params=params)    #EoS_MESA_opacity(params, settings)\n",
    "        \n",
    "        comb[job_nickname] = {\n",
    "            xyzs: {\n",
    "                'times': np.full(len(file_indexes), np.nan) * units.yr,\n",
    "                'lums' : np.full(len(file_indexes), np.nan) * units.Lsun,\n",
    "                'lums_err': np.full(len(file_indexes), np.nan) * units.Lsun,\n",
    "                'lums_olim' : np.full(len(file_indexes), np.nan) * units.Lsun,\n",
    "                'lums_err_olim': np.full(len(file_indexes), np.nan) * units.Lsun,\n",
    "                'Teffs'  : np.full(len(file_indexes), np.nan) * units.K,\n",
    "                'Aphs_xy': np.full(len(file_indexes), np.nan) * units.au**2,\n",
    "                'Rphs_z' : np.full(len(file_indexes), np.nan) * units.au,\n",
    "                # no of particles at the photosphere - lower bound (weighted average per pixel, weighted by lums contribution)\n",
    "                # i.e. how resolved the photosphere is\n",
    "                'N_res': np.full(len(file_indexes), -1) * units.dimensionless_unscaled,\n",
    "                'Nphs_kcs': np.full(len(file_indexes), -1) * units.dimensionless_unscaled,\n",
    "                'wavlens': wavlens,\n",
    "                'L_wavs': np.full((len(file_indexes), len(wavlens)), np.nan) * (units.Lsun/units.angstrom),\n",
    "                'L_wavs_olim': np.full((len(file_indexes), len(wavlens)), np.nan) * (units.Lsun/units.angstrom),\n",
    "                'params': {'PHOTOSPHERE_TAU': PHOTOSPHERE_TAU, 'z_olim_kc': z_olim_kc, **params},\n",
    "                '_meta_': {\n",
    "                    'lums' : { 'Description': \"Luminosity.\", },\n",
    "                    'lums_olim' : { 'Description': \"Luminosity, but ignoring source function of particles outside the surface\"\n",
    "                                    \"(defined by summed column kernel kcs < z_olim_kc.\", },\n",
    "                    'Aphs_xy': { 'Description': (\n",
    "                        \"Visible size of the simulated object (i.e. pixel * (area per pixel) if tau > PHOTOSPHERE_TAU).\"), },\n",
    "                    'Rphs_z': { 'Description': (\n",
    "                        \"Visible size of the simulated object (i.e. pixel * (area per pixel)).\"), },\n",
    "                    'N_res': { 'Description': (\n",
    "                            \"no of particles contributed for each ray - lower bound\"\n",
    "                            \"(weighted average per pixel, weighted by lums contribution per pixel)\"), },\n",
    "                    'Nphs_kcs': { 'Description': (\n",
    "                            \"no of particles above the photosphere\"\n",
    "                            \"Summed column kernel divided by expected average column kernel per particle\"), },\n",
    "                },\n",
    "            } for xyzs in xyzs_list\n",
    "        }\n",
    "\n",
    "            \n",
    "        for ifile, file_index in enumerate(file_indexes):\n",
    "            # init\n",
    "    \n",
    "            mpdf = mpdf_read(job_name, file_index, eos_opacity, reset_xyz_by='R1', use_Tscales=use_Tscales, verbose=verbose)\n",
    "            mpdf.calc_sdf_params(['R1'])\n",
    "            sdf  = mpdf.data['gas']\n",
    "            # kernel = sdf.kernel\n",
    "            # kernel_rad = float(kernel.get_radius())\n",
    "            # col_kernel = kernel.get_column_kernel_func(samples=1000)\n",
    "            srcfuncs = mpdf.const['sigma_sb'] * sdf['T']**4 / pi\n",
    "            sdf['srcfunc'] = srcfuncs\n",
    "\n",
    "            with mupl.hdf5_open(\n",
    "                f\"{interm_dir}{job_nickname}_{file_index:05d}.lcgen.{no_xy_txt}.hdf5\",\n",
    "                'a', metadata) as out_interm_grp1:\n",
    "                #out_interm_grp1 = mupl.hdf5_subgroup(out_interm_file, f\"{file_index:05d}\", {})\n",
    "\n",
    "                for xyzs in xyzs_list:\n",
    "                    xyzs_names_list = [x for x in xyzs]\n",
    "        \n",
    "                    # record time used\n",
    "                    python_time_start = now()\n",
    "                    print(f\"Start: {python_time_start.isoformat()}\")\n",
    "                    print(f\"\\tWorking on {job_nickname}_{file_index:05d}_{xyzs}...\")\n",
    "        \n",
    "                    \n",
    "                    # get rays\n",
    "                    rays, areas, dXs = get_xy_grids_of_rays(\n",
    "                        sdf, no_xy=no_xy, frac_contained=100.,\n",
    "                        use_adaptive_grid=False, xyzs_names_list=xyzs_names_list)\n",
    "                    ray_areas = areas\n",
    "                    pts    = np.array(sdf[xyzs_names_list])\n",
    "                    hs     = np.array(sdf[ 'h' ])    # npart-shaped array\n",
    "                    \n",
    "                    rays_u = (rays * mpdf.units['dist']).to(units.au)\n",
    "                    areas_u = (areas * mpdf.units['dist']**2).to(units.au**2)\n",
    "        \n",
    "                    \n",
    "                    # do integration without error estimation\n",
    "                    srcfuncs = np.array(srcfuncs)\n",
    "                    srcfuncs_err = None # ask to re-calc below\n",
    "                    (\n",
    "                        lum, lum_err, rads, jfact_used,\n",
    "                        lum_olim, lum_err_olim, rads_olim, jfact_olim_used,\n",
    "                        estis, pphns, pphzs, indes, contr, pts_order_used,\n",
    "                    ) = integrate_along_rays_gridxy(\n",
    "                        sdf, srcfuncs, srcfuncs_err, rays, ray_areas,\n",
    "                        nsample_pp=nsample_pp, z_olim_kc=z_olim_kc, photosphere_tau=PHOTOSPHERE_TAU,\n",
    "                        xyzs_names_list=xyzs_names_list, parallel=True, verbose=verbose,\n",
    "                    )\n",
    "\n",
    "                    \n",
    "                    # record time used\n",
    "                    python_time_ended = now()\n",
    "                    python_time__used  = python_time_ended - python_time_start\n",
    "                    print(f\"In Progress: {python_time_ended.isoformat()}\\nTime Used: {python_time__used}\\n\")\n",
    "\n",
    "                    \n",
    "                    lum     = set_as_quantity(lum,     mpdf.units['lum']).to(units.Lsun)\n",
    "                    lum_err = set_as_quantity(lum_err, mpdf.units['lum']).to(units.Lsun)\n",
    "                    lum_olim     = set_as_quantity(lum_olim,     mpdf.units['lum']).to(units.Lsun)\n",
    "                    lum_err_olim = set_as_quantity(lum_err_olim, mpdf.units['lum']).to(units.Lsun)\n",
    "                    print(f\"\\n{job_nickname}_{file_index:05d}_{xyzs}:\\n\\n\\t{lum = :.3f}, {lum_err = :.3f}\\n\")\n",
    "                    print(f\"Time = {mpdf.get_time(unit=units.yr):.2f}\\n\")\n",
    "\n",
    "                    \n",
    "                    # SEDs\n",
    "                    Ts      = set_as_quantity(sdf['T'].iloc[pts_order_used], mpdf.units['temp']).cgs\n",
    "                    Aeffjs      = set_as_quantity(jfact_used,      mpdf.units['dist']**2).cgs\n",
    "                    Aeffjs_olim = set_as_quantity(jfact_olim_used, mpdf.units['dist']**2).cgs\n",
    "                    L_wavs      = L_wav_nb(wavlens.cgs.value, Ts.cgs.value, Aeffjs.cgs.value     ) * (units.erg/units.s/units.cm)\n",
    "                    L_wavs_olim = L_wav_nb(wavlens.cgs.value, Ts.cgs.value, Aeffjs_olim.cgs.value) * (units.erg/units.s/units.cm)\n",
    "                    L_wavs      = L_wavs.to(     units.Lsun / units.angstrom)\n",
    "                    L_wavs_olim = L_wavs_olim.to(units.Lsun / units.angstrom)\n",
    "                    \n",
    "                    L_int = np.trapezoid(L_wavs, wavlens).to(units.Lsun)\n",
    "                    L_int_olim = np.trapezoid(L_wavs_olim, wavlens).to(units.Lsun)\n",
    "                    print(f\"{L_int      = :.3f} (rel err {(L_int/lum-1.).to(units.percent):.3f})\\n\")\n",
    "                    print(f\"{L_int_olim = :.3f} (rel err {(L_int_olim/lum_olim-1.).to(units.percent):.3f})\\n\")\n",
    "                    print()\n",
    "\n",
    "\n",
    "                    rads      = (rads      * mpdf.units['sigma_sb'] * mpdf.units['temp']**4 / units.sr).cgs\n",
    "                    rads_olim = (rads_olim * mpdf.units['sigma_sb'] * mpdf.units['temp']**4 / units.sr).cgs\n",
    "                    indes *= units.dimensionless_unscaled\n",
    "                    contr = 100 * contr * units.percent\n",
    "                    lum1  = ((4 * pi * units.sr) * (rads * areas_u)).sum().to(units.solLum)\n",
    "                    print(f\"Lum    : {lum1    = :12.2f} +/- {lum_err     :12.2f} ({(lum_err / lum1).to(units.percent): 6.2f})\")\n",
    "                    print(f\"Low lim: {lum_olim= :12.2f} +/- {lum_err_olim:12.2f} ({(lum_err_olim/lum_olim).to(units.percent): 6.2f})\")\n",
    "                    estis  = (estis * mpdf.units['sigma_sb'] * mpdf.units['temp']**4 / units.sr).cgs\n",
    "                    lume = ((4 * pi * units.sr) * (estis * areas_u)).sum().to(units.solLum)\n",
    "                    print(f\"Old est: {lume    = :12.2f}\" +\n",
    "                          f\"    (rel diff = {(2 * (lum1 - lume) / (lum1 + lume)).to(units.percent):+6.2f})\")\n",
    "                    print()\n",
    "\n",
    "\n",
    "                    try:\n",
    "                        mask = np.logical_and(~np.isnan(contr), contr.value)\n",
    "                        N_res = np.average(\n",
    "                            1. / contr[mask], weights=(rads*areas_u)[mask].value,\n",
    "                        ).to(units.dimensionless_unscaled)\n",
    "                    except ZeroDivisionError:\n",
    "                        N_res = 0.\n",
    "\n",
    "                    # Aeffis= pones * areas_u\n",
    "                    # area  = Aeffis.sum()\n",
    "                    Zphs  = (pphzs * mpdf.units['dist']).to(units.au)\n",
    "                    iphs = ~np.isnan(Zphs)\n",
    "                    Nphs_kcs = pphns[iphs] * units.dimensionless_unscaled\n",
    "                    Nph_kcs = np.average(Nphs_kcs)\n",
    "                    Aph_xy= (iphs * areas_u).sum()\n",
    "                    Teffs = ((rads * pi * units.sr / const.sigma_sb) ** 0.25).cgs\n",
    "                    Teffs_olim = ((rads_olim * pi * units.sr / const.sigma_sb) ** 0.25).cgs\n",
    "                    Teff = np.average(Teffs[iphs])\n",
    "                    Rph_xy = (Aph_xy/pi)**0.5\n",
    "                    Rphs_z = np.sqrt(Zphs**2 + np.sum(rays_u[:, 0, :2]**2, axis=1))\n",
    "                    Rph_z = np.average(Rphs_z[iphs])\n",
    "                    \n",
    "    \n",
    "                    if is_verbose(verbose, 'info'):\n",
    "                        say('info', 'main()', verbose,\n",
    "                            f\"lum = {lum}\",\n",
    "                            # f\"area (from <1>) = ({area  /areas_u.sum()*100: 5.1f}%) {area}\",\n",
    "                            f\"photosphere T = {Teff:.3f} +/- {np.std(Teffs[iphs]):.3f}\",\n",
    "                            f\"photosphere area = ({Aph_xy/areas_u.sum()*100: 5.1f}%) {Aph_xy:.3f}  (radius {Rph_xy:.3f})\",\n",
    "                            f\"photosphere radius (from z) = {Rph_z:.3f} +/- {np.std(Rphs_z[iphs]):.3f}\",\n",
    "                            f\"total possible area = {areas_u.sum():.3f}\",\n",
    "                            f\"lower bound of the # of particles for each ray, weighted avg over lum per pixels = {N_res:.3f} \",\n",
    "                            f\"avg particles above photosphere (based on summed column kernel) per pixels = {Nph_kcs:.3f} \",\n",
    "                        )\n",
    "                \n",
    "                    # save interm data\n",
    "                    data = {}\n",
    "                    data['lum'  ] = lum\n",
    "                    data['lum_err' ] = lum_err\n",
    "                    data['lum_olim'  ] = lum_olim\n",
    "                    data['lum_err_olim' ] = lum_err_olim\n",
    "                    data['Rph_z'] = Rph_z\n",
    "                    data['Rph_xy'] = Rph_xy\n",
    "                    data['Aph_xy'] = Aph_xy\n",
    "                    data['Teff'] = Teff\n",
    "                    data['N_res'] = N_res\n",
    "                    data['Nph_kcs'] = Nph_kcs\n",
    "                    data['Zphs'] = Zphs\n",
    "                    data['Teffs'] = Teffs\n",
    "                    data['Nphs_kcs'] = Nphs_kcs\n",
    "                    data['xyzs' ] = xyzs\n",
    "                    data['time' ] = mpdf.get_time()\n",
    "                    data['mpdf_params'] = mpdf.params\n",
    "                    data['rays' ] = rays_u[:, 0, :2]\n",
    "                    data['ray_unit_vec'] = get_ray_unit_vec(rays_u[0].value)\n",
    "                    data['area_per_ray'] = areas_u[0] #areas_u\n",
    "                    data['rads' ] = rads\n",
    "                    data['rads_olim' ] = rads_olim\n",
    "                    data['contr'] = contr\n",
    "                    data['wavlens'] = wavlens\n",
    "                    data['L_wavs'] = L_wavs\n",
    "                    data['L_wavs_olim'] = L_wavs_olim\n",
    "                    # data['Aeffis'] = Aeffis\n",
    "                    data['Aeffjs'] = Aeffjs\n",
    "                    data['Aeffjs_olim'] = Aeffjs_olim\n",
    "                    \n",
    "                    data['_meta_'] = {\n",
    "                        'N_res': comb[job_nickname][xyzs]['_meta_']['N_res'],\n",
    "                        'rays' : { 'Description': \"Pixel centers on the 2D plane defined by xyzs.\", },\n",
    "                        'rads' : { 'Description': \"Specific intensity per pixel.\", },\n",
    "                        'contr': {\n",
    "                            'Description': \"Maximum contributed particle's contribution towards the specific intensity, per pixel.\", },\n",
    "                    }\n",
    "    \n",
    "                    \n",
    "                    mupl.hdf5_dump(data, mupl.hdf5_subgroup(out_interm_grp1, xyzs, overwrite=True), {})\n",
    "        \n",
    "                    comb[job_nickname][xyzs]['times'][ifile] = data['time']\n",
    "                    comb[job_nickname][xyzs]['lums' ][ifile] = data['lum' ]\n",
    "                    comb[job_nickname][xyzs]['lums_err'][ifile] = data['lum_err']\n",
    "                    comb[job_nickname][xyzs]['lums_olim' ][ifile] = data['lum_olim' ]\n",
    "                    comb[job_nickname][xyzs]['lums_err_olim'][ifile] = data['lum_err_olim']\n",
    "                    comb[job_nickname][xyzs]['Teffs'][ifile] = data['Teff']\n",
    "                    comb[job_nickname][xyzs]['Aphs_xy'][ifile] = data['Aph_xy']\n",
    "                    comb[job_nickname][xyzs]['Rphs_z'][ifile] = data['Rph_z']\n",
    "                    comb[job_nickname][xyzs]['N_res'][ifile] = data['N_res']\n",
    "                    comb[job_nickname][xyzs]['Nphs_kcs'][ifile] = data['Nph_kcs']\n",
    "                    comb[job_nickname][xyzs]['L_wavs'][ifile] = data['L_wavs']\n",
    "                    comb[job_nickname][xyzs]['L_wavs_olim'][ifile] = data['L_wavs_olim']\n",
    "\n",
    "                    # debug\n",
    "                    print()\n",
    "                    print(f\"{int(rads.size/2)-1 = }\\n{rads[int(rads.size/2)-1].cgs = }\")\n",
    "                    inds = ~np.isnan(pphzs)\n",
    "                    print(f\"{np.count_nonzero(inds) / pphzs.size * 100} % rays hit photosphere\")\n",
    "                    print(f\"{np.std(rads[inds]) / np.average(rads[inds]) = }\")\n",
    "                    lum_in_ph = (4*pi*units.sr*(rads[inds] * areas_u[inds]).sum()).to(units.Lsun)\n",
    "                    print(f\"{lum       = :.2f}\\n{lum_in_ph = :.2f}    ({(lum_in_ph / lum).to(units.percent):.2f})\")\n",
    "        \n",
    "                    # record time used\n",
    "                    python_time_ended = now()\n",
    "                    python_time__used  = python_time_ended - python_time_start\n",
    "                    print(f\"Ended: {python_time_ended.isoformat()}\\nTime Used: {python_time__used}\\n\")\n",
    "\n",
    "\n",
    "            # save data for now\n",
    "            mupl.hdf5_dump({job_nickname: comb[job_nickname]}, f\"{interm_dir}lcgen.{no_xy_txt}.{job_nickname}.tmp.hdf5\", metadata)\n",
    "        mupl.hdf5_dump({job_nickname: comb[job_nickname]}, f\"{interm_dir}lcgen.{no_xy_txt}.{job_nickname}.hdf5\", metadata)\n",
    "                \n",
    "    plt.close('all')\n",
    "    mupl.hdf5_dump(comb, f\"{interm_dir}lcgen.{no_xy_txt}.hdf5.gz\", metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8ac991-4aa8-4b05-9714-3398d5427772",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
