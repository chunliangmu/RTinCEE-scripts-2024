{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa711f22-c313-4343-a584-e3efdecd3ad4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Scripts for analyzing of phantom outputs.\\n\\nThis script generate lightcurves (LC) by doing radiative transfer on a grid.\\n\\nAuthor: Chunliang Mu (at Macquarie University, expected duration 2023-2026)\\n\\n\\n-------------------------------------------------------------------------------\\n\\nSide note: Remember to limit line length to 79 characters according to PEP-8\\n    https://peps.python.org/pep-0008/#maximum-line-length    \\nwhich is the length of below line of '-' characters.\\n\\n-------------------------------------------------------------------------------\\n\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Scripts for analyzing of phantom outputs.\n",
    "\n",
    "This script generate lightcurves (LC) by doing radiative transfer on a grid.\n",
    "\n",
    "Author: Chunliang Mu (at Macquarie University, expected duration 2023-2026)\n",
    "\n",
    "\n",
    "-------------------------------------------------------------------------------\n",
    "\n",
    "Side note: Remember to limit line length to 79 characters according to PEP-8\n",
    "    https://peps.python.org/pep-0008/#maximum-line-length    \n",
    "which is the length of below line of '-' characters.\n",
    "\n",
    "-------------------------------------------------------------------------------\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b19be2-030c-4fa9-ade5-f4bb38a0c13a",
   "metadata": {},
   "source": [
    ".\n",
    "\n",
    "# Def\n",
    "\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035afbe0-6134-4121-8416-f0dbbab169fb",
   "metadata": {},
   "source": [
    "## Imports & Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "093b4271-dab0-4b64-8b31-1588fc264966",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from numpy import pi\n",
    "from astropy import units\n",
    "from astropy import constants as const\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from numba import jit\n",
    "import sarracen\n",
    "import itertools\n",
    "from scipy import integrate, fft\n",
    "from scipy.spatial import kdtree\n",
    "# fix weird moviepy cannot find my ffmpeg exe error\n",
    "try: from moviepy import editor\n",
    "except RuntimeError: import os; os.environ[\"IMAGEIO_FFMPEG_EXE\"] = \"/usr/bin/ffmpeg\"\n",
    "#from moviepy.editor import ImageSequenceClip\n",
    "#from os import path\n",
    "\n",
    "from datetime import datetime, UTC\n",
    "now_utc = lambda: datetime.now(UTC)\n",
    "now = now_utc\n",
    "\n",
    "# fix numpy v1.* compatibility\n",
    "try: np.trapezoid\n",
    "except AttributeError: np.trapezoid = np.trapz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c75c48c-a738-4783-a6af-560603d629a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import my modules listed in ./main/\n",
    "\n",
    "import clmuphantomlib as mupl\n",
    "from clmuphantomlib            import MyPhantomDataFrames, get_eos, get_col_kernel_funcs\n",
    "from clmuphantomlib.log        import is_verbose, say\n",
    "#from clmuphantomlib.settings   import DEFAULT_SETTINGS as settings\n",
    "from clmuphantomlib.units_util import get_val_in_unit, set_as_quantity #, get_units_field_name, get_units_cgs\n",
    "from clmuphantomlib.io         import json_dump, json_load\n",
    "from clmuphantomlib.eos        import get_eos_opacity\n",
    "from clmuphantomlib.light      import get_optical_depth_by_ray_tracing_3D, get_photosphere_on_ray\n",
    "#from clmuphantomlib.sph_interp import get_col_kernel_funcs\n",
    "\n",
    "from multiprocessing import cpu_count, Pool #Process, Queue\n",
    "NPROCESSES = 1 if cpu_count() is None else max(cpu_count(), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "859f4f11-9307-4f36-b896-3778f252d65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*   Note   :    script:\n",
      "\tWill use 8 processes for parallelization\n"
     ]
    }
   ],
   "source": [
    "# settings\n",
    "#\n",
    "#   imported from script_input.py file\n",
    "\n",
    "\n",
    "from script_LCGen__input import (\n",
    "    verbose, verbose_loop,\n",
    "    interm_dir, output_dir, JOB_PROFILES_DICT, job_nicknames, xyzs_list, no_xy, no_xy_txt,\n",
    "    unitsOut, PHOTOSPHERE_TAU, wavlens, use_Tscales,\n",
    ")\n",
    "from _sharedFuncs import mpdf_read\n",
    "\n",
    "unitsOutTxt = {  key  : unitsOut[key].to_string('latex_inline') for key in unitsOut.keys() }\n",
    "\n",
    "\n",
    "# set metadata\n",
    "with open(\"_metadata__input.json\", 'r') as f:\n",
    "    metadata = json_load(f)\n",
    "metadata['Title'] = \"Getting light curves by intergrating across a grid of rays\"\n",
    "metadata['Description'] = f\"\"\"Getting light curves by intergrating across a grid of rays with the same directions\n",
    "for dump file data generated by phantom\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "if __name__ == '__main__' and is_verbose(verbose, 'note'):\n",
    "    # remember to check if name is '__main__' if you wanna say anything\n",
    "    #    so when you do multiprocessing the program doesn't freak out\n",
    "    say('note', \"script\", verbose, f\"Will use {NPROCESSES} processes for parallelization\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4853c017-d945-4a40-9098-74dd24e60180",
   "metadata": {},
   "outputs": [],
   "source": [
    "from clmuphantomlib.log import say, is_verbose\n",
    "from clmuphantomlib.geometry import get_dist2_between_2pt, get_closest_pt_on_line\n",
    "from clmuphantomlib.sph_interp import get_sph_interp, get_sph_gradient, get_h_from_rho, get_no_neigh, _get_sph_interp_phantom_np\n",
    "from clmuphantomlib.units_util import set_as_quantity, set_as_quantity_temperature, get_units_field_name\n",
    "from clmuphantomlib.eos import EoS_Base\n",
    "#from clmuphantomlib.light import integrate_along_ray_grid, integrate_along_ray_gridxy\n",
    "\n",
    "#  import (general)\n",
    "import numpy as np\n",
    "from numpy import typing as npt\n",
    "import numba\n",
    "from numba import jit, prange\n",
    "import sarracen\n",
    "\n",
    "from clmuphantomlib.geometry import get_dist2_from_pts_to_line, get_dist2_from_pt_to_line_nb, get_ray_unit_vec, get_rays_unit_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645cc989-5868-4956-9009-e3fd293c8736",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fba0ff6-57ab-4dc7-88d8-874c885a5e45",
   "metadata": {},
   "source": [
    "### LC integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3e20ab-9650-46fc-898f-b676d35540b8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Backup codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8551e10-e248-4947-bd7a-1671298482db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_along_ray(\n",
    "    sdf, ray, srcfuncs, photosphere_tau,\n",
    "    verbose: int = 3,\n",
    "):\n",
    "    pts_on_ray, dtaus, pts_order = get_optical_depth_by_ray_tracing_3D(sdf=sdf, ray=ray)\n",
    "    if is_verbose(verbose, 'debug'):\n",
    "        say('debug', None, verbose, 'optical depth got.')\n",
    "\n",
    "    dtaus_ordered = dtaus[pts_order]\n",
    "    if is_verbose(verbose, 'debug'):\n",
    "        say('debug', None, verbose, 'ordered.')\n",
    "    srcfuncs_ordered = srcfuncs[pts_order]\n",
    "    if is_verbose(verbose, 'debug'):\n",
    "        say('debug', None, verbose, 'srcfuncs_ordered.')\n",
    "    dat_steps = np.full_like(dtaus_ordered, np.nan)\n",
    "\n",
    "    if True:\n",
    "    #if backwards:\n",
    "        # closest to observer to furtherest\n",
    "        dat = 0.\n",
    "        taus_ordered = np.array([0., *np.cumsum(dtaus_ordered[:-1])])\n",
    "        # dat_bwd_inc: dat_backward_contributions\n",
    "        dat_bwd_inc = np.exp(-taus_ordered) * (1 - np.exp(-dtaus_ordered)) * srcfuncs_ordered\n",
    "        dat = np.sum(dat_bwd_inc)\n",
    "        if is_verbose(verbose, 'info'):\n",
    "            say('info', None, verbose,\n",
    "                f\"backward dat = {dat}\")    # debug\n",
    "        if False:\n",
    "            # commented\n",
    "            # get the percentage of contribution to lum from photosphere outwards\n",
    "            photosphere_loc_index = np.searchsorted(taus_ordered, photosphere_tau) - 1\n",
    "            photosphere_contri_percent = np.sum(dat_bwd_inc[:photosphere_loc_index+2]) / dat * 100\n",
    "            dat_percent_index = np.where(np.cumsum(dat_bwd_inc) / dat<0.5)[0][-1]\n",
    "            if is_verbose(verbose, 'info'):\n",
    "                say('info', None, verbose,\n",
    "                    f\"\\tContribution to L from photosphere and outwards is: {photosphere_contri_percent} %\",\n",
    "                    f\"\\t50% Contributed correspond to tau = {taus_ordered[dat_percent_index]} \")\n",
    "        taus_ordered = taus_ordered[::-1]\n",
    "\n",
    "    else:\n",
    "        # furtherest to observer to closest\n",
    "        dat = 0.\n",
    "        #  pts_order[::-1]: reverse pts_order so that the furtherest particles comes first\n",
    "        taus_ordered = np.array([0., *np.cumsum(dtaus_ordered[:-1])])[::-1]\n",
    "        exp_mdtaus_r = np.exp(-dtaus_ordered[::-1])\n",
    "        srcfuncs_ordered_r = srcfuncs_ordered[::-1]\n",
    "        for index, srcfunc in enumerate(srcfuncs_ordered_r):\n",
    "            exp_mdtau = exp_mdtaus_r[index]\n",
    "            dat = exp_mdtau * dat + (1-exp_mdtau) * srcfunc\n",
    "            dat_steps[index] = dat\n",
    "        if is_verbose(verbose, 'info'):\n",
    "            say('info', None, verbose,\n",
    "                f\"forward dat = {dat}\")    # debug\n",
    "        \n",
    "    dtaus_ordered = dtaus_ordered[::-1]\n",
    "    pts_order = pts_order[::-1]  # furtherest to observer to closest\n",
    "    pts_on_ray_ordered = pts_on_ray[pts_order]\n",
    "    \n",
    "    \n",
    "    return  pts_order, pts_on_ray, dtaus_ordered, taus_ordered, \\\n",
    "            dat, dat_steps, dat_bwd_inc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc0e7a4e-ec6f-42a0-b0df-9786ca451f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_error_along_ray(\n",
    "    dtaus_ordered, # furtherest to closest\n",
    "    srcfuncs_ordered, srcfuncs_ordered_errp, srcfuncs_ordered_errm,\n",
    "    photosphere_tau,\n",
    "):\n",
    "    #if backwards:\n",
    "    if True:\n",
    "        \n",
    "        # closest to observer to furtherest\n",
    "        dtaus_ordered = dtaus_ordered[::-1]\n",
    "        \n",
    "        # calc data + error\n",
    "        taus_ordered = np.array([0., *np.cumsum(dtaus_ordered[:-1])])\n",
    "        srcfuncs_errs = np.stack([srcfuncs_ordered, srcfuncs_ordered_errp, srcfuncs_ordered_errm])\n",
    "        srcfuncs_errs = np.flip(srcfuncs_errs, axis=-1) # flip it since we are working backwards\n",
    "        dat_bwd_inc_errs = np.exp(-taus_ordered) * (1 - np.exp(-dtaus_ordered)) * srcfuncs_errs\n",
    "        dat_errs = np.sum(dat_bwd_inc_errs, axis=-1)\n",
    "        \n",
    "        dat_bwd_inc_errs = np.flip(dat_bwd_inc_errs, axis=-1)\n",
    "        \n",
    "        if False:\n",
    "            # get data\n",
    "            dat_bwd_inc = dat_bwd_inc_errs[0]\n",
    "            dat = dat_errs[0]\n",
    "            dat_errp = dat_errs[1]\n",
    "            dat_errm = dat_errs[2]\n",
    "        \n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "        # furtherest to observer to closest\n",
    "        dat = 0.\n",
    "        dat_steps = np.full_like(dtaus_ordered, np.nan)\n",
    "        #  pts_order[::-1]: reverse pts_order so that the furtherest particles comes first\n",
    "        taus_ordered = np.array([0., *np.cumsum(dtaus_ordered[:-1])])[::-1]\n",
    "        exp_mdtaus_r = np.exp(-dtaus_ordered[::-1])\n",
    "        srcfuncs_ordered_r = srcfuncs_ordered[::-1]\n",
    "        for index, srcfunc in enumerate(srcfuncs_ordered_r):\n",
    "            exp_mdtau = exp_mdtaus_r[index]\n",
    "            dat = exp_mdtau * dat + (1-exp_mdtau) * srcfunc\n",
    "            dat_steps[index] = dat\n",
    "        print(\"forward dat = \", dat)    # debug\n",
    "        \n",
    "    #dtaus_ordered = dtaus_ordered[::-1]\n",
    "    return dat_errs, dat_bwd_inc_errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30983d75-44d0-4eec-9ae8-efb05e6cc93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True, parallel=True)\n",
    "def _integrate_along_ray_gridxy_sub_parallel_analysis_old_bkp(\n",
    "    pts_ordered          : np.ndarray,    # (npart, 3)-shaped\n",
    "    hs_ordered           : np.ndarray,    # (npart,  )-shaped\n",
    "    mkappa_div_h2_ordered: np.ndarray,    # (npart,  )-shaped\n",
    "    srcfuncs_ordered     : np.ndarray,    # (npart,  )-shaped\n",
    "    rays                 : np.ndarray,    # (nray, 2, 3)-shaped\n",
    "    kernel_rad           : float,\n",
    "    col_kernel           : numba.core.registry.CPUDispatcher,\n",
    "    pts_order            : np.ndarray,    # (npart,  )-shaped\n",
    "    rel_tol              : float = 1e-16, # because float64 is only has only 16 digits accuracy\n",
    ") -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Sub process for integrate_along_ray_gridxy(). Numba parallel version (using prange).\n",
    "\n",
    "    Unit vec must be [0., 0., 1.] (i.e. all rays must point upwards towards +z).\n",
    "\n",
    "    Private function. Assumes specific input type. See source code comments.\n",
    "\n",
    "    \"\"\"\n",
    "    #raise NotImplementedError\n",
    "\n",
    "    nray  = len(rays)\n",
    "    npart = len(srcfuncs_ordered)\n",
    "    ndim  = pts_ordered.shape[-1]\n",
    "    anses = np.zeros(nray)\n",
    "    indes = np.zeros(nray, dtype=np.int64)    # indexes of max contribution particle\n",
    "    contr = np.zeros(nray)    # contribution of the max contribution particle\n",
    "    jused = np.full(npart, False)    # is j-th particle in the ordered list used for this calculation?\n",
    "\n",
    "    # error tolerance of tau (part 1)\n",
    "    tol_tau_base = np.log(srcfuncs_ordered.sum()) - np.log(rel_tol)\n",
    "\n",
    "    # hr = h * kernel_rad\n",
    "    hrs_ordered = hs_ordered * kernel_rad\n",
    "\n",
    "    # loop over ray\n",
    "    for i in prange(nray):\n",
    "        ray = rays[i]\n",
    "        tau = 0.\n",
    "        ans = 0.\n",
    "        dans= 0.\n",
    "        dans_max_tmp = 0.\n",
    "        ind = -1\n",
    "\n",
    "        #   xy-grid specific solution\n",
    "        ray_x = ray[0, 0]\n",
    "        ray_y = ray[0, 1]\n",
    "        \n",
    "        # loop over particles\n",
    "        #for pt, hr, mkappa_div_h2, srcfunc in zip(\n",
    "        #    pts_ordered, hrs_ordered, mkappa_div_h2_ordered, srcfuncs_ordered):\n",
    "        for j in range(npart):\n",
    "            pt = pts_ordered[j]\n",
    "            hr = hrs_ordered[j]\n",
    "            \n",
    "            # check if the particle is within range\n",
    "            #   general solution\n",
    "            #q = get_dist2_from_pt_to_line_nb(pt, ray)**0.5 / h\n",
    "            #if q < kernel_rad:\n",
    "            #   xy-grid specific solution\n",
    "            if ray_x - hr < pt[0] and pt[0] < ray_x + hr and ray_y - hr < pt[1] and pt[1] < ray_y + hr:\n",
    "                h = hs_ordered[ j]\n",
    "                q = ((pt[0] - ray_x)**2 + (pt[1] - ray_y)**2)**0.5 / h\n",
    "                if q < kernel_rad:\n",
    "\n",
    "                    jused[j] = True\n",
    "                    \n",
    "                    # now do radiative transfer\n",
    "                    \n",
    "                    mkappa_div_h2 = mkappa_div_h2_ordered[j]\n",
    "                    srcfunc = srcfuncs_ordered[j]\n",
    "                    \n",
    "                    dtau = mkappa_div_h2 * col_kernel(q, ndim-1)\n",
    "                    #tau += dtau/2.\n",
    "                    dans = np.exp(-tau) * (1. - np.exp(-dtau)) * srcfunc\n",
    "                    ans += dans\n",
    "                    tau += dtau#/2.\n",
    "\n",
    "                    # note down the largest contributor\n",
    "                    if dans > dans_max_tmp:\n",
    "                        dans_max_tmp = dans\n",
    "                        ind = pts_order[j]\n",
    "    \n",
    "                    # terminate the calc for this ray if tau is sufficient large\n",
    "                    #    such that the relative error on ans is smaller than rel_tol\n",
    "                    # i.e. since when tau > np.log(srcfuncs_ordered.sum()) - np.log(rel_tol) - np.log(ans),\n",
    "                    #    we know that ans[i] - ans[i][k] < rel_tol * ans[i]\n",
    "                    # see my notes for derivation\n",
    "                    if tau > tol_tau_base - np.log(ans):\n",
    "                        break\n",
    "            \n",
    "        anses[i] = ans\n",
    "        indes[i] = ind\n",
    "        contr[i] = dans_max_tmp / ans\n",
    "    \n",
    "    return anses, indes, contr, jused\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2dc4b419-b471-4c9b-8405-2faf75fae819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test runs - Backup\n",
    "@jit(nopython=True, parallel=True)\n",
    "def _integrate_along_ray_gridxy_sub_parallel_analysis_test(\n",
    "    pts_ordered          : npt.NDArray[np.float64],    # (npart, 3)-shaped\n",
    "    hs_ordered           : npt.NDArray[np.float64],    # (npart,  )-shaped\n",
    "    mkappa_div_h2_ordered: npt.NDArray[np.float64],    # (npart,  )-shaped\n",
    "    srcfuncs_ordered     : npt.NDArray[np.float64],    # (npart,  )-shaped\n",
    "    rays                 : npt.NDArray[np.float64],    # (nray, 2, 3)-shaped\n",
    "    kernel_rad           : float,\n",
    "    col_kernel           : numba.core.registry.CPUDispatcher,\n",
    "    pts_order            : npt.NDArray[np.float64],    # (npart,  )-shaped\n",
    "    rel_tol              : float = 1e-16, # because float64 is only has only 16 digits accuracy\n",
    ") -> tuple[\n",
    "    npt.NDArray[np.float64],    # anses\n",
    "    npt.NDArray[np.float64],    # pones\n",
    "    npt.NDArray[np.float64],    # ptaus\n",
    "    npt.NDArray[np.int64  ],    # indes\n",
    "    npt.NDArray[np.float64],    # contr\n",
    "    npt.NDArray[np.bool_  ],    # jused\n",
    "]:\n",
    "    \"\"\"Sub process for integrate_along_ray_gridxy(). Numba parallel version (using prange).\n",
    "\n",
    "    Unit vec must be [0., 0., 1.] (i.e. all rays must point upwards towards +z).\n",
    "\n",
    "    Private function. Assumes specific input type. See source code comments.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    anses, pones, ptaus, indes, contr, jused\n",
    "    \n",
    "    anses: (nray,)-shaped np.ndarray[float]\n",
    "        Radiance (i.e. specific intensities) for each ray.\n",
    "\n",
    "    pones: (nray,)-shaped np.ndarray[float]\n",
    "        <1> for each pixel,\n",
    "        i.e. same integration of radiance but for a constant 'srcfunc' of '1', for each ray.\n",
    "        Helpful for consistency check (should be more or less 1 where ptaus is nan.)\n",
    "        do weighted average by weight of areas per pixel to get the total area of the object!\n",
    "\n",
    "    ptaus: (nray,)-shaped np.ndarray[float]\n",
    "        The optical depth for each pixel\n",
    "        *** WILL BE np.nan IF OPTICAL DEPTH IS DEEP (which will be MOST OF THE TIME.)  ***\n",
    "        can be used as an alternative way to calculate the area of the object.\n",
    "\n",
    "    indes: (nray,)-shaped np.ndarray[int]\n",
    "        indexes of max contribution particle\n",
    "\n",
    "    contr: (nray,)-shaped np.ndarray[float]\n",
    "        relative contribution (in fractions) of the max contribution particle\n",
    "\n",
    "    jused: (npart,)-shaped np.ndarray[bool]\n",
    "        whether j-th particle was used in the calculation.\n",
    "\n",
    "    \"\"\"\n",
    "    #raise NotImplementedError\n",
    "\n",
    "    nray  = len(rays)\n",
    "    npart = len(srcfuncs_ordered)\n",
    "    ndim  = pts_ordered.shape[-1]\n",
    "    anses = np.zeros(nray)\n",
    "    indes = np.zeros(nray, dtype=np.int64)    # indexes of max contribution particle\n",
    "    contr = np.zeros(nray)    # relative contribution of the max contribution particle\n",
    "    jused = np.full( npart, False)    # is j-th particle in the ordered list used for this calculation?\n",
    "    pones = np.zeros(nray)\n",
    "    ptaus = np.full(nray, np.nan)    # lower bound of the optical depth\n",
    "    \n",
    "\n",
    "    # error tolerance of tau (part 1)\n",
    "    tol_tau_base = np.log(srcfuncs_ordered.sum()) - np.log(rel_tol)\n",
    "\n",
    "    # hr = h * kernel_rad\n",
    "    hrs_ordered = hs_ordered * kernel_rad\n",
    "\n",
    "    # loop over ray\n",
    "    for i in prange(nray):\n",
    "        ray = rays[i]\n",
    "        tau = 0.\n",
    "        ans = 0.\n",
    "        dans= 0.\n",
    "        dans_max_tmp = 0.\n",
    "        ind = -1\n",
    "        fac = 0. # effectively <1>\n",
    "        dfac= 0. # factor\n",
    "\n",
    "        #   xy-grid specific solution\n",
    "        ray_x = ray[0, 0]\n",
    "        ray_y = ray[0, 1]\n",
    "        \n",
    "        # loop over particles\n",
    "        #for pt, hr, mkappa_div_h2, srcfunc in zip(\n",
    "        #    pts_ordered, hrs_ordered, mkappa_div_h2_ordered, srcfuncs_ordered):\n",
    "        for j in range(npart):\n",
    "            pt = pts_ordered[j]\n",
    "            hr = hrs_ordered[j]\n",
    "            \n",
    "            # check if the particle is within range\n",
    "            #   general solution\n",
    "            #q = get_dist2_from_pt_to_line_nb(pt, ray)**0.5 / h\n",
    "            #if q < kernel_rad:\n",
    "            #   xy-grid specific solution\n",
    "            if ray_x - hr < pt[0] and pt[0] < ray_x + hr and ray_y - hr < pt[1] and pt[1] < ray_y + hr:\n",
    "                h = hs_ordered[ j]\n",
    "                q = ((pt[0] - ray_x)**2 + (pt[1] - ray_y)**2)**0.5 / h\n",
    "                if q < kernel_rad:\n",
    "\n",
    "                    jused[j] = True\n",
    "                    \n",
    "                    # now do radiative transfer\n",
    "                    \n",
    "                    mkappa_div_h2 = mkappa_div_h2_ordered[j]\n",
    "                    srcfunc = srcfuncs_ordered[j]\n",
    "                    \n",
    "                    dtau = mkappa_div_h2 * col_kernel(q, ndim-1)\n",
    "                    #tau += dtau/2.\n",
    "                    dfac = np.exp(-tau) * (1. - np.exp(-dtau))\n",
    "                    dans = dfac * srcfunc\n",
    "                    ans += dans\n",
    "                    fac += dfac\n",
    "                    tau += dtau#/2.\n",
    "\n",
    "                    # note down the largest contributor\n",
    "                    if dans > dans_max_tmp:\n",
    "                        dans_max_tmp = dans\n",
    "                        ind = pts_order[j]\n",
    "    \n",
    "                    # terminate the calc for this ray if tau is sufficient large\n",
    "                    #    such that the relative error on ans is smaller than rel_tol\n",
    "                    # i.e. since when tau > np.log(srcfuncs_ordered.sum()) - np.log(rel_tol) - np.log(ans),\n",
    "                    #    we know that ans[i] - ans[i][k] < rel_tol * ans[i]\n",
    "                    # see my notes for derivation\n",
    "                    if tau > tol_tau_base - np.log(ans):\n",
    "                        break\n",
    "\n",
    "        else:\n",
    "            ptaus[i]=tau\n",
    "            \n",
    "        anses[i] = ans\n",
    "        indes[i] = ind\n",
    "        contr[i] = dans_max_tmp / ans\n",
    "        pones[i] = fac\n",
    "    \n",
    "    return anses, pones, ptaus, indes, contr, jused\n",
    "\n",
    "\n",
    "_integrate_along_ray_gridxy_sub_parallel_analysis = _integrate_along_ray_gridxy_sub_parallel_analysis_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9aea1f60-0d2b-456f-b542-b87ad2b73105",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True, parallel=True)\n",
    "def _integrate_along_ray_gridxy_sub_parallel_err_ind(\n",
    "    pts_ordered          : np.ndarray,    # (npart, 3)-shaped\n",
    "    hs_ordered           : np.ndarray,    # (npart,  )-shaped\n",
    "    mkappa_div_h2_ordered: np.ndarray,    # (npart,  )-shaped\n",
    "    srcfuncs_ordered     : np.ndarray,    # (npart,  )-shaped\n",
    "    srcfuncs_err_ordered : np.ndarray,    # (npart,  )-shaped\n",
    "    rays                 : np.ndarray,    # (nray, 2, 3)-shaped\n",
    "    kernel_rad           : float,\n",
    "    col_kernel           : numba.core.registry.CPUDispatcher,\n",
    "    pts_order            : np.ndarray,    # (npart,  )-shaped\n",
    "    rel_tol              : float = 1e-16, # because float64 is only has only 16 digits accuracy\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Sub process for integrate_along_ray_gridxy(). Numba parallel version (using prange).\n",
    "\n",
    "    Unit vec must be [0., 0., 1.] (i.e. all rays must point upwards towards +z).\n",
    "\n",
    "    Private function. Assumes specific input type. See source code comments.\n",
    "\n",
    "    \"\"\"\n",
    "    #raise NotImplementedError\n",
    "\n",
    "    nray  = len(rays)\n",
    "    npart = len(srcfuncs_ordered)\n",
    "    ndim  = pts_ordered.shape[-1]\n",
    "    anses = np.zeros(nray)\n",
    "    erres = np.zeros(nray)\n",
    "\n",
    "    # error tolerance of tau (part 1)\n",
    "    tol_tau_base = np.log(srcfuncs_ordered.sum()) - np.log(rel_tol)\n",
    "\n",
    "    # hr = h * kernel_rad\n",
    "    hrs_ordered = hs_ordered * kernel_rad\n",
    "\n",
    "    # loop over ray\n",
    "    for i in prange(nray):\n",
    "        ray = rays[i]\n",
    "        tau = 0.\n",
    "        ans = 0.\n",
    "        err = 0.\n",
    "        dans= 0.\n",
    "\n",
    "        #   xy-grid specific solution\n",
    "        ray_x = ray[0, 0]\n",
    "        ray_y = ray[0, 1]\n",
    "        \n",
    "        # loop over particles\n",
    "        #for pt, hr, mkappa_div_h2, srcfunc in zip(\n",
    "        #    pts_ordered, hrs_ordered, mkappa_div_h2_ordered, srcfuncs_ordered):\n",
    "        for j in range(npart):\n",
    "            pt = pts_ordered[j]\n",
    "            hr = hrs_ordered[j]\n",
    "            \n",
    "            # check if the particle is within range\n",
    "            #   general solution\n",
    "            #q = get_dist2_from_pt_to_line_nb(pt, ray)**0.5 / h\n",
    "            #if q < kernel_rad:\n",
    "            #   xy-grid specific solution\n",
    "            if ray_x - hr < pt[0] and pt[0] < ray_x + hr and ray_y - hr < pt[1] and pt[1] < ray_y + hr:\n",
    "                h = hs_ordered[ j]\n",
    "                q = ((pt[0] - ray_x)**2 + (pt[1] - ray_y)**2)**0.5 / h\n",
    "                if q < kernel_rad:\n",
    "                    \n",
    "                    # now do radiative transfer\n",
    "                    \n",
    "                    mkappa_div_h2 = mkappa_div_h2_ordered[j]\n",
    "                    srcfunc = srcfuncs_ordered[j]\n",
    "                    srcfunc_err = srcfuncs_err_ordered[j]\n",
    "\n",
    "                    dtau = mkappa_div_h2 * col_kernel(q, ndim-1)\n",
    "                    #tau += dtau/2.\n",
    "                    dans = np.exp(-tau) * (1. - np.exp(-dtau)) * srcfunc\n",
    "                    err += np.exp(-tau) * (1. - np.exp(-dtau)) * srcfunc_err\n",
    "                    ans += dans\n",
    "                    tau += dtau#/2.\n",
    "    \n",
    "                    # terminate the calc for this ray if tau is sufficient large\n",
    "                    #    such that the relative error on ans is smaller than rel_tol\n",
    "                    # i.e. since when tau > np.log(srcfuncs_ordered.sum()) - np.log(rel_tol) - np.log(ans),\n",
    "                    #    we know that ans[i] - ans[i][k] < rel_tol * ans[i]\n",
    "                    # see my notes for derivation\n",
    "                    if tau > tol_tau_base - np.log(ans):\n",
    "                        break\n",
    "            \n",
    "        anses[i] = ans\n",
    "        erres[i] = err\n",
    "    \n",
    "    return anses, erres\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d1cec59-4709-4421-9750-0ef81a11e9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# integrate only, no error estiamtes\n",
    "\n",
    "def integrate_along_ray_gridxy_ind(\n",
    "    sdf     : sarracen.SarracenDataFrame,\n",
    "    srcfuncs: np.ndarray,\n",
    "    rays    : np.ndarray,\n",
    "    ray_unit_vec: np.ndarray|None = None,\n",
    "    kernel  : sarracen.kernels.BaseKernel = None,\n",
    "    parallel: bool = False,\n",
    "    err_h   : float = 1.0,\n",
    "    rel_tol : float = 1e-15,\n",
    "    sdf_kdtree : kdtree.KDTree = None,\n",
    "    xyzs_names_list : list = ['x', 'y', 'z'],\n",
    "    verbose : int = 3,\n",
    ") -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Backward integration of source functions along a grided ray (traced backwards), weighted by optical depth.\n",
    "    \n",
    "    Assuming all rays facing +z direction. (with the same ray_unit_vec [0., 0., 1.])\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sdf: sarracen.SarracenDataFrame\n",
    "        Must contain columns: x, y, z, h, m, kappa\n",
    "        \n",
    "    rays: (nray, 2, 3)-shaped array\n",
    "        Representing the ray trajectory. Currently only straight infinite lines are supported.\n",
    "        each ray is of the format:\n",
    "        [[begin point], [end point]]\n",
    "        where the end point is closer to the observer.\n",
    "\n",
    "    srcfuncs: 1D array\n",
    "        arrays describing the source function for every particle\n",
    "        \n",
    "    kernel: sarracen.kernels.base_kernel\n",
    "        Smoothing kernel for SPH data interpolation.\n",
    "        If None, will use the one in sdf.\n",
    "\n",
    "    parallel: bool\n",
    "        If to use the numba parallel function\n",
    "\n",
    "    err_h: float ( > 0. )\n",
    "        determine confidence level.\n",
    "        e.g.,\n",
    "            1.0 will give error assuming error range is +/-1.0 smoothing length h;\n",
    "            0.5 will give error assuming error range is +/-0.5 smoothing length h;\n",
    "            etc. etc.\n",
    "            \n",
    "    rel_tol : float\n",
    "        maximum relative error tolerence per ray.\n",
    "        Default 1e-15 because float64 is only accurate to ~16th digits.\n",
    "\n",
    "    sdf_kdtree : kdtree.KDTree\n",
    "        KDTree built from sdf[['x', 'y', 'z']], for fast neighbour search.\n",
    "        \n",
    "    xyzs_names_list: list\n",
    "        list of names of the columns that represents x, y, z axes (i.e. coord axes names)\n",
    "        MUST INCLUDE ALL THREE AXES LABELS.\n",
    "        If only 2 is included, WILL ASSUME IT IS 2D CACULATIONS.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    anses, indes, contr, pts_order_used\n",
    "    \n",
    "    anses: np.ndarray\n",
    "        Radiance (i.e. specific intensities) for each ray.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # init\n",
    "    npart : int = len(sdf)\n",
    "    nray  : int = len(rays)\n",
    "    if kernel is None: kernel = sdf.kernel\n",
    "    kernel_rad = float(kernel.get_radius())\n",
    "    col_kernel = kernel.get_column_kernel_func(samples=1000) # w integrated from z\n",
    "    if ray_unit_vec is None: ray_unit_vec = get_ray_unit_vec(rays[0])\n",
    "    \n",
    "    pts    = np.array(sdf[xyzs_names_list], order='C')    # (npart, 3)-shaped array (must be this shape for pts_order sorting below)\n",
    "    hs     = np.array(sdf[ 'h'           ], order='C')    # npart-shaped array\n",
    "    masses = np.array(sdf[ 'm'           ], order='C')\n",
    "    kappas = np.array(sdf[ 'kappa'       ], order='C')\n",
    "    srcfuncs = np.array(srcfuncs          , order='C')\n",
    "    ndim   = pts.shape[-1]\n",
    "    mkappa_div_h2_arr = masses * kappas / hs**(ndim-1)\n",
    "    \n",
    "    # sanity check\n",
    "    if is_verbose(verbose, 'err') and not np.allclose(ray_unit_vec, get_rays_unit_vec(rays)):\n",
    "        raise ValueError(f\"Inconsistent ray_unit_vec {ray_unit_vec} with the rays.\")\n",
    "\n",
    "    if is_verbose(verbose, 'warn') and ndim != 3:\n",
    "        say('warn', None, verbose, f\"ndim == {ndim} is not 3.\")\n",
    "\n",
    "    # (npart-shaped array of the indices of the particles from closest to the observer to the furthest)\n",
    "    pts_order             = np.argsort( np.sum(pts * ray_unit_vec, axis=-1) )[::-1]\n",
    "    pts_ordered           = pts[     pts_order]\n",
    "    hs_ordered            = hs[      pts_order]\n",
    "    mkappa_div_h2_ordered = mkappa_div_h2_arr[pts_order]\n",
    "    srcfuncs_ordered      = srcfuncs[pts_order]\n",
    "\n",
    "    # get used particles indexes\n",
    "    if parallel:\n",
    "        anses, areas, ptaus, indes, contr, jused = _integrate_along_ray_gridxy_sub_parallel_analysis(\n",
    "            pts_ordered, hs_ordered, mkappa_div_h2_ordered, srcfuncs_ordered, rays, kernel_rad, col_kernel, pts_order, rel_tol=rel_tol)\n",
    "    else:\n",
    "        raise NotImplementedError(\"parallel=False version of this function not yet implemented.\")\n",
    "\n",
    "    pts_order_used = pts_order[jused]\n",
    "    if is_verbose(verbose, 'info'):\n",
    "        nused = len(pts_order_used)\n",
    "        say('info', None, verbose,\n",
    "            f\"{nused} particles actually participated calculation\",\n",
    "            f\"({int(nused/npart*10000)/100.}% of all particles,\",\n",
    "            f\"average {int(nused/nray*100)/100.} per ray.)\", sep=' ')\n",
    "\n",
    "    \n",
    "    return anses, areas, ptaus, indes, contr, pts_order_used\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3517015f-7833-48fd-ae31-31678d6c50df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# integrate and integrate error\n",
    "\n",
    "\n",
    "def integrate_along_ray_gridxy_err_ind(\n",
    "    sdf     : sarracen.SarracenDataFrame,\n",
    "    srcfuncs: np.ndarray,\n",
    "    rays    : np.ndarray,\n",
    "    ray_unit_vec: np.ndarray|None = None,\n",
    "    kernel  : sarracen.kernels.BaseKernel = None,\n",
    "    parallel: bool = False,\n",
    "    err_h   : float = 1.0,\n",
    "    rel_tol : float = 1e-16,\n",
    "    sdf_kdtree : kdtree.KDTree = None,\n",
    "    xyzs_names_list : list = ['x', 'y', 'z'],\n",
    "    verbose : int = 3,\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Backward integration of source functions along a grided ray (traced backwards), weighted by optical depth.\n",
    "    \n",
    "    Assuming all rays facing +z direction. (with the same ray_unit_vec [0., 0., 1.])\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sdf: sarracen.SarracenDataFrame\n",
    "        Must contain columns: x, y, z, h, m, kappa\n",
    "        \n",
    "    rays: (nray, 2, 3)-shaped array\n",
    "        Representing the ray trajectory. Currently only straight infinite lines are supported.\n",
    "        each ray is of the format:\n",
    "        [[begin point], [end point]]\n",
    "        where the end point is closer to the observer.\n",
    "\n",
    "    srcfuncs: 1D array\n",
    "        arrays describing the source function for every particle\n",
    "        \n",
    "    kernel: sarracen.kernels.base_kernel\n",
    "        Smoothing kernel for SPH data interpolation.\n",
    "        If None, will use the one in sdf.\n",
    "\n",
    "    parallel: bool\n",
    "        If to use the numba parallel function\n",
    "\n",
    "    err_h: float ( > 0. )\n",
    "        determine confidence level.\n",
    "        e.g.,\n",
    "            1.0 will give error assuming error range is +/-1.0 smoothing length h;\n",
    "            0.5 will give error assuming error range is +/-0.5 smoothing length h;\n",
    "            etc. etc.\n",
    "            \n",
    "    rel_tol : float\n",
    "        maximum relative error tolerence per ray.\n",
    "        Default 1e-15 because float64 is only accurate to ~16th digits.\n",
    "\n",
    "    sdf_kdtree : kdtree.KDTree\n",
    "        KDTree built from sdf[['x', 'y', 'z']], for fast neighbour search.\n",
    "        \n",
    "    xyzs_names_list: list\n",
    "        list of names of the columns that represents x, y, z axes (i.e. coord axes names)\n",
    "        MUST INCLUDE ALL THREE AXES LABELS.\n",
    "        If only 2 is included, WILL ASSUME IT IS 2D CACULATIONS.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    rads, errs\n",
    "    \n",
    "    rads: np.ndarray\n",
    "        Radiance (i.e. specific intensities) for each ray.\n",
    "\n",
    "    errs: np.ndarray\n",
    "        Uncertainties of Radiance (i.e. specific intensities) for each ray.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # init\n",
    "    npart : int = len(sdf)\n",
    "    nray  : int = len(rays)\n",
    "    if kernel is None: kernel = sdf.kernel\n",
    "    kernel_rad = float(kernel.get_radius())\n",
    "    col_kernel = kernel.get_column_kernel_func(samples=1000) # w integrated from z\n",
    "    if ray_unit_vec is None: ray_unit_vec = get_ray_unit_vec(rays[0])\n",
    "    \n",
    "    pts    = np.array(sdf[xyzs_names_list], order='C')    # (npart, 3)-shaped array (must be this shape for pts_order sorting below)\n",
    "    hs     = np.array(sdf[ 'h'           ], order='C')    # npart-shaped array\n",
    "    masses = np.array(sdf[ 'm'           ], order='C')\n",
    "    kappas = np.array(sdf[ 'kappa'       ], order='C')\n",
    "    srcfuncs = np.array(srcfuncs          , order='C')\n",
    "    ndim   = pts.shape[-1]\n",
    "    mkappa_div_h2_arr = masses * kappas / hs**(ndim-1)\n",
    "    \n",
    "    # sanity check\n",
    "    if is_verbose(verbose, 'err') and not np.allclose(ray_unit_vec, get_rays_unit_vec(rays)):\n",
    "        raise ValueError(f\"Inconsistent ray_unit_vec {ray_unit_vec} with the rays.\")\n",
    "\n",
    "    if is_verbose(verbose, 'warn') and ndim != 3:\n",
    "        say('warn', None, verbose, f\"ndim == {ndim} is not 3.\")\n",
    "\n",
    "    # (npart-shaped array of the indices of the particles from closest to the observer to the furthest)\n",
    "    pts_order             = np.argsort( np.sum(pts * ray_unit_vec, axis=-1) )[::-1]\n",
    "    pts_ordered           = pts[     pts_order]\n",
    "    hs_ordered            = hs[      pts_order]\n",
    "    mkappa_div_h2_ordered = mkappa_div_h2_arr[pts_order]\n",
    "    srcfuncs_ordered      = srcfuncs[pts_order]\n",
    "\n",
    "    # get used particles indexes\n",
    "    if parallel:\n",
    "        anses, areas, ptaus, indes, contr, jused = _integrate_along_ray_gridxy_sub_parallel_analysis(\n",
    "            pts_ordered, hs_ordered, mkappa_div_h2_ordered, srcfuncs_ordered, rays, kernel_rad, col_kernel, pts_order, rel_tol=rel_tol)\n",
    "    else:\n",
    "        raise NotImplementedError(\"Non-parallel version of this function not yet implemented.\")\n",
    "\n",
    "    pts_order_used = pts_order[jused]\n",
    "    if is_verbose(verbose, 'info'):\n",
    "        nused = len(pts_order_used)\n",
    "        say('info', None, verbose,\n",
    "            f\"{nused} particles actually participated in calculation\",\n",
    "            f\"({int(nused/npart*10000)/100.}% of all particles,\",\n",
    "            f\"average {int(nused/nray*100)/100.} per ray.)\", sep=' ')\n",
    "\n",
    "\n",
    "    # calculate error in those particles\n",
    "    sdf['_srcfunc'] = srcfuncs\n",
    "    srcfuncs_err_orderedu = get_sph_error(sdf, '_srcfunc', pts_order_used, err_h=err_h, sdf_kdtree=sdf_kdtree,\n",
    "                                 kernel=kernel, xyzs_names_list=xyzs_names_list, verbose=verbose)[:, 0]\n",
    "    sdf['_srcfunc_err'] = np.nan\n",
    "    sdf['_srcfunc_err'].iloc[pts_order_used] = srcfuncs_err_orderedu\n",
    "\n",
    "    if is_verbose(verbose, 'debug'):\n",
    "        nused = len(pts_order_used)\n",
    "        say('debug', None, verbose,\n",
    "            f\"error calculation completed.\",\n",
    "            f\"average relative error of srcfunc\",\n",
    "            f\"{int((srcfuncs_err_orderedu/srcfuncs_ordered[jused]).sum()/len(pts_order_used)*10000)/100.}%\",\n",
    "            sep=' ')\n",
    "\n",
    "    rads, errs = _integrate_along_ray_gridxy_sub_parallel_err_ind(\n",
    "        pts_ordered[jused], hs_ordered[jused], mkappa_div_h2_ordered[jused], srcfuncs_ordered[jused],\n",
    "        srcfuncs_err_orderedu, rays, kernel_rad, col_kernel, pts_order, rel_tol=rel_tol)\n",
    "    #raise NotImplementedError\n",
    "    \n",
    "    return rads, errs, areas, ptaus\n",
    "\n",
    "#integrate_along_ray_gridxy_ind = integrate_along_ray_gridxy_err_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4962c464-ee9e-46ff-b8c3-01c07166e880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test runs\n",
    "@jit(nopython=True, parallel=True)\n",
    "def _integrate_along_ray_gridxy_sub_parallel_old(\n",
    "    pts_ordered          : npt.NDArray[np.float64],    # (npart, 3)-shaped\n",
    "    hs_ordered           : npt.NDArray[np.float64],    # (npart,  )-shaped\n",
    "    mkappa_div_h2_ordered: npt.NDArray[np.float64],    # (npart,  )-shaped\n",
    "    srcfuncs_ordered     : npt.NDArray[np.float64],    # (npart,  )-shaped\n",
    "    rays                 : npt.NDArray[np.float64],    # (nray, 2, 3)-shaped\n",
    "    ray_areas            : npt.NDArray[np.float64],    # (nray,     )-shaped\n",
    "    kernel_rad           : float,\n",
    "    col_kernel           : numba.core.registry.CPUDispatcher,\n",
    "    pts_order            : npt.NDArray[np.float64],    # (npart,  )-shaped\n",
    "    rel_tol              : float = 1e-16, # because float64 is only has only 16 digits accuracy\n",
    ") -> tuple[\n",
    "    npt.NDArray[np.float64],    # anses\n",
    "    npt.NDArray[np.float64],    # pones\n",
    "    npt.NDArray[np.float64],    # ptaus\n",
    "    npt.NDArray[np.int64  ],    # indes\n",
    "    npt.NDArray[np.float64],    # contr\n",
    "    npt.NDArray[np.float64],    # jfact\n",
    "]:\n",
    "    \"\"\"Sub process for integrate_along_ray_gridxy(). Numba parallel version (using prange).\n",
    "\n",
    "    Unit vec must be [0., 0., 1.] (i.e. all rays must point upwards towards +z).\n",
    "\n",
    "    Private function. Assumes specific input type. See source code comments.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    anses, pones, ptaus, indes, contr, jfact\n",
    "    \n",
    "    anses: (nray,)-shaped np.ndarray[float]\n",
    "        Radiance (i.e. specific intensities) for each ray.\n",
    "\n",
    "    pones: (nray,)-shaped np.ndarray[float]\n",
    "        <1> for each pixel,\n",
    "        i.e. same integration of radiance but for a constant 'srcfunc' of '1', for each ray.\n",
    "        Helpful for consistency check (should be more or less 1 where ptaus is nan.)\n",
    "        do weighted average by weight of areas per pixel to get the total area of the object!\n",
    "\n",
    "    ptaus: (nray,)-shaped np.ndarray[float]\n",
    "        The optical depth for each pixel\n",
    "        *** WILL BE np.nan IF OPTICAL DEPTH IS DEEP (which will be MOST OF THE TIME.)  ***\n",
    "        can be used as an alternative way to calculate the area of the object.\n",
    "\n",
    "    indes: (nray,)-shaped np.ndarray[int]\n",
    "        indexes of the max contribution particle\n",
    "\n",
    "    contr: (nray,)-shaped np.ndarray[float]\n",
    "        relative contribution (in fractions) of the max contribution particle\n",
    "\n",
    "    jfact: (npart,)-shaped np.ndarray[float]\n",
    "        Contribution factor for j-th particle.\n",
    "        Multiply it with 4 * pi * srcfuncs and sum it up as an alternative way to get the luminosity.\n",
    "        Will be zero if particle is not used.\n",
    "        \n",
    "\n",
    "    \"\"\"\n",
    "    #raise NotImplementedError\n",
    "\n",
    "    nray  = len(rays)\n",
    "    npart = len(srcfuncs_ordered)\n",
    "    ndim  = pts_ordered.shape[-1]\n",
    "    anses = np.zeros(nray)\n",
    "    indes = np.zeros(nray, dtype=np.int64)    # indexes of max contribution particle\n",
    "    contr = np.zeros(nray)    # relative contribution of the max contribution particle\n",
    "    jfact = np.zeros(npart)    # is j-th particle in the ordered list used for this calculation?\n",
    "    pones = np.zeros(nray)\n",
    "    ptaus = np.full(nray, np.nan)    # lower bound of the optical depth\n",
    "    \n",
    "\n",
    "    # error tolerance of tau (part 1)\n",
    "    tol_tau_base = np.log(srcfuncs_ordered.sum()) - np.log(rel_tol)\n",
    "\n",
    "    # hr = h * kernel_rad\n",
    "    hrs_ordered = hs_ordered * kernel_rad\n",
    "\n",
    "    # loop over ray\n",
    "    for i in prange(nray):\n",
    "        ray = rays[i]\n",
    "        ray_area = ray_areas[i]\n",
    "        tau = 0.\n",
    "        ans = 0.\n",
    "        dans= 0.\n",
    "        dans_max_tmp = 0.\n",
    "        ind = -1\n",
    "        fac = 0. # effectively <1>\n",
    "        dfac= 0. # factor\n",
    "\n",
    "        #   xy-grid specific solution\n",
    "        ray_x = ray[0, 0]\n",
    "        ray_y = ray[0, 1]\n",
    "        \n",
    "        # loop over particles\n",
    "        #for pt, hr, mkappa_div_h2, srcfunc in zip(\n",
    "        #    pts_ordered, hrs_ordered, mkappa_div_h2_ordered, srcfuncs_ordered):\n",
    "        for j in range(npart):\n",
    "            pt = pts_ordered[j]\n",
    "            hr = hrs_ordered[j]\n",
    "            \n",
    "            # check if the particle is within range\n",
    "            #   general solution\n",
    "            #q = get_dist2_from_pt_to_line_nb(pt, ray)**0.5 / h\n",
    "            #if q < kernel_rad:\n",
    "            #   xy-grid specific solution\n",
    "            if ray_x - hr < pt[0] and pt[0] < ray_x + hr and ray_y - hr < pt[1] and pt[1] < ray_y + hr:\n",
    "                h = hs_ordered[ j]\n",
    "                q = ((pt[0] - ray_x)**2 + (pt[1] - ray_y)**2)**0.5 / h\n",
    "                if q < kernel_rad:\n",
    "\n",
    "                    #jfact[j] = True\n",
    "                    \n",
    "                    # now do radiative transfer\n",
    "                    \n",
    "                    mkappa_div_h2 = mkappa_div_h2_ordered[j]\n",
    "                    srcfunc = srcfuncs_ordered[j]\n",
    "                    \n",
    "                    dtau = mkappa_div_h2 * col_kernel(q, ndim-1)\n",
    "                    dfac = np.exp(-tau) * (1. - np.exp(-dtau))\n",
    "                    dans = dfac * srcfunc\n",
    "                    ans += dans\n",
    "                    fac += dfac\n",
    "                    tau += dtau\n",
    "\n",
    "                    jfact[j] += dfac * ray_area\n",
    "\n",
    "                    # note down the largest contributor\n",
    "                    if dans > dans_max_tmp:\n",
    "                        dans_max_tmp = dans\n",
    "                        ind = pts_order[j]\n",
    "    \n",
    "                    # terminate the calc for this ray if tau is sufficient large\n",
    "                    #    such that the relative error on ans is smaller than rel_tol\n",
    "                    # i.e. since when tau > np.log(srcfuncs_ordered.sum()) - np.log(rel_tol) - np.log(ans),\n",
    "                    #    we know that ans[i] - ans[i][k] < rel_tol * ans[i]\n",
    "                    # see my notes for derivation\n",
    "                    if tau > tol_tau_base - np.log(ans):\n",
    "                        break\n",
    "\n",
    "        else:\n",
    "            ptaus[i]=tau\n",
    "            \n",
    "        anses[i] = ans\n",
    "        indes[i] = ind\n",
    "        if ans > 0: contr[i] = dans_max_tmp / ans\n",
    "        pones[i] = fac\n",
    "    \n",
    "    return anses, pones, ptaus, indes, contr, jfact\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6bffc98d-0beb-4cbe-ac9a-ad232329df26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# integrate with error estiamtes\n",
    "\n",
    "def integrate_along_ray_gridxy_old(\n",
    "    sdf         : sarracen.SarracenDataFrame,\n",
    "    srcfuncs    : npt.NDArray[np.float64],    # (npart,  )-shaped\n",
    "    srcfuncs_err: None|npt.NDArray[np.float64],    # (npart,  )-shaped\n",
    "    rays        : npt.NDArray[np.float64],    # (nray, 2, 3)-shaped\n",
    "    ray_areas   : npt.NDArray[np.float64],    # (nray,     )-shaped\n",
    "    ray_unit_vec: None|npt.NDArray[np.float64] = None,    # (nray, 3)-shaped\n",
    "    kernel      : None|sarracen.kernels.BaseKernel = None,\n",
    "    hfact       : None|float = None,\n",
    "    parallel    : bool = False,\n",
    "    err_h       : float = 1.0,\n",
    "    rel_tol     : float = 1e-16,\n",
    "    sdf_kdtree  : None|kdtree.KDTree = None,\n",
    "    xyzs_names_list : list = ['x', 'y', 'z'],\n",
    "    verbose     : int = 3,\n",
    ") -> tuple[\n",
    "    float,    # lum\n",
    "    float,    # lum_err\n",
    "    npt.NDArray[np.float64],    # rads\n",
    "    npt.NDArray[np.float64],    # pones\n",
    "    npt.NDArray[np.float64],    # ptaus\n",
    "    npt.NDArray[np.float64],    # indes\n",
    "    npt.NDArray[np.float64],    # contr\n",
    "    tuple[npt.NDArray[np.int64],],    # jused - 'j' for j-th particle;\n",
    "    # 'used' means the (indices of) particles (in the ordered list) that actually participated in the calculation\n",
    "    npt.NDArray[np.float64],    # jfact_used\n",
    "]:\n",
    "    \"\"\"Backward integration of source functions along a grided ray (traced backwards), weighted by optical depth.\n",
    "    \n",
    "    Assuming all rays facing +z direction. (with the same ray_unit_vec [0., 0., 1.])\n",
    "\n",
    "    WARNING: will overwrite sdf['srcfunc'] if srcfuncs_err is None.\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sdf: sarracen.SarracenDataFrame\n",
    "        Must contain columns: x, y, z, h, m, kappa\n",
    "        \n",
    "    rays: (nray, 2, 3)-shaped array\n",
    "        Representing the ray trajectory. Currently only straight infinite lines are supported.\n",
    "        each ray is of the format:\n",
    "        [[begin point], [end point]]\n",
    "        where the end point is closer to the observer.\n",
    "\n",
    "    srcfuncs: 1D array\n",
    "        arrays describing the source function for every particle\n",
    "        \n",
    "    kernel: sarracen.kernels.base_kernel\n",
    "        Smoothing kernel for SPH data interpolation.\n",
    "        If None, will use the one in sdf.\n",
    "\n",
    "    hfact : None|float\n",
    "\n",
    "    parallel: bool\n",
    "        If to use the numba parallel function\n",
    "\n",
    "    err_h: float ( > 0. )\n",
    "        determine confidence level.\n",
    "        e.g.,\n",
    "            1.0 will give error assuming error range is +/-1.0 smoothing length h;\n",
    "            0.5 will give error assuming error range is +/-0.5 smoothing length h;\n",
    "            etc. etc.\n",
    "            \n",
    "    rel_tol : float\n",
    "        maximum relative error tolerence per ray.\n",
    "        Default 1e-15 because float64 is only accurate to ~16th digits.\n",
    "\n",
    "    sdf_kdtree : kdtree.KDTree\n",
    "        KDTree built from sdf[['x', 'y', 'z']], for fast neighbour search.\n",
    "        if None, will build one.\n",
    "        \n",
    "    xyzs_names_list: list\n",
    "        list of names of the columns that represents x, y, z axes (i.e. coord axes names)\n",
    "        MUST INCLUDE ALL THREE AXES LABELS.\n",
    "        If only 2 is included, WILL ASSUME IT IS 2D CACULATIONS.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    lum, lum_err, rads, pones, ptaus, indes, contr, jused, jfact_used\n",
    "    \n",
    "    rads: np.ndarray\n",
    "        Radiance (i.e. specific intensities) for each ray.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # init\n",
    "    npart : int = len(sdf)\n",
    "    nray  : int = len(rays)\n",
    "    if kernel is None: kernel = sdf.kernel\n",
    "    kernel_rad = float(kernel.get_radius())\n",
    "    col_kernel = kernel.get_column_kernel_func(samples=1000) # w integrated from z\n",
    "    if ray_unit_vec is None: ray_unit_vec = get_ray_unit_vec(rays[0])\n",
    "    \n",
    "    pts    = np.array(sdf[xyzs_names_list], order='C')    # (npart, 3)-shaped array (must be this shape for pts_order sorting below)\n",
    "    hs     = np.array(sdf[ 'h'           ], order='C')    # npart-shaped array\n",
    "    masses = np.array(sdf[ 'm'           ], order='C')\n",
    "    kappas = np.array(sdf[ 'kappa'       ], order='C')\n",
    "    srcfuncs = np.array(srcfuncs          , order='C')\n",
    "    ndim   = pts.shape[-1]\n",
    "    mkappa_div_h2_arr = masses * kappas / hs**(ndim-1)\n",
    "    \n",
    "    # sanity check\n",
    "    if is_verbose(verbose, 'err') and not np.allclose(ray_unit_vec, get_rays_unit_vec(rays)):\n",
    "        raise ValueError(f\"Inconsistent ray_unit_vec {ray_unit_vec} with the rays.\")\n",
    "\n",
    "    if is_verbose(verbose, 'warn') and ndim != 3:\n",
    "        say('warn', None, verbose, f\"ndim == {ndim} is not 3.\")\n",
    "\n",
    "    # (npart-shaped array of the indices of the particles from closest to the observer to the furthest)\n",
    "    pts_order             = np.argsort( np.sum(pts * ray_unit_vec, axis=-1) )[::-1]\n",
    "    pts_ordered           = pts[     pts_order]\n",
    "    hs_ordered            = hs[      pts_order]\n",
    "    mkappa_div_h2_ordered = mkappa_div_h2_arr[pts_order]\n",
    "    srcfuncs_ordered      = srcfuncs[pts_order]\n",
    "\n",
    "    # get used particles indexes\n",
    "    if parallel:\n",
    "        rads, pones, ptaus, indes, contr, jfact = _integrate_along_ray_gridxy_sub_parallel_old(\n",
    "            pts_ordered, hs_ordered, mkappa_div_h2_ordered, srcfuncs_ordered,\n",
    "            rays, ray_areas, kernel_rad, col_kernel, pts_order, rel_tol=rel_tol)\n",
    "    else:\n",
    "        raise NotImplementedError(\"parallel=False version of this function not yet implemented.\")\n",
    "\n",
    "    jused = np.where(jfact)\n",
    "    jfact_used = jfact[jused]\n",
    "    pts_order_used = pts_order[jused]\n",
    "    \n",
    "    lum  = 4 * pi * (rads * ray_areas).sum()\n",
    "    lum2 = 4 * pi * (srcfuncs_ordered[jused] * jfact_used).sum()\n",
    "    say('debug', None, verbose,\n",
    "        f\"{lum = }, {lum2 = }\",\n",
    "        f\"{rads.shape=}, {ray_areas.shape=}\",\n",
    "        f\"{srcfuncs[jused].shape=}, {jfact_used.shape=}\",\n",
    "    )\n",
    "    #assert np.isclose(lum, lum2)\n",
    "\n",
    "    if srcfuncs_err is None:\n",
    "        # calc error of source function now\n",
    "        sdf['srcfunc'] = srcfuncs\n",
    "        srcfuncs_grad_used = get_sph_gradient(\n",
    "            sdf,\n",
    "            val_names   ='srcfunc',\n",
    "            locs        = pts_ordered[     jused],\n",
    "            vals_at_locs= srcfuncs_ordered[jused],\n",
    "            hs_at_locs  = hs_ordered[      jused],\n",
    "            kernel      = kernel,\n",
    "            hfact       = hfact,\n",
    "            sdf_kdtree  = sdf_kdtree,\n",
    "            ndim        = ndim,\n",
    "            xyzs_names_list=xyzs_names_list,\n",
    "            parallel    = parallel,\n",
    "            verbose     = verbose,\n",
    "        )[:, :, 0]    # get_sph_gradient returns a (nlocs, ndim, nvals)-shaped np.ndarray\n",
    "        srcfuncs_err_used = np.sum(srcfuncs_grad_used**2, axis=1)**0.5 * hs_ordered[jused] * err_h\n",
    "    else:\n",
    "        srcfuncs_err_used = srcfuncs_err[pts_order_used]\n",
    "\n",
    "    lum_err = 4 * pi * ((srcfuncs_err_used * jfact_used)**2).sum()**0.5\n",
    "    \n",
    "\n",
    "    if is_verbose(verbose, 'info'):\n",
    "        nused = len(jfact_used)\n",
    "        say('info', None, verbose,\n",
    "            f\"{nused} particles actually participated calculation\",\n",
    "            f\"({int(nused/npart*10000)/100.}% of all particles,\",\n",
    "            f\"average {int(nused/nray*100)/100.} per ray.)\", sep=' ')\n",
    "\n",
    "    \n",
    "    return lum, lum_err, rads, pones, ptaus, indes, contr, pts_order_used, jfact_used\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b50fce4-ed22-4476-b246-91df71636124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test runs\n",
    "@jit(nopython=True, parallel=True, fastmath=True)\n",
    "def _integrate_along_rays_gridxy_sub_parallel_with_shallow_err(\n",
    "    pts_ordered          : npt.NDArray[np.float64],    # (npart, 3)-shaped\n",
    "    hs_ordered           : npt.NDArray[np.float64],    # (npart,  )-shaped\n",
    "    mkappa_div_h2_ordered: npt.NDArray[np.float64],    # (npart,  )-shaped\n",
    "    srcfuncs_ordered     : npt.NDArray[np.float64],    # (npart,  )-shaped\n",
    "    rays_xy              : npt.NDArray[np.float64],    # (nray, 2 )-shaped\n",
    "    ray_areas            : npt.NDArray[np.float64],    # (nray,   )-shaped\n",
    "    kernel_rad           : float,\n",
    "    kernel_col           : numba.core.registry.CPUDispatcher,\n",
    "    kernel_csz           : numba.core.registry.CPUDispatcher,\n",
    "    kernel_w             : numba.core.registry.CPUDispatcher,\n",
    "    pts_order            : npt.NDArray[np.float64],    # (npart,  )-shaped\n",
    "    rel_tol              : float = 1e-16, # because float64 has only 16 digits accuracy\n",
    "    nsample              : int   = 100,  # no of sample points for integration\n",
    ") -> tuple[\n",
    "    npt.NDArray[np.float64],    # anses\n",
    "    npt.NDArray[np.float64],    # pones\n",
    "    npt.NDArray[np.float64],    # ptaus\n",
    "    npt.NDArray[np.int64  ],    # indes\n",
    "    npt.NDArray[np.float64],    # contr\n",
    "    npt.NDArray[np.float64],    # jfact\n",
    "]:\n",
    "    \"\"\"Sub process for integrate_along_ray_gridxy(). Numba parallel version (using prange).\n",
    "    ---------------------------------------------------------------------------\n",
    "\n",
    "    Calculating the luminosity using\n",
    "    $$\n",
    "    L \\approx\n",
    "        4 \\pi \\sum_i \\sum_j \\triangle A_i\n",
    "            S_j \\frac{\\kappa_j m_j}{h_j^2}\n",
    "            \\int\n",
    "                e^{\n",
    "                    -\\sum_k \\frac{\\kappa_k m_k}{h_k^2}\n",
    "                    w_\\mathrm{csz}(q_{xy, ik}, \\, -q_{z, k})\n",
    "                }\n",
    "                w(q_{ij}) d(q_{z, j})\n",
    "    $$\n",
    "\n",
    "    Unit vec must be [0., 0., 1.] (i.e. all rays must point upwards towards +z)\n",
    "\n",
    "    Private function. Assumes specific input type. See source code comments.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    anses, pones, ptaus, indes, contr, jfact\n",
    "    \n",
    "    anses: (nray,)-shaped np.ndarray[float]\n",
    "        Radiance (i.e. specific intensities) for each ray.\n",
    "\n",
    "    pones: (nray,)-shaped np.ndarray[float]\n",
    "        <1> for each pixel,\n",
    "        i.e. same integration of radiance but for a constant 'srcfunc' of '1', for each ray.\n",
    "        Helpful for consistency check (should be more or less 1 where ptaus is nan.)\n",
    "        do weighted average by weight of areas per pixel to get the total area of the object!\n",
    "\n",
    "    ptaus: (nray,)-shaped np.ndarray[float]\n",
    "        The optical depth for each pixel\n",
    "        *** WILL BE np.nan IF OPTICAL DEPTH IS DEEP (which will be MOST OF THE TIME.)  ***\n",
    "        can be used as an alternative way to calculate the area of the object.\n",
    "\n",
    "    indes: (nray,)-shaped np.ndarray[int]\n",
    "        indexes of the max contribution particle\n",
    "\n",
    "    contr: (nray,)-shaped np.ndarray[float]\n",
    "        relative contribution (in fractions) of the max contribution particle\n",
    "\n",
    "    jfact: (npart,)-shaped np.ndarray[float]\n",
    "        Contribution factor for j-th particle.\n",
    "        Multiply it with 4 * pi * srcfuncs and sum it up as an alternative way to get the luminosity.\n",
    "        Will be zero if particle is not used.\n",
    "\n",
    "    estis: (nray,)-shaped np.ndarray[float]\n",
    "        Estimations of radiance (i.e. specific intensities) for each ray, using old method\n",
    "        (Also estis means 'was' in Esperanto, so it's a fitting name)\n",
    "\n",
    "    \"\"\"\n",
    "    #raise NotImplementedError\n",
    "\n",
    "    nray  = len(rays_xy)\n",
    "    npart = len(srcfuncs_ordered)\n",
    "    ndim  = pts_ordered.shape[-1]\n",
    "    anses = np.zeros(nray)\n",
    "    indes = np.zeros(nray, dtype=np.int64)    # indexes of max contribution particle\n",
    "    contr = np.zeros(nray)     # relative contribution of the max contribution particle\n",
    "    # ifact = np.zeros(nray)     # effective xsec for i-th ray  # NOTE: ifact = pones * ray_areas\n",
    "    jfact = np.zeros(npart)    # effective xsec for j-th particle\n",
    "    pones = np.zeros(nray)\n",
    "    ptaus = np.full(nray, np.nan)    # lower bound of the optical depth\n",
    "    estis = np.zeros(nray)\n",
    "    nsample_half = int(nsample/2)\n",
    "    \n",
    "\n",
    "    # error tolerance of tau (part 1)\n",
    "    tol_tau_base = np.log(srcfuncs_ordered.sum()) - np.log(rel_tol)\n",
    "\n",
    "\n",
    "    # cache calcs\n",
    "    hrs_ordered = hs_ordered * kernel_rad\n",
    "\n",
    "    \n",
    "\n",
    "    # loop over ray\n",
    "    for i in prange(nray):\n",
    "        ray_xy  = rays_xy[i]\n",
    "        ray_area= ray_areas[i]\n",
    "        tau     = 0.\n",
    "        ans     = 0.\n",
    "        dans    = 0.\n",
    "        rad_est = 0.   # estimation of radiance (i.e. ans)\n",
    "        # dans_max_tmp = 0.\n",
    "        dfac_max_tmp = 0.\n",
    "        ind     = -1\n",
    "        fac     = 0. # effectively <1>\n",
    "        dfac    = 0. # factor\n",
    "        used_j  = 0    # j = used_indexes[used_j]\n",
    "\n",
    "        #   xy-grid specific solution\n",
    "        ray_x = ray_xy[0]\n",
    "        ray_y = ray_xy[1]\n",
    "\n",
    "        \n",
    "\n",
    "        # First, try to find out how many relevant particles are there\n",
    "        \n",
    "        nused_i = 0    # no of used particles for i-th ray\n",
    "        for j in range(npart):\n",
    "            x_j = pts_ordered[j, 0]\n",
    "            y_j = pts_ordered[j, 1]\n",
    "            hr = hrs_ordered[j]\n",
    "            # check if the particle is within range\n",
    "            #   xy-grid specific solution\n",
    "            if ray_x - hr < x_j and x_j < ray_x + hr and ray_y - hr < y_j and y_j < ray_y + hr:\n",
    "                nused_i += 1\n",
    "\n",
    "        # now we know roughly how many particles are relevant...\n",
    "        \n",
    "        used_indexes = np.full(nused_i, -1, dtype=np.int64)\n",
    "        used_dtaus   = np.full(nused_i, np.nan)\n",
    "        used_qs_xy   = np.full(nused_i, np.nan)\n",
    "\n",
    "        used_j = 0    # j = used_indexes[used_j]\n",
    "        tau    = 0.\n",
    "        rad_est= 0.   # estimation of radiance (i.e. ans)\n",
    "        for j in range(npart):\n",
    "            x_j = pts_ordered[j, 0]\n",
    "            y_j = pts_ordered[j, 1]\n",
    "            hr  = hrs_ordered[j]\n",
    "            \n",
    "            # check if the particle is within range\n",
    "            #   xy-grid specific solution\n",
    "            if ray_x - hr < x_j and x_j < ray_x + hr and ray_y - hr < y_j and y_j < ray_y + hr:\n",
    "                h = hs_ordered[ j]\n",
    "                q_xy = ((x_j - ray_x)**2 + (y_j - ray_y)**2)**0.5 / h\n",
    "                if q_xy < kernel_rad:\n",
    "\n",
    "                    # log\n",
    "                    mkappa_div_h2 = mkappa_div_h2_ordered[j]\n",
    "                    srcfunc = srcfuncs_ordered[j]\n",
    "                    dtau = mkappa_div_h2 * kernel_col(q_xy, ndim)\n",
    "                    \n",
    "                    used_indexes[used_j] = j\n",
    "                    used_dtaus[  used_j] = dtau\n",
    "                    used_qs_xy[  used_j] = q_xy\n",
    "                    used_j += 1\n",
    "\n",
    "                    dfac = np.exp(-tau) * (1. - np.exp(-dtau))\n",
    "                    #dans = dfac * srcfunc\n",
    "                    rad_est += dfac * srcfunc #dans\n",
    "                    tau += dtau\n",
    "                    \n",
    "                    # terminate the calc for this ray if tau is sufficient large\n",
    "                    #    such that the relative error on ans is smaller than rel_tol\n",
    "                    # i.e. since when tau > np.log(srcfuncs_ordered.sum()) - np.log(rel_tol) - np.log(ans),\n",
    "                    #    we know that ans[i] - ans[i][k] < rel_tol * ans[i]\n",
    "                    # see my notes for derivation\n",
    "                    if tau > tol_tau_base - np.log(rad_est):\n",
    "                        break\n",
    "        else:\n",
    "            ptaus[i]=tau\n",
    "        nused_i = used_j     # update used indexes size\n",
    "        estis[i]= rad_est\n",
    "        \n",
    "\n",
    "        \n",
    "        # Now, loop over particles\n",
    "        \n",
    "        for used_j in range(nused_i):\n",
    "            j    = used_indexes[used_j]\n",
    "            #pt   = pts_ordered[j]\n",
    "            q_xy = used_qs_xy[used_j] # ((pt[0] - ray_x)**2 + (pt[1] - ray_y)**2)**0.5 / h\n",
    "            hr   = hrs_ordered[j]\n",
    "            h    = hs_ordered[ j]\n",
    "\n",
    "            \n",
    "            # now do radiative transfer\n",
    "            \n",
    "            mkappa_div_h2 = mkappa_div_h2_ordered[j]\n",
    "            srcfunc = srcfuncs_ordered[j]\n",
    "            \n",
    "\n",
    "            # get optical depth\n",
    "            \n",
    "            z       = pts_ordered[j, 2]\n",
    "            zmhr    = z - hr\n",
    "            zphr    = z + hr\n",
    "            dzs_j   = (zphr - zmhr) / nsample\n",
    "            zs_j    = np.linspace(zmhr + dzs_j/2., zphr - dzs_j/2., nsample)\n",
    "            taus_j  = np.zeros(nsample)\n",
    "            tau_pr  = 0.   # \\tau'_j - the summed optical depth for particles that are 'fully' ahead of j-th particle\n",
    "            #assert np.isclose(dzs_j, zs_j[1] - zs_j[0])\n",
    "            for used_k in range(nused_i):\n",
    "                k = used_indexes[used_k]\n",
    "                z_k  = pts_ordered[k, 2]\n",
    "                hr_k = hrs_ordered[k]\n",
    "                # ****** NOTE: the following could use optimization ******\n",
    "                #    e.g. maybe generate a list of k beforehand?\n",
    "                if zmhr - hr_k < z_k:\n",
    "                    # particle being relevant\n",
    "                    if           z_k < zphr + hr_k:\n",
    "                        # particle range intersects\n",
    "                        h_k = hs_ordered[k]\n",
    "                        mkappa_div_h2_k = mkappa_div_h2_ordered[k]\n",
    "                        q_xy_k = used_qs_xy[used_k] #((x_k - ray_x)**2 + (y_k - ray_y)**2)**0.5 / h_k\n",
    "                        for ji in range(nsample):\n",
    "                            q_z_k  = (zs_j[ji] - z_k) / h_k\n",
    "                            taus_j[ji] += mkappa_div_h2_k * kernel_csz(q_xy_k, -q_z_k, ndim)\n",
    "                    else:\n",
    "                        # particle is fully ahead\n",
    "                        tau_pr += used_dtaus[used_k]\n",
    "\n",
    "            \n",
    "            # integrate through opitcal depth for j\n",
    "            dfac  = 0.\n",
    "            q2_xy = q_xy**2\n",
    "            dqz_j = dzs_j / h\n",
    "            for ji in range(nsample):\n",
    "                # ****** Integration pending improvement ******\n",
    "                # ****** same for the integration in w_csz ******\n",
    "                q_ji = (q2_xy + ((zs_j[ji] - z)/h)**2)**0.5\n",
    "                dfac += np.exp(-taus_j[ji]) * kernel_w(q_ji, ndim) * dqz_j\n",
    "            dfac *= mkappa_div_h2 * np.exp(-tau_pr)\n",
    "            dans  = dfac * srcfunc\n",
    "            ans  += dans    # for getting <S>\n",
    "            fac  += dfac    # for getting <1>\n",
    "            #tau = tau_pr + taus_j[nsample_half]\n",
    "\n",
    "            jfact[j] += dfac * ray_area\n",
    "\n",
    "            # # note down the largest contributor\n",
    "            # if dans > dans_max_tmp:\n",
    "            #     dans_max_tmp = dans\n",
    "            #     ind = pts_order[j]\n",
    "            if dfac > dfac_max_tmp:\n",
    "                dfac_max_tmp = dfac\n",
    "                ind = pts_order[j]\n",
    "\n",
    "\n",
    "            ## terminate the calc for this ray if tau is sufficiently large\n",
    "            #if tau > tol_tau_base - np.log(ans):\n",
    "            #    break\n",
    "            \n",
    "        anses[i] = ans\n",
    "        indes[i] = ind\n",
    "        if ans > 0: contr[i] = dfac_max_tmp / fac  # dans_max_tmp / ans\n",
    "        pones[i] = fac\n",
    "    \n",
    "    return anses, pones, ptaus, indes, contr, jfact, estis\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc182304-ba12-4e70-bb26-b9af37be2fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# integrate with error estiamtes\n",
    "\n",
    "def integrate_along_rays_gridxy_with_shallow_err(\n",
    "    sdf         : sarracen.SarracenDataFrame,\n",
    "    srcfuncs    : npt.NDArray[np.float64],    # (npart,  )-shaped\n",
    "    srcfuncs_err: None|npt.NDArray[np.float64],    # (npart,  )-shaped\n",
    "    rays        : npt.NDArray[np.float64],    # (nray, 2, 3)-shaped\n",
    "    ray_areas   : npt.NDArray[np.float64],    # (nray,     )-shaped\n",
    "    ray_unit_vec: None|npt.NDArray[np.float64] = None,    # (nray, 3)-shaped\n",
    "    kernel      : None|sarracen.kernels.BaseKernel = None,\n",
    "    kernel_col  : None|numba.core.registry.CPUDispatcher = None,\n",
    "    kernel_csz  : None|numba.core.registry.CPUDispatcher = None,\n",
    "    hfact       : None|float = None,\n",
    "    parallel    : bool = False,\n",
    "    err_h       : float = 1.0,\n",
    "    rel_tol     : float = 1e-16,\n",
    "    sdf_kdtree  : None|kdtree.KDTree = None,\n",
    "    xyzs_names_list : list = ['x', 'y', 'z'],\n",
    "    verbose     : int = 3,\n",
    ") -> tuple[\n",
    "    float,    # lum\n",
    "    float,    # lum_err\n",
    "    npt.NDArray[np.float64],    # rads\n",
    "    npt.NDArray[np.float64],    # pones\n",
    "    npt.NDArray[np.float64],    # ptaus\n",
    "    npt.NDArray[np.int64  ],    # indes\n",
    "    npt.NDArray[np.float64],    # contr\n",
    "    npt.NDArray[np.int64  ],    # pts_order_used\n",
    "    npt.NDArray[np.float64],    # jfact_used - 'j' for j-th particle;\n",
    "    # 'used' means the particles (in the order_used list) that actually participated in the calculation\n",
    "]:\n",
    "    \"\"\"Backward integration of source functions along a grided ray (traced backwards), weighted by optical depth.\n",
    "    ---------------------------------------------------------------------------\n",
    "    \n",
    "    Assuming all rays facing +z direction\n",
    "    (with the same ray_unit_vec [0., 0., 1.])\n",
    "\n",
    "    WARNING: will overwrite sdf['srcfunc'] if srcfuncs_err is None.\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sdf: sarracen.SarracenDataFrame\n",
    "        Must contain columns: x, y, z, h, m, kappa\n",
    "        \n",
    "    rays: (nray, 2, 3)-shaped array\n",
    "        Representing the ray trajectory. Currently only straight infinite lines are supported.\n",
    "        each ray is of the format:\n",
    "        [[begin point], [end point]]\n",
    "        where the end point is closer to the observer.\n",
    "\n",
    "    srcfuncs: 1D array\n",
    "        arrays describing the source function for every particle\n",
    "        \n",
    "    kernel: sarracen.kernels.base_kernel\n",
    "        Smoothing kernel for SPH data interpolation.\n",
    "        If None, will use the one in sdf.\n",
    "\n",
    "    kernel_csz: func\n",
    "        Cumulative summed kernel along z axis:\n",
    "        $ \\int_{-w_\\mathrm{rad}}^{q_z} w(\\sqrt{q_{xy}^2 + q_z^2}) dq_z $\n",
    "\n",
    "    hfact : None|float\n",
    "\n",
    "    parallel: bool\n",
    "        If to use the numba parallel function\n",
    "\n",
    "    err_h: float ( > 0. )\n",
    "        determine confidence level.\n",
    "        e.g.,\n",
    "            1.0 will give error assuming error range is +/-1.0 smoothing length h;\n",
    "            0.5 will give error assuming error range is +/-0.5 smoothing length h;\n",
    "            etc. etc.\n",
    "            \n",
    "    rel_tol : float\n",
    "        maximum relative error tolerence per ray.\n",
    "        Default 1e-15 because float64 is only accurate to ~16th digits.\n",
    "\n",
    "    sdf_kdtree : kdtree.KDTree\n",
    "        KDTree built from sdf[['x', 'y', 'z']], for fast neighbour search.\n",
    "        if None, will build one.\n",
    "        \n",
    "    xyzs_names_list: list\n",
    "        list of names of the columns that represents x, y, z axes (i.e. coord axes names)\n",
    "        MUST INCLUDE ALL THREE AXES LABELS.\n",
    "        If only 2 is included, WILL ASSUME IT IS 2D CACULATIONS.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    lum, lum_err, rads, pones, ptaus, indes, contr, jused, jfact_used\n",
    "    \n",
    "    rads: np.ndarray\n",
    "        Radiance (i.e. specific intensities) for each ray.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # init\n",
    "    npart : int = len(sdf)\n",
    "    nray  : int = len(rays)\n",
    "    if kernel is None: kernel = sdf.kernel\n",
    "    kernel_rad = float(kernel.get_radius())\n",
    "    #kernel_col = kernel.get_column_kernel_func(samples=1000) # w integrated from z\n",
    "    if kernel_col is None or kernel_csz is None:\n",
    "        kernel_col, kernel_csz, _, _ = get_col_kernel_funcs(kernel)\n",
    "    if ray_unit_vec is None: ray_unit_vec = get_ray_unit_vec(rays[0])\n",
    "    \n",
    "    pts    = np.array(sdf[xyzs_names_list], order='C')    # (npart, 3)-shaped array (must be this shape for pts_order sorting below)\n",
    "    hs     = np.array(sdf[ 'h'           ], order='C')    # npart-shaped array\n",
    "    masses = np.array(sdf[ 'm'           ], order='C')\n",
    "    kappas = np.array(sdf[ 'kappa'       ], order='C')\n",
    "    srcfuncs = np.array(srcfuncs          , order='C')\n",
    "    ndim   = pts.shape[-1]\n",
    "    mkappa_div_h2_arr = masses * kappas / hs**(ndim-1)\n",
    "    \n",
    "    # sanity check\n",
    "    if is_verbose(verbose, 'err') and not np.allclose(ray_unit_vec, get_rays_unit_vec(rays)):\n",
    "        raise ValueError(f\"Inconsistent ray_unit_vec {ray_unit_vec} with the rays.\")\n",
    "\n",
    "    if is_verbose(verbose, 'warn') and ndim != 3:\n",
    "        say('warn', None, verbose, f\"ndim == {ndim} is not 3.\")\n",
    "\n",
    "    if is_verbose(verbose, 'fatal') and not np.allclose(ray_unit_vec, np.array([0., 0., 1.])):\n",
    "        raise NotImplementedError(\n",
    "            f\"Unsupported {ray_unit_vec=}:\"+\n",
    "            \"currently all rays must point towards +z direction (ray_unit_vec = np.array([0., 0., 1.])) \")\n",
    "    # *** warning: the following line only works with +z point rays ***\n",
    "    rays_xy = rays[:, 0, 0:2]\n",
    "\n",
    "    # (npart-shaped array of the indices of the particles from closest to the observer to the furthest)\n",
    "    pts_order             = np.argsort( np.sum(pts * ray_unit_vec, axis=-1) )[::-1]\n",
    "    pts_ordered           = pts[     pts_order]\n",
    "    hs_ordered            = hs[      pts_order]\n",
    "    mkappa_div_h2_ordered = mkappa_div_h2_arr[pts_order]\n",
    "    srcfuncs_ordered      = srcfuncs[pts_order]\n",
    "\n",
    "    # get used particles indexes\n",
    "    if parallel:\n",
    "        rads, pones, ptaus, indes, contr, jfact, estis = _integrate_along_rays_gridxy_sub_parallel(\n",
    "            pts_ordered, hs_ordered, mkappa_div_h2_ordered, srcfuncs_ordered,\n",
    "            rays_xy, ray_areas, kernel_rad, kernel_col, kernel_csz, kernel.w,\n",
    "            pts_order, rel_tol=rel_tol)\n",
    "    else:\n",
    "        raise NotImplementedError(\"parallel=False version of this function not yet implemented.\")\n",
    "\n",
    "    jused = np.where(jfact)\n",
    "    jfact_used = jfact[jused]\n",
    "    pts_order_used = pts_order[jused]\n",
    "    \n",
    "    lum  = 4 * pi * (rads * ray_areas).sum()\n",
    "    lum2 = 4 * pi * (srcfuncs_ordered[jused] * jfact_used).sum()\n",
    "    say('debug', None, verbose,\n",
    "        f\"{lum = }, {lum2 = }\",\n",
    "        f\"{rads.shape=}, {ray_areas.shape=}\",\n",
    "        f\"{srcfuncs[jused].shape=}, {jfact_used.shape=}\",\n",
    "    )\n",
    "    #assert np.isclose(lum, lum2)\n",
    "\n",
    "    if srcfuncs_err is None:\n",
    "        # calc error of source function now\n",
    "        sdf['srcfunc'] = srcfuncs\n",
    "        srcfuncs_grad_used = get_sph_gradient(\n",
    "            sdf,\n",
    "            val_names   ='srcfunc',\n",
    "            locs        = pts_ordered[     jused],\n",
    "            vals_at_locs= srcfuncs_ordered[jused],\n",
    "            hs_at_locs  = hs_ordered[      jused],\n",
    "            kernel      = kernel,\n",
    "            hfact       = hfact,\n",
    "            sdf_kdtree  = sdf_kdtree,\n",
    "            ndim        = ndim,\n",
    "            xyzs_names_list=xyzs_names_list,\n",
    "            parallel    = parallel,\n",
    "            verbose     = verbose,\n",
    "        )[:, :, 0]    # get_sph_gradient returns a (nlocs, ndim, nvals)-shaped np.ndarray\n",
    "        srcfuncs_err_used = np.sum(srcfuncs_grad_used**2, axis=1)**0.5 * hs_ordered[jused] * err_h\n",
    "    else:\n",
    "        srcfuncs_err_used = srcfuncs_err[pts_order_used]\n",
    "\n",
    "    lum_err = 4 * pi * ((srcfuncs_err_used * jfact_used)**2).sum()**0.5\n",
    "    \n",
    "\n",
    "    if is_verbose(verbose, 'info'):\n",
    "        nused = len(jfact_used)\n",
    "        say('info', None, verbose,\n",
    "            f\"{nused} particles actually participated calculation\",\n",
    "            f\"({int(nused/npart*10000)/100.}% of all particles,\",\n",
    "            f\"average {int(nused/nray*100)/100.} per ray.)\", sep=' ')\n",
    "\n",
    "    \n",
    "    return lum, lum_err, rads, pones, ptaus, indes, contr, pts_order_used, jfact_used, estis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394b11f2-040f-484e-9ed4-6132700848e1",
   "metadata": {},
   "source": [
    "#### Test codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6399d9d2-e267-49ca-bbe7-564d78d8b335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test runs\n",
    "@jit(nopython=True, parallel=True, fastmath=True)\n",
    "def _integrate_along_rays_gridxy_sub_parallel(\n",
    "    pts_ordered          : npt.NDArray[np.float64],    # (npart, 3)-shaped\n",
    "    hs_ordered           : npt.NDArray[np.float64],    # (npart,  )-shaped\n",
    "    mkappa_div_h2_ordered: npt.NDArray[np.float64],    # (npart,  )-shaped\n",
    "    srcfuncs_ordered     : npt.NDArray[np.float64],    # (npart,  )-shaped\n",
    "    rays_xy              : npt.NDArray[np.float64],    # (nray,  2)-shaped\n",
    "    ray_areas            : npt.NDArray[np.float64],    # (nray,   )-shaped\n",
    "    kernel_rad           : float,\n",
    "    kernel_col           : numba.core.registry.CPUDispatcher,\n",
    "    kernel_csz           : numba.core.registry.CPUDispatcher,\n",
    "    kernel_w             : numba.core.registry.CPUDispatcher,\n",
    "    pts_order            : npt.NDArray[np.float64],    # (npart,  )-shaped\n",
    "    rel_tol              : float = 1e-16, # because float64 has only 16 digits accuracy\n",
    "    nsample              : int   = 100,  # no of sample points for integration\n",
    ") -> tuple[\n",
    "    npt.NDArray[np.float64],    # anses\n",
    "    npt.NDArray[np.float64],    # pones\n",
    "    npt.NDArray[np.float64],    # ptaus\n",
    "    npt.NDArray[np.int64  ],    # indes\n",
    "    npt.NDArray[np.float64],    # contr\n",
    "    npt.NDArray[np.float64],    # jfact\n",
    "    npt.NDArray[np.float64],    # estis\n",
    "]:\n",
    "    \"\"\"Sub process for integrate_along_ray_gridxy(). Numba parallel version (using prange).\n",
    "    ---------------------------------------------------------------------------\n",
    "\n",
    "    Calculating the luminosity using\n",
    "    $$\n",
    "    L \\approx\n",
    "        4 \\pi \\sum_i \\sum_j \\triangle A_i\n",
    "            S_j \\frac{\\kappa_j m_j}{h_j^2}\n",
    "            \\int\n",
    "                e^{\n",
    "                    -\\sum_k \\frac{\\kappa_k m_k}{h_k^2}\n",
    "                    w_\\mathrm{csz}(q_{xy, ik}, \\, -q_{z, k})\n",
    "                }\n",
    "                w(q_{ij}) d(q_{z, j})\n",
    "    $$\n",
    "\n",
    "    Unit vec must be [0., 0., 1.] (i.e. all rays must point upwards towards +z)\n",
    "\n",
    "    Private function. Assumes specific input type. See source code comments.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    anses, pones, ptaus, indes, contr, jfact\n",
    "    \n",
    "    anses: (nray,)-shaped np.ndarray[float]\n",
    "        Radiance (i.e. specific intensities) for each ray.\n",
    "\n",
    "    pones: (nray,)-shaped np.ndarray[float]\n",
    "        <1> for each pixel,\n",
    "        i.e. same integration of radiance but for a constant 'srcfunc' of '1', for each ray.\n",
    "        Helpful for consistency check (should be more or less 1 where ptaus is nan.)\n",
    "        do weighted average by weight of areas per pixel to get the total area of the object!\n",
    "\n",
    "    ptaus: (nray,)-shaped np.ndarray[float]\n",
    "        The optical depth for each pixel\n",
    "        *** WILL BE np.nan IF OPTICAL DEPTH IS DEEP (which will be MOST OF THE TIME.)  ***\n",
    "        can be used as an alternative way to calculate the area of the object.\n",
    "\n",
    "    indes: (nray,)-shaped np.ndarray[int]\n",
    "        indexes of the max contribution particle\n",
    "\n",
    "    contr: (nray,)-shaped np.ndarray[float]\n",
    "        relative contribution (in fractions) of the max contribution particle\n",
    "\n",
    "    jfact: (npart,)-shaped np.ndarray[float]\n",
    "        Contribution factor for j-th particle.\n",
    "        Multiply it with 4 * pi * srcfuncs and sum it up as an alternative way to get the luminosity.\n",
    "        Will be zero if particle is not used.\n",
    "\n",
    "    estis: (nray,)-shaped np.ndarray[float]\n",
    "        Estimations of radiance (i.e. specific intensities) for each ray, using old method\n",
    "        (Also estis means 'was' in Esperanto, so it's a fitting name)\n",
    "\n",
    "    \"\"\"\n",
    "    #raise NotImplementedError\n",
    "\n",
    "    nray  = len(rays_xy)\n",
    "    npart = len(srcfuncs_ordered)\n",
    "    ndim  = pts_ordered.shape[-1]\n",
    "    anses = np.zeros(nray)\n",
    "    indes = np.zeros(nray, dtype=np.int64)    # indexes of max contribution particle\n",
    "    contr = np.zeros(nray)     # relative contribution of the max contribution particle\n",
    "    # ifact = np.zeros(nray)     # effective xsec for i-th ray  # NOTE: ifact = pones * ray_areas\n",
    "    jfact = np.zeros(npart)    # effective xsec for j-th particle\n",
    "    pones = np.zeros(nray)\n",
    "    ptaus = np.full(nray, np.nan)    # lower bound of the optical depth\n",
    "    estis = np.zeros(nray)\n",
    "    nsample_half = int(nsample/2)\n",
    "    \n",
    "\n",
    "    # error tolerance of tau (part 1)\n",
    "    tol_tau_base = np.log(srcfuncs_ordered.sum()) - np.log(rel_tol)\n",
    "\n",
    "\n",
    "    # cache calcs\n",
    "    hrs_ordered = hs_ordered * kernel_rad\n",
    "\n",
    "    \n",
    "\n",
    "    # loop over ray\n",
    "    for i in prange(nray):\n",
    "        ray_xy  = rays_xy[i]\n",
    "        ray_area= ray_areas[i]\n",
    "        tau     = 0.\n",
    "        ans     = 0.\n",
    "        dans    = 0.\n",
    "        rad_est = 0.   # estimation of radiance (i.e. ans)\n",
    "        # dans_max_tmp = 0.\n",
    "        dfac_max_tmp = 0.\n",
    "        ind     = -1\n",
    "        fac     = 0. # effectively <1>\n",
    "        dfac    = 0. # factor\n",
    "        used_j  = 0    # j = used_indexes[used_j]\n",
    "        # dLi_div_4pikappaj = 0.    # is sum_Sk_p_Aeffk_div_p_kappaj, for estimation os error from \\\\delta \\\\kappa\n",
    "\n",
    "        #   xy-grid specific solution\n",
    "        ray_x = ray_xy[0]\n",
    "        ray_y = ray_xy[1]\n",
    "\n",
    "        \n",
    "\n",
    "        # First, try to find out how many relevant particles are there\n",
    "        \n",
    "        nused_i = 0    # no of used particles for i-th ray\n",
    "        for j in range(npart):\n",
    "            x_j = pts_ordered[j, 0]\n",
    "            y_j = pts_ordered[j, 1]\n",
    "            hr = hrs_ordered[j]\n",
    "            # check if the particle is within range\n",
    "            #   xy-grid specific solution\n",
    "            if ray_x - hr < x_j and x_j < ray_x + hr and ray_y - hr < y_j and y_j < ray_y + hr:\n",
    "                nused_i += 1\n",
    "\n",
    "        # now we know roughly how many particles are relevant...\n",
    "        \n",
    "        used_indexes = np.full(nused_i, -1, dtype=np.int64)\n",
    "        used_dtaus   = np.full(nused_i, np.nan)\n",
    "        used_qs_xy   = np.full(nused_i, np.nan)\n",
    "\n",
    "        used_j = 0    # j = used_indexes[used_j]\n",
    "        tau    = 0.\n",
    "        rad_est= 0.   # estimation of radiance (i.e. ans)\n",
    "        for j in range(npart):\n",
    "            x_j = pts_ordered[j, 0]\n",
    "            y_j = pts_ordered[j, 1]\n",
    "            hr  = hrs_ordered[j]\n",
    "            \n",
    "            # check if the particle is within range\n",
    "            #   xy-grid specific solution\n",
    "            if ray_x - hr < x_j and x_j < ray_x + hr and ray_y - hr < y_j and y_j < ray_y + hr:\n",
    "                h = hs_ordered[ j]\n",
    "                q_xy = ((x_j - ray_x)**2 + (y_j - ray_y)**2)**0.5 / h\n",
    "                if q_xy < kernel_rad:\n",
    "\n",
    "                    # log\n",
    "                    mkappa_div_h2 = mkappa_div_h2_ordered[j]\n",
    "                    srcfunc = srcfuncs_ordered[j]\n",
    "                    dtau = mkappa_div_h2 * kernel_col(q_xy, ndim)\n",
    "                    \n",
    "                    used_indexes[used_j] = j\n",
    "                    used_dtaus[  used_j] = dtau\n",
    "                    used_qs_xy[  used_j] = q_xy\n",
    "                    used_j += 1\n",
    "\n",
    "                    dfac = np.exp(-tau) * (1. - np.exp(-dtau))\n",
    "                    #dans = dfac * srcfunc\n",
    "                    rad_est += dfac * srcfunc #dans\n",
    "                    tau += dtau\n",
    "                    \n",
    "                    # terminate the calc for this ray if tau is sufficient large\n",
    "                    #    such that the relative error on ans is smaller than rel_tol\n",
    "                    # i.e. since when tau > np.log(srcfuncs_ordered.sum()) - np.log(rel_tol) - np.log(ans),\n",
    "                    #    we know that ans[i] - ans[i][k] < rel_tol * ans[i]\n",
    "                    # see my notes for derivation\n",
    "                    if tau > tol_tau_base - np.log(rad_est):\n",
    "                        break\n",
    "        else:\n",
    "            ptaus[i]=tau\n",
    "        nused_i = used_j     # update used indexes size\n",
    "        estis[i]= rad_est\n",
    "        \n",
    "\n",
    "        \n",
    "        # Now, loop over particles\n",
    "        \n",
    "        for used_j in range(nused_i):\n",
    "            j    = used_indexes[used_j]\n",
    "            #pt   = pts_ordered[j]\n",
    "            q_xy = used_qs_xy[used_j] # ((pt[0] - ray_x)**2 + (pt[1] - ray_y)**2)**0.5 / h\n",
    "            hr   = hrs_ordered[j]\n",
    "            h    = hs_ordered[ j]\n",
    "\n",
    "            \n",
    "            # now do radiative transfer\n",
    "            \n",
    "            mkappa_div_h2 = mkappa_div_h2_ordered[j]\n",
    "            srcfunc = srcfuncs_ordered[j]\n",
    "            \n",
    "\n",
    "            # get optical depth\n",
    "            \n",
    "            z       = pts_ordered[j, 2]\n",
    "            q_z     = z / h\n",
    "            zmhr    = z - hr\n",
    "            zphr    = z + hr\n",
    "            dzs_j   = (zphr - zmhr) / nsample\n",
    "            zs_j    = np.linspace(zmhr + dzs_j/2., zphr - dzs_j/2., nsample)\n",
    "            taus_j  = np.zeros(nsample)\n",
    "            tau_pr  = 0.   # \\tau'_j - the summed optical depth for particles that are 'fully' ahead of j-th particle\n",
    "            #assert np.isclose(dzs_j, zs_j[1] - zs_j[0])\n",
    "            for used_k in range(nused_i):\n",
    "                k = used_indexes[used_k]\n",
    "                z_k  = pts_ordered[k, 2]\n",
    "                hr_k = hrs_ordered[k]\n",
    "                # ****** NOTE: the following could use optimization ******\n",
    "                #    e.g. maybe generate a list of k beforehand?\n",
    "                if zmhr - hr_k < z_k:\n",
    "                    # particle being relevant\n",
    "                    if           z_k < zphr + hr_k:\n",
    "                        # particle range intersects\n",
    "                        h_k = hs_ordered[k]\n",
    "                        mkappa_div_h2_k = mkappa_div_h2_ordered[k]\n",
    "                        q_xy_k = used_qs_xy[used_k] #((x_k - ray_x)**2 + (y_k - ray_y)**2)**0.5 / h_k\n",
    "                        for ji in range(nsample):\n",
    "                            q_z_k  = (zs_j[ji] - z_k) / h_k\n",
    "                            taus_j[ji] += mkappa_div_h2_k * kernel_csz(q_xy_k, -q_z_k, ndim)\n",
    "                    else:\n",
    "                        # particle is fully ahead\n",
    "                        tau_pr += used_dtaus[used_k]\n",
    "\n",
    "            \n",
    "            # integrate through opitcal depth for j\n",
    "            dfac  = 0.\n",
    "            q2_xy = q_xy**2\n",
    "            dqz_j = dzs_j / h\n",
    "            for ji in range(nsample):\n",
    "                # ****** Integration pending improvement ******\n",
    "                # ****** same for the integration in w_csz ******\n",
    "                q_ji = (q2_xy + ((zs_j[ji] - z)/h)**2)**0.5\n",
    "                dfac += np.exp(-taus_j[ji]) * kernel_w(q_ji, ndim) * dqz_j\n",
    "            dfac *= mkappa_div_h2 * np.exp(-tau_pr)\n",
    "            dans  = dfac * srcfunc\n",
    "            ans  += dans    # for getting <S>\n",
    "            fac  += dfac    # for getting <1>\n",
    "            #tau = tau_pr + taus_j[nsample_half]\n",
    "            \n",
    "\n",
    "            # calc error from kappa - To be continued\n",
    "            \n",
    "            # dLi_div_4pikappaj += dans / kappa # kappa import TBD!\n",
    "            # Sm_div_h2 = srcfunc * mkappa_div_h2 / kappa\n",
    "            # for used_k in range(nused_i):\n",
    "            #     k = used_indexes[used_k]\n",
    "            #     z_k  = pts_ordered[k, 2]\n",
    "            #     hr_k = hrs_ordered[k]\n",
    "            #     # ****** NOTE: the following could use optimization ******\n",
    "            #     if zmhr - hr_k < z_k and z_k < zphr + hr_k: # particle range intersects\n",
    "            #         h_k = hs_ordered[k]\n",
    "            #         dqz_k = dqz_j * h / h_k\n",
    "            #         mkappa_div_h2_k = mkappa_div_h2_ordered[k]\n",
    "            #         q2_xy_k = used_qs_xy[used_k]**2\n",
    "            #         fac12138 = 0.\n",
    "            #         for ji in range(nsample):\n",
    "            #             q_ki = np.sqrt(q2_xy_k + ((zs_j[ji] - z_k) / h_k)**2)\n",
    "            #             q_z  = (zs_j[ji] - z) / h\n",
    "            #             fac12138 += kernel_csz(q_xy, -q_z, ndim) * np.exp(-taus_j[ji]) * kernel_w(q_ki, ndim) * dqz_k\n",
    "            #         dLi_div_4pikappaj -= Sm_div_h2 * mkappa_div_h2_k * fac12138\n",
    "\n",
    "            \n",
    "            jfact[j] += dfac * ray_area\n",
    "\n",
    "            # # note down the largest contributor\n",
    "            # if dans > dans_max_tmp:\n",
    "            #     dans_max_tmp = dans\n",
    "            #     ind = pts_order[j]\n",
    "            if dfac > dfac_max_tmp:\n",
    "                dfac_max_tmp = dfac\n",
    "                ind = pts_order[j]\n",
    "\n",
    "\n",
    "            ## terminate the calc for this ray if tau is sufficiently large\n",
    "            #if tau > tol_tau_base - np.log(ans):\n",
    "            #    break\n",
    "            \n",
    "        anses[i] = ans\n",
    "        indes[i] = ind\n",
    "        if ans > 0: contr[i] = dfac_max_tmp / fac  # dans_max_tmp / ans\n",
    "        pones[i] = fac\n",
    "    \n",
    "    return anses, pones, ptaus, indes, contr, jfact, estis\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a391db2-a336-4e5b-96be-d3c0692e96ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# integrate with error estiamtes\n",
    "\n",
    "def integrate_along_rays_gridxy(\n",
    "    sdf         : sarracen.SarracenDataFrame,\n",
    "    srcfuncs    : npt.NDArray[np.float64],    # (npart,  )-shaped\n",
    "    srcfuncs_err: None|npt.NDArray[np.float64],    # (npart,  )-shaped\n",
    "    rays        : npt.NDArray[np.float64],    # (nray, 2, 3)-shaped\n",
    "    ray_areas   : npt.NDArray[np.float64],    # (nray,     )-shaped\n",
    "    ray_unit_vec: None|npt.NDArray[np.float64] = None,    # (nray, 3)-shaped\n",
    "    kernel      : None|sarracen.kernels.BaseKernel = None,\n",
    "    kernel_col  : None|numba.core.registry.CPUDispatcher = None,\n",
    "    kernel_csz  : None|numba.core.registry.CPUDispatcher = None,\n",
    "    hfact       : None|float = None,\n",
    "    parallel    : bool = False,\n",
    "    err_h       : float = 1.0,\n",
    "    rel_tol     : float = 1e-16,\n",
    "    sdf_kdtree  : None|kdtree.KDTree = None,\n",
    "    xyzs_names_list : list = ['x', 'y', 'z'],\n",
    "    verbose     : int = 3,\n",
    ") -> tuple[\n",
    "    float,    # lum\n",
    "    float,    # lum_err\n",
    "    npt.NDArray[np.float64],    # rads\n",
    "    npt.NDArray[np.float64],    # pones\n",
    "    npt.NDArray[np.float64],    # ptaus\n",
    "    npt.NDArray[np.int64  ],    # indes\n",
    "    npt.NDArray[np.float64],    # contr\n",
    "    npt.NDArray[np.int64  ],    # pts_order_used\n",
    "    npt.NDArray[np.float64],    # jfact_used - 'j' for j-th particle;\n",
    "    # 'used' means the particles (in the order_used list) that actually participated in the calculation\n",
    "]:\n",
    "    \"\"\"Backward integration of source functions along a grided ray (traced backwards), weighted by optical depth.\n",
    "    ---------------------------------------------------------------------------\n",
    "    \n",
    "    Assuming all rays facing +z direction\n",
    "    (with the same ray_unit_vec [0., 0., 1.])\n",
    "\n",
    "    WARNING: will overwrite sdf['srcfunc'] if srcfuncs_err is None.\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sdf: sarracen.SarracenDataFrame\n",
    "        Must contain columns: x, y, z, h, m, kappa\n",
    "        \n",
    "    rays: (nray, 2, 3)-shaped array\n",
    "        Representing the ray trajectory. Currently only straight infinite lines are supported.\n",
    "        each ray is of the format:\n",
    "        [[begin point], [end point]]\n",
    "        where the end point is closer to the observer.\n",
    "\n",
    "    srcfuncs: 1D array\n",
    "        arrays describing the source function for every particle\n",
    "        \n",
    "    kernel: sarracen.kernels.base_kernel\n",
    "        Smoothing kernel for SPH data interpolation.\n",
    "        If None, will use the one in sdf.\n",
    "\n",
    "    kernel_csz: func\n",
    "        Cumulative summed kernel along z axis:\n",
    "        $ \\int_{-w_\\mathrm{rad}}^{q_z} w(\\sqrt{q_{xy}^2 + q_z^2}) dq_z $\n",
    "\n",
    "    hfact : None|float\n",
    "\n",
    "    parallel: bool\n",
    "        If to use the numba parallel function\n",
    "\n",
    "    err_h: float ( > 0. )\n",
    "        determine confidence level.\n",
    "        e.g.,\n",
    "            1.0 will give error assuming error range is +/-1.0 smoothing length h;\n",
    "            0.5 will give error assuming error range is +/-0.5 smoothing length h;\n",
    "            etc. etc.\n",
    "            \n",
    "    rel_tol : float\n",
    "        maximum relative error tolerence per ray.\n",
    "        Default 1e-15 because float64 is only accurate to ~16th digits.\n",
    "\n",
    "    sdf_kdtree : kdtree.KDTree\n",
    "        KDTree built from sdf[['x', 'y', 'z']], for fast neighbour search.\n",
    "        if None, will build one.\n",
    "        \n",
    "    xyzs_names_list: list\n",
    "        list of names of the columns that represents x, y, z axes (i.e. coord axes names)\n",
    "        MUST INCLUDE ALL THREE AXES LABELS.\n",
    "        If only 2 is included, WILL ASSUME IT IS 2D CACULATIONS.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    lum, lum_err, rads, pones, ptaus, indes, contr, jused, jfact_used\n",
    "    \n",
    "    rads: np.ndarray\n",
    "        Radiance (i.e. specific intensities) for each ray.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # init\n",
    "    npart : int = len(sdf)\n",
    "    nray  : int = len(rays)\n",
    "    if kernel is None: kernel = sdf.kernel\n",
    "    kernel_rad = float(kernel.get_radius())\n",
    "    #kernel_col = kernel.get_column_kernel_func(samples=1000) # w integrated from z\n",
    "    if kernel_col is None or kernel_csz is None:\n",
    "        kernel_col, kernel_csz, _, _ = get_col_kernel_funcs(kernel)\n",
    "    if ray_unit_vec is None: ray_unit_vec = get_ray_unit_vec(rays[0])\n",
    "    \n",
    "    pts    = np.array(sdf[xyzs_names_list], order='C')    # (npart, 3)-shaped array (must be this shape for pts_order sorting below)\n",
    "    hs     = np.array(sdf[ 'h'           ], order='C')    # npart-shaped array\n",
    "    masses = np.array(sdf[ 'm'           ], order='C')\n",
    "    kappas = np.array(sdf[ 'kappa'       ], order='C')\n",
    "    srcfuncs = np.array(srcfuncs          , order='C')\n",
    "    ndim   = pts.shape[-1]\n",
    "    mkappa_div_h2_arr = masses * kappas / hs**(ndim-1)\n",
    "    \n",
    "    # sanity check\n",
    "    if is_verbose(verbose, 'err') and not np.allclose(ray_unit_vec, get_rays_unit_vec(rays)):\n",
    "        raise ValueError(f\"Inconsistent ray_unit_vec {ray_unit_vec} with the rays.\")\n",
    "\n",
    "    if is_verbose(verbose, 'warn') and ndim != 3:\n",
    "        say('warn', None, verbose, f\"ndim == {ndim} is not 3.\")\n",
    "\n",
    "    if is_verbose(verbose, 'fatal') and not np.allclose(ray_unit_vec, np.array([0., 0., 1.])):\n",
    "        raise NotImplementedError(\n",
    "            f\"Unsupported {ray_unit_vec=}:\"+\n",
    "            \"currently all rays must point towards +z direction (ray_unit_vec = np.array([0., 0., 1.])) \")\n",
    "    # *** warning: the following line only works with +z point rays ***\n",
    "    rays_xy = rays[:, 0, 0:2]\n",
    "\n",
    "    # (npart-shaped array of the indices of the particles from closest to the observer to the furthest)\n",
    "    pts_order             = np.argsort( np.sum(pts * ray_unit_vec, axis=-1) )[::-1]\n",
    "    pts_ordered           = pts[     pts_order]\n",
    "    hs_ordered            = hs[      pts_order]\n",
    "    mkappa_div_h2_ordered = mkappa_div_h2_arr[pts_order]\n",
    "    srcfuncs_ordered      = srcfuncs[pts_order]\n",
    "\n",
    "    # get used particles indexes\n",
    "    if parallel:\n",
    "        rads, pones, ptaus, indes, contr, jfact, estis = _integrate_along_rays_gridxy_sub_parallel(\n",
    "            pts_ordered, hs_ordered, mkappa_div_h2_ordered, srcfuncs_ordered,\n",
    "            rays_xy, ray_areas, kernel_rad, kernel_col, kernel_csz, kernel.w,\n",
    "            pts_order, rel_tol=rel_tol)\n",
    "    else:\n",
    "        raise NotImplementedError(\"parallel=False version of this function not yet implemented.\")\n",
    "\n",
    "    jused = np.where(jfact)\n",
    "    jfact_used = jfact[jused]\n",
    "    pts_order_used = pts_order[jused]\n",
    "    \n",
    "    lum  = 4 * pi * (rads * ray_areas).sum()\n",
    "    lum2 = 4 * pi * (srcfuncs_ordered[jused] * jfact_used).sum()\n",
    "    say('debug', None, verbose,\n",
    "        f\"{lum = }, {lum2 = }\",\n",
    "        f\"{rads.shape=}, {ray_areas.shape=}\",\n",
    "        f\"{srcfuncs[jused].shape=}, {jfact_used.shape=}\",\n",
    "    )\n",
    "    #assert np.isclose(lum, lum2)\n",
    "\n",
    "    if srcfuncs_err is None:\n",
    "        # calc error of source function now\n",
    "        sdf['srcfunc'] = srcfuncs\n",
    "        srcfuncs_grad_used = get_sph_gradient(\n",
    "            sdf,\n",
    "            val_names   ='srcfunc',\n",
    "            locs        = pts_ordered[     jused],\n",
    "            vals_at_locs= srcfuncs_ordered[jused],\n",
    "            hs_at_locs  = hs_ordered[      jused],\n",
    "            kernel      = kernel,\n",
    "            hfact       = hfact,\n",
    "            sdf_kdtree  = sdf_kdtree,\n",
    "            ndim        = ndim,\n",
    "            xyzs_names_list=xyzs_names_list,\n",
    "            parallel    = parallel,\n",
    "            verbose     = verbose,\n",
    "        )[:, :, 0]    # get_sph_gradient returns a (nlocs, ndim, nvals)-shaped np.ndarray\n",
    "        srcfuncs_err_used = np.sum(srcfuncs_grad_used**2, axis=1)**0.5 * hs_ordered[jused] * err_h\n",
    "    else:\n",
    "        srcfuncs_err_used = srcfuncs_err[pts_order_used]\n",
    "\n",
    "    lum_err = 4 * pi * ((srcfuncs_err_used * jfact_used)**2).sum()**0.5\n",
    "    \n",
    "\n",
    "    if is_verbose(verbose, 'info'):\n",
    "        nused = len(jfact_used)\n",
    "        say('info', None, verbose,\n",
    "            f\"{nused} particles actually participated calculation\",\n",
    "            f\"({int(nused/npart*10000)/100.}% of all particles,\",\n",
    "            f\"average {int(nused/nray*100)/100.} per ray.)\", sep=' ')\n",
    "\n",
    "    \n",
    "    return lum, lum_err, rads, pones, ptaus, indes, contr, pts_order_used, jfact_used, estis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ea50ff-b1e8-4239-9172-0982ef13ede3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### rays grid generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5aa6efe-41e2-4f0a-be7f-b5307c1b7708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xy_grids_of_rays(\n",
    "    sdf  : None|sarracen.SarracenDataFrame = None,\n",
    "    #dXs  : None|list[list[float], list[float]]|np.ndarray= None,\n",
    "    no_xy: tuple[int, int] = (32, 32),\n",
    "    #orig_vec: np.ndarray = np.zeros(3),\n",
    "    frac_contained: float = 100., #99.73,\n",
    "    use_adaptive_grid: bool = False,\n",
    "    xyzs_names_list : list = ['x', 'y', 'z'],\n",
    "    w_rad: None|float = None,\n",
    "    verbose: int = 3,\n",
    ") -> tuple[np.ndarray, np.ndarray, list[np.ndarray]]:\n",
    "    \"\"\"Get a grid of rays (must pointing at z direction (i.e. xyzs_names_list[-1] direction) for now).\n",
    "\n",
    "    Supply either sdf or both dx and dy.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sdf: sarracen.SarracenDataFrame\n",
    "\n",
    "    no_xy: tuple[int, int]\n",
    "        number of the rays per axes.\n",
    "        \n",
    "    frac_contained : float\n",
    "        Suggested percentage of the particle that are contained within the grid. in (0, 100]\n",
    "\n",
    "    use_adaptive_grid : bool\n",
    "        if True,\n",
    "            will scale dXs according to particle distribution instead of even intervals,\n",
    "            if dXs is None or (None, None).\n",
    "\n",
    "    xyzs_names_list: list\n",
    "        list of names of the columns that represents x, y, z axes (i.e. coord axes names)\n",
    "        WARNING: since ndim==len(xyzs_names_list), if len(xyzs_names_list) !=3 will resulting non-3D results.\n",
    "            In which case you will need to change no_xy as well.\n",
    "\n",
    "\n",
    "    Returns: rays, areas, dXs\n",
    "    -------\n",
    "    rays: (no_ray, 2, 3)-shaped np.ndarray\n",
    "\n",
    "    areas: (no_ray)-shaped np.ndarray\n",
    "        areas corresponding to each ray in the grid\n",
    "        \n",
    "    dXs: list of no_xy[i]-shaped np.ndarray\n",
    "        width of the grid cells. in sdf units['dist'].\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    unit_vec = np.zeros(len(xyzs_names_list))\n",
    "    unit_vec[-1] = 1.\n",
    "    #x0, y0, z0 = orig_vec\n",
    "    z0 = 0.    # z value for rays\n",
    "    if w_rad is None:\n",
    "        w_rad = sdf.kernel.get_radius()\n",
    "\n",
    "    # sanity checks\n",
    "    if is_verbose(verbose, 'warn') and len(xyzs_names_list) != 3:\n",
    "        say('warn', 'get_xy_grids_of_rays()', verbose,\n",
    "            f\"xyzs_names_list being {xyzs_names_list}, its len = {len(xyzs_names_list)} is not 3.\",\n",
    "            f\"This means we are assuming {len(xyzs_names_list)}D.\")\n",
    "    if is_verbose(verbose, 'error') and len(xyzs_names_list) != len(no_xy) + 1:\n",
    "        say('error', 'get_xy_grids_of_rays()', verbose,\n",
    "            f\"ndim (=={len(xyzs_names_list)}) != len(no_xy) (=={len(no_xy)}) + 1\",\n",
    "            f\"i.e. asked ray grid dimension {no_xy} does not makes sense.\",\n",
    "            \"This will likely cause error in the next steps.\")\n",
    "    \n",
    "    # get dx & dy\n",
    "    frac_contained_m = 50. - frac_contained / 2.\n",
    "    frac_contained_p = 50. + frac_contained / 2.\n",
    "\n",
    "    Xs_edges = []\n",
    "    for i, label in enumerate(xyzs_names_list[:-1]):\n",
    "        #i0 = orig_vec[i]\n",
    "        if use_adaptive_grid:\n",
    "            # fraction points for the adaptive grid\n",
    "            fracs = np.linspace(frac_contained_m, frac_contained_p, no_xy[i]+1)\n",
    "            # edge points for the grid\n",
    "            Xs_edges.append(\n",
    "                np.percentile(np.concatenate((sdf[label] - w_rad*sdf['h'], sdf[label] + w_rad*sdf['h'])), fracs))\n",
    "        else:\n",
    "            Xs_edges.append(\n",
    "                np.linspace(\n",
    "                    *np.percentile(\n",
    "                        np.concatenate(\n",
    "                            (sdf[label] - w_rad*sdf['h'], sdf[label] + w_rad*sdf['h']),\n",
    "                        ), (frac_contained_m, frac_contained_p),\n",
    "                    ), no_xy[i]+1,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    dXs = np.array([np.diff(Xi_edges) for Xi_edges in Xs_edges])    # each item is (no_xy[i]+1,)-shaped\n",
    "    Xs_centers = np.array([Xi_edges[:-1] + dXi/2. for dXi, Xi_edges in zip(dXs, Xs_edges)])    # each item is (no_xy[i],)-shaped\n",
    "\n",
    "    # Note: orig_vecs must be 2D (i.e. in shape of (no_ray, 3))\n",
    "    orig_vecs = np.array([[*xy, z0] for xy in itertools.product(*Xs_centers)])\n",
    "    #orig_vecs = [[[x, y, z0] for x, y in zip(xs, ys)] for xs, ys in zip(*np.meshgrid(*xys))]\n",
    "    areas = np.array([dx*dy for dy in dXs[1] for dx in dXs[0]])\n",
    "\n",
    "    rays = mupl.geometry.get_rays(orig_vecs=orig_vecs, unit_vecs=unit_vec)\n",
    "    \n",
    "    return rays, areas, dXs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d16cde-9d77-4113-844d-c50637f3a07a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97c973f4-9f35-4dd6-ba34-7836d5a16625",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_imshow(\n",
    "    no_xy: tuple[int, int],\n",
    "    rays: units.Quantity|np.ndarray,\n",
    "    data: units.Quantity|np.ndarray,\n",
    "    job_profile  : dict= None,\n",
    "    file_index   : int = -1,\n",
    "    title_suffix : str =\"\",\n",
    "    notes        : dict= None,\n",
    "    data_label   : str =\"\",\n",
    "    save_label   : str =\"\",\n",
    "    xyzs         : str|list[str] = 'xyz',\n",
    "    out_exts     : list[str] = ['pdf', 'png'],\n",
    "    norm=None,\n",
    "    cmap=None,\n",
    "    output_dir:str|None=None,\n",
    "    verbose = 4,\n",
    "):\n",
    "    \"\"\"Plotting a heatmap (contourf) of 1D data located at rays\"\"\"\n",
    "\n",
    "\n",
    "    if not isinstance(data, units.Quantity):\n",
    "        data = set_as_quantity(data, units.dimensionless_unscaled)\n",
    "\n",
    "    if not isinstance(rays, units.Quantity):\n",
    "        rays = set_as_quantity(rays, units.dimensionless_unscaled)\n",
    "\n",
    "    if job_profile is None:\n",
    "        job_profile = {\n",
    "            'plot_title_suffix': '',\n",
    "            'nickname'         : '',\n",
    "        }\n",
    "\n",
    "    #Xs = rays[:, 0, 0]\n",
    "    #Ys = rays[:, 0, 1]\n",
    "    rays_val = rays.reshape(*no_xy, *rays.shape[1:]).value\n",
    "    extent = (\n",
    "        rays_val[ 0, 0, 0, 0] - (rays_val[ 1, 0, 0, 0] - rays_val[ 0, 0, 0, 0])/2,\n",
    "        rays_val[-1,-1, 0, 0] + (rays_val[-1,-1, 0, 0] - rays_val[-2,-1, 0, 0])/2,\n",
    "        rays_val[ 0, 0, 0, 1] - (rays_val[ 0, 1, 0, 1] - rays_val[ 0, 0, 0, 1])/2,\n",
    "        rays_val[-1,-1, 0, 1] + (rays_val[-1,-1, 0, 1] - rays_val[-1,-2, 0, 1])/2,\n",
    "    )\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    \n",
    "    cax = ax.imshow(data.reshape(no_xy).T.value, norm=norm, cmap=cmap, origin='lower', extent=extent)\n",
    "    #cax = ax.contourf(Xs.reshape(no_xy), Ys.reshape(no_xy), data.reshape(no_xy), cmap=cmap)\n",
    "    fig.colorbar(cax, label=f\"{data_label} / {data.unit.to_string('latex_inline')}\")\n",
    "    ax.set_xlabel(f\"${xyzs[0]}$ / {rays.unit.to_string('latex_inline')}\")\n",
    "    ax.set_ylabel(f\"${xyzs[1]}$ / {rays.unit.to_string('latex_inline')}\")\n",
    "    if notes is not None:\n",
    "        ax.text(\n",
    "            0.98, 0.98,\n",
    "            f\"Time = {notes['time']:.1f}\\n\" + \\\n",
    "            f\" $L$ = {notes['lum']:.0f}\",\n",
    "            #f\" $L = {notes['lum'].value:.0f}$ {notes['lum'].unit.to_string('latex_inline')}\",\n",
    "            #color = \"black\",\n",
    "            ha = 'right', va = 'top',\n",
    "            transform=ax.transAxes,\n",
    "        )\n",
    "\n",
    "    \n",
    "    if output_dir is not None:\n",
    "        no_xy_txt = 'x'.join([f'{i}' for i in no_xy])\n",
    "        outfilename_noext = f\"{output_dir}heat_{job_profile['nickname']}_{file_index:05d}_{''.join(xyzs)}_{save_label}_{no_xy_txt}\"\n",
    "        outfilenames = []\n",
    "    \n",
    "        # write pdf\n",
    "        for out_ext in out_exts:\n",
    "            outfilename = f\"{outfilename_noext}.{out_ext}\"\n",
    "            if out_ext == 'pdf':\n",
    "                ax.set_title('')\n",
    "            else:\n",
    "                ax.set_title(f\"Heatmap of {data_label}\\n{job_profile['plot_title_suffix']}\")\n",
    "            fig.savefig(outfilename)\n",
    "            outfilenames.append(outfilename)\n",
    "            if is_verbose(verbose, 'note'):\n",
    "                say('note', None, verbose, f\"Fig saved to {outfilename}.\")\n",
    "        \n",
    "    return fig, ax, outfilenames\n",
    "\n",
    "## example\n",
    "#fig, ax = plot_imshow(\n",
    "#    no_xy, rays * units.Rsun, anses, data_label=\"$I$\", save_label=\"I_xyz\",\n",
    "#    job_profile=job_profile, file_index=file_index, output_dir=output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8daf8b-0b46-4deb-80a1-f6d4ad05bc06",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Error estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9cd2dae-0051-430f-b6f4-ee4d84316ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sph_neighbours(\n",
    "    sdf_kdtree : kdtree.KDTree,\n",
    "    xyz_i      : np.ndarray,\n",
    "    h_i        : float,\n",
    "    w_rad      : float,\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Find neighbours of xyz_i within (w_rad*h_i) distance, using k-d tree.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sdf_kdtree : kdtree.KDTree\n",
    "        KDTree built from sdf[['x', 'y', 'z']], for fast neighbour search.\n",
    "    xyz_i : (ndim,)-shaped numpy array\n",
    "        position of the querying point\n",
    "    h_i : float\n",
    "        Smoothing length\n",
    "    w_rad: float\n",
    "        radius of the smoothing kernel w.\n",
    "    \n",
    "    Returns: dists, indices\n",
    "    -------\n",
    "    dists : np.ndarray\n",
    "        distances of the neighbouring points to the querying point\n",
    "    indices : np.ndarray\n",
    "        indices of the neighbouring points\n",
    "    \"\"\"\n",
    "    npart = sdf_kdtree.n\n",
    "    dists, indices = sdf_kdtree.query(xyz_i, k=npart, distance_upper_bound=w_rad*h_i)\n",
    "    indices_indices = indices<npart\n",
    "    return dists[indices_indices], indices[indices_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6cc574e5-6910-40f0-b845-66e99c39055b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sph error estimation\n",
    "\n",
    "# found this func in my old codes ../photosphere/Analysis_PhLoc.ipynb\n",
    "\n",
    "def get_sph_error(\n",
    "    sdf        : sarracen.SarracenDataFrame,\n",
    "    target_labels   : str|list[str],\n",
    "    target_indicies : int|list[int]|np.ndarray = [],\n",
    "    err_h      : float    = 1.0,\n",
    "    sdf_kdtree : kdtree.KDTree = None,\n",
    "    kernel     : sarracen.kernels.BaseKernel = None,\n",
    "    xyzs_names_list : list = ['x', 'y', 'z'],\n",
    "    verbose: int = 3,\n",
    ") -> np.ndarray:    # (ntarget, nval)-shaped\n",
    "    \"\"\"Calculate error bar for sarracen data frame.\n",
    "    \n",
    "    Assuming 3D.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sdf: sarracen.SarracenDataFrame\n",
    "        Need to contain columns: x, y, z, m, h, rho.\n",
    "        If density (rho) is not in sdf, will compute rho.\n",
    "        \n",
    "    target_labels: str or list of str (len>2)\n",
    "        Column label of the target data in sdf for error computing\n",
    "        \n",
    "    target_indicies: int or list of int or np.ndarray\n",
    "        indices for particles in sdf for error calculating\n",
    "        \n",
    "    err_h: float ( > 0. )\n",
    "        determine confidence level.\n",
    "        e.g.,\n",
    "            1.0 will give error assuming error range is +/-1.0 smoothing length h;\n",
    "            0.5 will give error assuming error range is +/-0.5 smoothing length h;\n",
    "            etc. etc.\n",
    "            \n",
    "    sdf_kdtree: kdtree.KDTree\n",
    "        KDTree built from sdf[['x', 'y', 'z']], for fast neighbour search.\n",
    "        If None, will build one.\n",
    "        \n",
    "    kernel: sarracen.kernels.base_kernel\n",
    "        Smoothing kernel for SPH data interpolation.\n",
    "        If None, will use the one in sdf.\n",
    "\n",
    "    xyzs_names_list: list\n",
    "        list of names of the columns that represents x, y, z axes (i.e. coord axes names)\n",
    "        MUST INCLUDE ALL THREE AXES LABELS.\n",
    "        If only 2 is included, WILL ASSUME IT IS 2D CACULATIONS.\n",
    "    \n",
    "            \n",
    "    Returns: dvals\n",
    "    -------\n",
    "    dvalsp: (ntarget, nval)-shaped ndarray\n",
    "        error.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # init\n",
    "    \n",
    "    xyzs = sdf[xyzs_names_list].to_numpy()\n",
    "    ms   = sdf['m'   ].to_numpy()\n",
    "    hs   = sdf['h'   ].to_numpy()\n",
    "    rhos = sdf['rho' ].to_numpy()\n",
    "    target_indicies = np.atleast_1d(target_indicies)\n",
    "    dxes = err_h * hs[target_indicies]\n",
    "    # assuming 3D in the following calc\n",
    "    locs = xyzs[target_indicies]\n",
    "    vals =  sdf[ target_labels ].to_numpy()[target_indicies]\n",
    "    if vals.ndim == 2: nval = vals.shape[1]\n",
    "    else:              nval = 1\n",
    "    ntarget = len(target_indicies)\n",
    "    ndim = len(xyzs_names_list)\n",
    "    \n",
    "    if sdf_kdtree is None:\n",
    "        sdf_kdtree = kdtree.KDTree(xyzs)\n",
    "    if kernel is None:\n",
    "        kernel = sdf.kernel\n",
    "    kernel_rad = float(kernel.get_radius())\n",
    "    kernel_w   = kernel.w\n",
    "        \n",
    "    neigh_rad = kernel_rad + err_h\n",
    "    \n",
    "    # ans array\n",
    "    dvals = np.full((ntarget, nval), np.nan)\n",
    "    \n",
    "    \n",
    "    for i in range(ntarget):\n",
    "        loc = locs[i]\n",
    "        val = vals[i]\n",
    "        dx  = dxes[i]\n",
    "        h   = hs[target_indicies[i]]\n",
    "        # find all neighbours within 3h, this includes all points needed for the calc of error for this particle\n",
    "        _, neigh_inds = get_sph_neighbours(sdf_kdtree, loc, h, neigh_rad)\n",
    "\n",
    "        # prepare data\n",
    "        sdf_temp = sdf.iloc[neigh_inds]\n",
    "        \n",
    "        loc_plus_dx = [loc for i in range(ndim*2)]\n",
    "        for j in range(ndim):\n",
    "            loc_plus_dx[j][j] += dx\n",
    "            loc_plus_dx[ndim+j][j] -= dx\n",
    "\n",
    "        dval_xyz = get_sph_interp(sdf_temp, target_labels, loc_plus_dx, kernel=kernel, verbose=0) - val\n",
    "        dvals[i] = ((dval_xyz**2).sum(axis=0)/len(dval_xyz))**0.5\n",
    "    \n",
    "    #dvals = dvals.squeeze()\n",
    "    return dvals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181371ab-d884-447f-a650-1c81b9210e04",
   "metadata": {},
   "source": [
    "### Spectrum Generation (Gray Opacity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4e7d3b5a-be7e-47cc-a00c-98009ef03ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spectrum generation\n",
    "\n",
    "\n",
    "def B_vu(freqs: units.Quantity, T: units.Quantity) -> units.Quantity:\n",
    "    return 2 * const.h / const.c**2 * freqs**3 / (np.exp(const.h * freqs / (const.k_B * T)) - 1)\n",
    "\n",
    "def B_wav(wavlens: units.Quantity, T: units.Quantity) -> units.Quantity:\n",
    "    return 2 * const.h * const.c**2 / wavlens**5 / (np.exp(const.h * const.c / (wavlens * const.k_B * T)) - 1)\n",
    "\n",
    "\n",
    "\n",
    "CONST_H = const.h.cgs.value\n",
    "CONST_C = const.c.cgs.value\n",
    "CONST_K_B = const.k_B.cgs.value\n",
    "CONST_SIG = const.sigma_sb.cgs.value\n",
    "\n",
    "@jit(nopython=True, fastmath=True)\n",
    "def B_vu_nb(freqs_Hz: npt.NDArray[float], T_K: float) -> npt.NDArray[float]:\n",
    "    return 2 * CONST_H / CONST_C**2 * freqs**3 / (np.exp(CONST_H * freqs / (CONST_K_B * T_K)) - 1)\n",
    "\n",
    "@jit(nopython=True, fastmath=True)\n",
    "def B_wav_nb(wavlens_cm: npt.NDArray[float], T_K: float) -> npt.NDArray[float]:\n",
    "    return 2 * CONST_H * CONST_C**2 / wavlens_cm**5 / (np.exp(CONST_H * CONST_C / (wavlens_cm * (CONST_K_B * T_K))) - 1)\n",
    "    \n",
    "\n",
    "@jit(nopython=True, fastmath=True, parallel=True)\n",
    "def L_vu_nb(\n",
    "    freqs_Hz : npt.NDArray[float],\n",
    "    Ts_K     : npt.NDArray[float],\n",
    "    Aeffjs_cm2: npt.NDArray[float],\n",
    ") -> npt.NDArray[float]:\n",
    "    \n",
    "    L_vus = np.zeros_like(freqs_Hz)\n",
    "    nused = len(Ts_K)\n",
    "    #debug_fact = np.zeros(nused)\n",
    "    for i in prange(nused):\n",
    "        dL_vus = 4 * pi * B_vu_nb(freqs_Hz, Ts_K[i]) * Aeffjs_cm2[i]\n",
    "        L_vus += dL_vus\n",
    "        ## debug\n",
    "        #dL_sig = 4 * pi * CONST_SIG * Ts_K[i]**4 / pi * Aeffjs_cm2[i]\n",
    "        #dL_int = np.trapezoid(dL_vus, freqs_Hz)\n",
    "        #debug_fact[i] = dL_int/dL_sig\n",
    "        #print(dL_int/dL_sig, Ts_K[i], Aeffjs_cm2[i])\n",
    "    return L_vus#, debug_fact\n",
    "\n",
    "\n",
    "@jit(nopython=True, fastmath=True, parallel=True)\n",
    "def L_wav_nb(\n",
    "    wavlens_cm: npt.NDArray[float],\n",
    "    Ts_K      : npt.NDArray[float],\n",
    "    Aeffjs_cm2 : npt.NDArray[float],\n",
    ") -> npt.NDArray[float]:\n",
    "    \n",
    "    L_wavs= np.zeros_like(wavlens_cm)\n",
    "    nused = len(Ts_K)\n",
    "    for i in prange(nused):\n",
    "        L_wavs += 4 * pi * B_wav_nb(wavlens_cm, Ts_K[i]) * Aeffjs_cm2[i]\n",
    "    return L_wavs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7516fd21-fc56-49d9-80dd-4578615c0a7c",
   "metadata": {},
   "source": [
    ".\n",
    "\n",
    "## Debug\n",
    "\n",
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1dcdac0a-c0ee-43d5-81cd-7c93ad0b978c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug\n",
    "\n",
    "\n",
    "do_debug = False\n",
    "if do_debug and __name__ == '__main__':\n",
    "    output_dir = '../fig/20240222_LCGen/test/'\n",
    "    interm_dir = '../interm/test/'\n",
    "    rads_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed16872-dfb4-4f93-a089-38f23f119e49",
   "metadata": {},
   "source": [
    "### Running on one dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5a8568e7-2714-482a-b62f-186cf9116cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_debug and __name__ == '__main__':\n",
    "\n",
    "    job_nickname, file_index = '2md', 1200\n",
    "    \n",
    "    job_profile = JOB_PROFILES_DICT[job_nickname]\n",
    "    job_name    = job_profile['job_name']\n",
    "    params      = job_profile['params']\n",
    "    eos_opacity = get_eos_opacity(ieos=10, params=params)    #EoS_MESA_opacity(params, settings)\n",
    "\n",
    "    mpdf = mpdf_read(job_name, file_index, eos_opacity, reset_xyz_by='R1', use_Tscales=use_Tscales, verbose=1)\n",
    "    sdf  = mpdf.data['gas']\n",
    "    sdf['kappa'] = sdf['kappa_dust']\n",
    "    hs  = mpdf.get_val('h').to(units.au)\n",
    "    R1s = mpdf.get_val('R1').to(units.au)\n",
    "    kappas = mpdf.get_val('kappa')\n",
    "    w_col = sdf.kernel.get_column_kernel_func(1000)\n",
    "    dtaus = (kappas * mpdf.get_val('m').to(units.Msun) / hs**2 * w_col(0, 3)).cgs\n",
    "    mask_outer = R1s > 14 * units.au    # arbitrarily decided\n",
    "    mask_block = dtaus > 0\n",
    "    print(f\"There are {np.count_nonzero(mask_outer)} particles in the outer region ({np.count_nonzero(mask_outer)/len(sdf)*100:8.4f}%)\")\n",
    "    print(f\"and {np.count_nonzero(mask_block)} particles are opaque ({np.count_nonzero(mask_block)/len(sdf)*100:8.4f}%)\")\n",
    "    print(f\"{min(R1s[mask_block]) = }\")\n",
    "    plt.hist(R1s[mask_block])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "79caccbb-e1e0-4530-aa21-95b158f8bdd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if do_debug and __name__ == '__main__':\n",
    "\n",
    "    \n",
    "    job_nicknames = ['2md', ]\n",
    "    FILE_INDEXES  = [ 1600, ] #[ 0, 400, 800, 1200, 1600, 2000, 4800, 6400, 8000, 15600, 17600 ] #[    0, ]\n",
    "    xyzs_list     = ['xyz', ]\n",
    "    no_xy         = (64, 64)\n",
    "    no_xy_txt = 'x'.join([f'{i}' for i in no_xy])\n",
    "    use_new_algo  = True\n",
    "\n",
    "    # - SED settings -\n",
    "    # freq: minimum range 1e9~1e20 Hz (covering microwave to x-ray)\n",
    "    # wavelen 1e-11m ~ 0.1m\n",
    "    #wavlens = (np.logspace(-10, 0, 10000) * units.m).cgs   # default (broad)\n",
    "    wavlens = (np.logspace(-2, 5., 10000) * units.um).cgs   # default\n",
    "    #wavlens = (np.logspace(-1, 3.5, 50) * units.um).cgs  # mcfost\n",
    "    \n",
    "    # init combined data\n",
    "    comb = {}\n",
    "    \n",
    "    for job_nickname in job_nicknames: #['2md', ]:\n",
    "        job_profile = JOB_PROFILES_DICT[job_nickname]\n",
    "        job_name    = job_profile['job_name']\n",
    "        file_indexes= FILE_INDEXES #job_profile['file_indexes']\n",
    "        params      = job_profile['params']\n",
    "        eos_opacity = get_eos_opacity(ieos=10, params=params)    #EoS_MESA_opacity(params, settings)\n",
    "        \n",
    "        comb[job_nickname] = {\n",
    "            xyzs: {\n",
    "                'times': np.full(len(file_indexes), np.nan) * units.yr,\n",
    "                'lums' : np.full(len(file_indexes), np.nan) * units.Lsun,\n",
    "                'areas': np.full(len(file_indexes), np.nan) * units.au**2,\n",
    "                # no of particles at the photosphere - lower bound (weighted average per pixel, weighted by lums contribution)\n",
    "                # i.e. how resolved the photosphere is\n",
    "                'N_res': np.full(len(file_indexes), -1) * units.dimensionless_unscaled,\n",
    "                '_meta_': {\n",
    "                    'lums' : { 'Description': \"Luminosity.\", },\n",
    "                    'areas': { 'Description': (\n",
    "                        \"Visible size of the simulated object.\" +\n",
    "                        \"(i.e. pixel * (area per pixel) * (tau if tau<1 else 1)\"), },\n",
    "                    'N_res': { 'Description': (\n",
    "                            \"no of particles at the photosphere - lower bound\" +\n",
    "                            \"(weighted average per pixel, weighted by lums contribution per pixel)\"), },\n",
    "                },\n",
    "            } for xyzs in xyzs_list\n",
    "        }\n",
    "\n",
    "            \n",
    "        for ifile, file_index in enumerate(file_indexes): # file_indexes\n",
    "            # init\n",
    "    \n",
    "            mpdf = mpdf_read(job_name, file_index, eos_opacity, reset_xyz_by='R1', use_Tscales=use_Tscales, verbose=1)\n",
    "            mpdf.calc_sdf_params(['R1'])\n",
    "            sdf  = mpdf.data['gas']\n",
    "            kernel = sdf.kernel\n",
    "            kernel_rad = float(kernel.get_radius())\n",
    "            col_kernel = kernel.get_column_kernel_func(samples=1000)\n",
    "            srcfuncs = mpdf.const['sigma_sb'] * sdf['T']**4 / pi\n",
    "            sdf['srcfunc'] = srcfuncs\n",
    "\n",
    "            with mupl.hdf5_open(f\"{interm_dir}{job_nickname}_{file_index:05d}.lcgen.{no_xy_txt}.hdf5\", 'a', metadata) as out_interm_grp1:\n",
    "                #out_interm_grp1 = mupl.hdf5_subgroup(out_interm_file, f\"{file_index:05d}\", {})\n",
    "\n",
    "                for xyzs in xyzs_list:\n",
    "                    xyzs_names_list = [x for x in xyzs]\n",
    "        \n",
    "                    # record time used\n",
    "                    python_time_start = now()\n",
    "                    print(f\"Start: {python_time_start.isoformat()}\")\n",
    "                    print(f\"\\tWorking on {job_nickname}_{file_index:05d}_{xyzs}...\")\n",
    "        \n",
    "                    \n",
    "                    # get rays\n",
    "                    rays, areas, dXs = get_xy_grids_of_rays(\n",
    "                        sdf, no_xy=no_xy, frac_contained=100., use_adaptive_grid=False, xyzs_names_list=xyzs_names_list)\n",
    "                    ray_areas = areas\n",
    "                    pts    = np.array(sdf[xyzs_names_list])\n",
    "                    hs     = np.array(sdf[ 'h' ])    # npart-shaped array\n",
    "                    \n",
    "                    rays_u = (rays * mpdf.units['dist']).to(units.au)\n",
    "                    areas_u = (areas * mpdf.units['dist']**2).to(units.au**2)\n",
    "        \n",
    "                    \n",
    "                    # do integration without error estimation\n",
    "                    srcfuncs = np.array(srcfuncs)\n",
    "                    srcfuncs_err = None # ask to re-calc below\n",
    "                    if use_new_algo:\n",
    "                        ans   = integrate_along_rays_gridxy(\n",
    "                            sdf, srcfuncs, srcfuncs_err, rays, ray_areas,\n",
    "                            xyzs_names_list=xyzs_names_list, parallel=True, verbose=verbose,\n",
    "                        )\n",
    "                        lum, lum_err, rads, areas_p, taus, inds, contr, pts_order_used, jfact_used, estis = ans\n",
    "                    else:\n",
    "                        ans   = integrate_along_ray_gridxy_old(\n",
    "                            sdf, srcfuncs, srcfuncs_err, rays, ray_areas,\n",
    "                            xyzs_names_list=xyzs_names_list, parallel=True, verbose=verbose,\n",
    "                        )\n",
    "                        lum, lum_err, rads, areas_p, taus, inds, contr, pts_order_used, jfact_used = ans\n",
    "\n",
    "                    \n",
    "                    # record time used\n",
    "                    python_time_ended = now()\n",
    "                    python_time__used  = python_time_ended - python_time_start\n",
    "                    print(f\"In Progress: {python_time_ended.isoformat()}\\nTime Used: {python_time__used}\\n\")\n",
    "\n",
    "                    \n",
    "                    lum     = set_as_quantity(lum,     mpdf.units['lum']).to(units.Lsun)\n",
    "                    lum_err = set_as_quantity(lum_err, mpdf.units['lum']).to(units.Lsun)\n",
    "                    print(f\"\\n{job_nickname}_{file_index:05d}_{xyzs}:\\n\\n\\t{lum = :.3f}, {lum_err = :.3f}\\n\")\n",
    "                    print(f\"Time = {mpdf.get_time(unit=units.yr):.2f}\\n\")\n",
    "\n",
    "\n",
    "                    rads  = (rads * mpdf.units['sigma_sb'] * mpdf.units['temp']**4 / units.sr).cgs\n",
    "                    inds *= units.dimensionless_unscaled\n",
    "                    contr = 100 * contr * units.percent\n",
    "                    try:\n",
    "                        N_res = np.average(\n",
    "                            np.where(np.isnan(contr), 0., 1. / contr),\n",
    "                            weights=(rads * areas_u).value,\n",
    "                        ).to(units.dimensionless_unscaled)\n",
    "                    except ZeroDivisionError:\n",
    "                        N_res = 0.\n",
    "                    lum1  = ((4 * pi * units.sr) * (rads * areas_u)).sum().to(units.solLum)\n",
    "                    print(    f\"Lum err       : {lum_err = :12.2f}\" +\n",
    "                          f\"    (rel err  = {(lum_err / lum1).to(units.percent): 6.2f})\")\n",
    "                    print(    f\"Lum           : {lum1    = :12.2f}\")\n",
    "                    if use_new_algo:\n",
    "                        estis  = (estis * mpdf.units['sigma_sb'] * mpdf.units['temp']**4 / units.sr).cgs\n",
    "                        lume = ((4 * pi * units.sr) * (estis * areas_u)).sum().to(units.solLum)\n",
    "                        print(f\"Old estimation: {lume    = :12.2f}\" +\n",
    "                              f\"    (rel diff = {(2 * (lum1 - lume) / (lum1 + lume)).to(units.percent):+6.2f})\")\n",
    "                    print()\n",
    "                    \n",
    "                    area  = (areas_p * areas_u).sum()\n",
    "                    area_2= (np.where(\n",
    "                        np.isnan(taus),\n",
    "                        1.,\n",
    "                        np.where(\n",
    "                            taus > PHOTOSPHERE_TAU,\n",
    "                            1.0,\n",
    "                            0.0,\n",
    "                        )) * areas_u).sum()\n",
    "                    #anses_fft = fft.fft2(rads.reshape(no_xy).value)\n",
    "    \n",
    "                    if is_verbose(verbose, 'info'):\n",
    "                        say('info', 'main()', verbose,\n",
    "                            f\"lum = {lum}\",\n",
    "                            f\"area (from <1>) = ({area  /areas_u.sum()*100: 5.1f}%) {area}\",\n",
    "                            f\"area (from tau) = ({area_2/areas_u.sum()*100: 5.1f}%) {area_2}\",\n",
    "                            f\"size (from <1>) = {area**0.5}\",\n",
    "                            f\"size (from tau) = {area_2**0.5}\",\n",
    "                            f\"total possible area = {areas_u.sum()}\",\n",
    "                            f\"lower bound of the # of particles at photosphere, weighted avg over lum per pixels = {N_res} \",\n",
    "                        )\n",
    "\n",
    "\n",
    "                    # debug\n",
    "                    print()\n",
    "                    print(f\"{int(rads.size/2)-1 = }\\n{rads[int(rads.size/2)-1].cgs = }\")\n",
    "                    inds_active = np.logical_or(taus > PHOTOSPHERE_TAU, np.isnan(taus))\n",
    "                    print(f\"{np.count_nonzero(inds_active) / taus.size * 100} % rays hit photosphere\")\n",
    "                    print(f\"{np.std(rads[inds_active]) / np.average(rads[inds_active]) = }\")\n",
    "                    inds = np.logical_or(taus > PHOTOSPHERE_TAU, np.isnan(taus))\n",
    "                    lum_in_ph = (4*pi*units.sr*(rads[inds] * areas_u[inds]).sum()).to(units.Lsun)\n",
    "                    print(f\"{lum       = :.2f}\\n{lum_in_ph = :.2f}    ({(lum_in_ph / lum).to(units.percent):.2f})\")\n",
    "                    rads_dict[use_new_algo] = rads[inds]\n",
    "                    if use_new_algo:\n",
    "                        rads_dict[not use_new_algo] = estis[inds]\n",
    "\n",
    "    \n",
    "                    # SEDs\n",
    "                    Ts      = set_as_quantity(sdf['T'].iloc[pts_order_used], mpdf.units['temp']).cgs\n",
    "                    Aeffjs   = set_as_quantity(jfact_used, mpdf.units['dist']**2).cgs\n",
    "                    L_wavs  = L_wav_nb(wavlens.cgs.value, Ts.cgs.value, Aeffjs.cgs.value)\n",
    "                    L_wavs *= units.erg / units.s / units.cm\n",
    "                    L_wavs  = L_wavs.to(units.Lsun / units.cm)\n",
    "                    \n",
    "                    L_int = np.trapezoid(L_wavs, wavlens).to(units.Lsun)\n",
    "                    print(f\"{L_int = }\\n{(L_int/lum-1.).to(units.percent) = }\\n\")\n",
    "                    print()\n",
    "                    \n",
    "                \n",
    "                    # save interm data\n",
    "                    data = {}\n",
    "                    data['lum'  ] = lum\n",
    "                    data['area_one'] = area\n",
    "                    data['area_tau'] = area_2\n",
    "                    data['N_res'] = N_res\n",
    "                    data['xyzs' ] = xyzs\n",
    "                    data['time' ] = mpdf.get_time()\n",
    "                    data['mpdf_params'] = mpdf.params\n",
    "                    data['rays' ] = rays_u[:, 0, :2]\n",
    "                    data['ray_unit_vec'] = get_ray_unit_vec(rays_u[0].value)\n",
    "                    data['area_per_ray'] = areas_u[0] #areas_u\n",
    "                    data['rads' ] = rads\n",
    "                    data['contr'] = contr\n",
    "                    data['Aeffjs'] = Aeffjs\n",
    "                    \n",
    "                    data['_meta_'] = {\n",
    "                        'N_res': comb[job_nickname][xyzs]['_meta_']['N_res'],\n",
    "                        'rays' : { 'Description': \"Pixel centers on the 2D plane defined by xyzs.\", },\n",
    "                        'rads' : { 'Description': \"Specific intensity per pixel.\", },\n",
    "                        'contr': {\n",
    "                            'Description': \"Maximum contributed particle's contribution towards the specific intensity, per pixel.\", },\n",
    "                    }\n",
    "    \n",
    "                    \n",
    "                    mupl.hdf5_dump(data, mupl.hdf5_subgroup(out_interm_grp1, xyzs, overwrite=True), {})\n",
    "        \n",
    "                    comb[job_nickname][xyzs]['times'][ifile] = data['time']\n",
    "                    comb[job_nickname][xyzs]['lums' ][ifile] = data['lum' ]\n",
    "                    comb[job_nickname][xyzs]['areas'][ifile] = data['area_one']\n",
    "                    comb[job_nickname][xyzs]['N_res'][ifile] = data['N_res']\n",
    "        \n",
    "                    \n",
    "                    # plotting\n",
    "                    if True:\n",
    "                        save_label_addon = '-new' if use_new_algo else '-old'\n",
    "                        plt.close('all')\n",
    "                        fig, ax, outfilenames = plot_imshow(\n",
    "                            no_xy, rays_u, rads, data_label=\"$I$\",\n",
    "                            xyzs=xyzs, save_label=f\"image{save_label_addon}\",\n",
    "                            job_profile=job_profile, file_index=file_index, cmap='inferno', notes=data,\n",
    "                            output_dir=output_dir, verbose=verbose_loop)\n",
    "                        fig, ax, outfilenames = plot_imshow(\n",
    "                            no_xy, rays_u, inds%20, data_label=\"index % 20 of the most contributed\",\n",
    "                            xyzs=xyzs, save_label=f\"dinds{save_label_addon}\",\n",
    "                            job_profile=job_profile, file_index=file_index, cmap='turbo', notes=data,\n",
    "                            output_dir=output_dir, verbose=verbose_loop)\n",
    "                        fig, ax, outfilenames = plot_imshow(\n",
    "                            no_xy, rays_u, contr, data_label=\"contribution fraction of the most contributed\",\n",
    "                            xyzs=xyzs, save_label=f\"contr{save_label_addon}\",\n",
    "                            job_profile=job_profile, file_index=file_index, cmap='seismic', notes=data,\n",
    "                            output_dir=output_dir, verbose=verbose_loop)\n",
    "                        fig, ax, outfilenames = plot_imshow(\n",
    "                            no_xy, rays_u, areas_p, data_label=\"$<1>$\",\n",
    "                            xyzs=xyzs, save_label=f\"pones{save_label_addon}\",\n",
    "                            job_profile=job_profile, file_index=file_index, notes=data,\n",
    "                            norm=plt.Normalize(0., 1.),\n",
    "                            output_dir=output_dir, verbose=verbose_loop)\n",
    "                        #fig, ax, outfilenames = plot_imshow(\n",
    "                        #    no_xy, rays_u, np.abs(anses_fft), data_label=\"FFt of $I$\", xyzs=xyzs, save_label=f\"I-fft\",\n",
    "                        #    norm=mpl.colors.LogNorm(),\n",
    "                        #    job_profile=job_profile, file_index=file_index, notes=data, output_dir=output_dir, verbose=verbose_loop)\n",
    "                    \n",
    "                    \n",
    "                    # plotting - spec in wavlen space\n",
    "                    spec_dist = 10 * units.parsec\n",
    "                    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "                    y = (L_wavs/(4*pi*spec_dist**2)).to((units.erg / units.s / units.cm**2) / units.angstrom)\n",
    "                    x = wavlens.to(units.angstrom)\n",
    "                    ax.loglog(x, y)\n",
    "                    ax.set_title(f\"SED (viewed at {spec_dist:.1f} with gray opacity)\\n{job_profile['plot_title_suffix']}\")\n",
    "                    ax.set_xlabel(f\"$\\\\lambda$ / {x.unit.to_string('latex_inline')}\")\n",
    "                    ax.set_ylabel(f\"$f_{{\\\\lambda}}$ / {y.unit.to_string('latex_inline')}\")\n",
    "                    ax.set_xlim(5e2, 1e6)\n",
    "                    ax.set_ylim(1e-9, 1e-3)\n",
    "                    ax.text(\n",
    "                        0.98, 0.02,\n",
    "                        f\"Time = {mpdf.get_time():.1f}\\n\" + \\\n",
    "                        f\" $L$ = {lum.value:.0f} {lum.unit.to_string('latex_inline')}\",\n",
    "                        #color = \"black\",\n",
    "                        ha = 'right', va = 'bottom',\n",
    "                        transform=ax.transAxes,\n",
    "                    )\n",
    "                    fig.savefig(f\"{output_dir}Spec_{job_nickname}_{file_index:05d}_{no_xy_txt}.png\")\n",
    "        \n",
    "                    # record time used\n",
    "                    python_time_ended = now()\n",
    "                    python_time__used  = python_time_ended - python_time_start\n",
    "                    print(f\"Ended: {python_time_ended.isoformat()}\\nTime Used: {python_time__used}\\n\")\n",
    "\n",
    "\n",
    "        # save data for now\n",
    "        with open(f\"{interm_dir}lcgen.{no_xy_txt}.json\", 'w') as f:\n",
    "            mupl.json_dump(comb, f, metadata)\n",
    "\n",
    "        \n",
    "        #plotting\n",
    "        plt.close('all')\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        for xyzs in xyzs_list:\n",
    "            ax.semilogy(\n",
    "                comb[job_nickname][xyzs]['times'].to_value(units.yr), comb[job_nickname][xyzs]['lums'].to_value(units.Lsun),\n",
    "                'o--', label=f\"Viewed from +{xyzs[2]}\")\n",
    "        ax.legend()\n",
    "        ax.set_xlabel('Time / yr')\n",
    "        ax.set_ylabel('Luminosity / Lsun')\n",
    "        ax.set_xlim(0., 45.)\n",
    "        ax.set_ylim(1e4, 5e6)\n",
    "        outfilename_noext = f\"{output_dir}LC_{job_nickname}_{no_xy_txt}\"\n",
    "\n",
    "        if False:\n",
    "            # write pdf\n",
    "            outfilename = f\"{outfilename_noext}.pdf\"\n",
    "            fig.savefig(outfilename)\n",
    "            if is_verbose(verbose, 'note'):\n",
    "                say('note', None, verbose, f\"Fig saved to {outfilename}.\")\n",
    "        \n",
    "        # write png (with plot title)\n",
    "        ax.set_title(f\"Light curve ({job_nickname}, {no_xy_txt} rays)\")\n",
    "        outfilename = f\"{outfilename_noext}.png\"\n",
    "        fig.savefig(outfilename)\n",
    "        if is_verbose(verbose, 'note'):\n",
    "            say('note', None, verbose, f\"Fig saved to {outfilename}.\")\n",
    "                \n",
    "    plt.close('all')\n",
    "    mupl.hdf5_dump(comb, f\"{interm_dir}lcgen.{no_xy_txt}.hdf5.gz\", metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9b692062-e3bf-4b52-82a2-d66d67091610",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if do_debug and __name__ == '__main__':\n",
    "    x = np.linspace(0., 100.)\n",
    "    for key in rads_dict:\n",
    "        label = 'new' if key else 'old'\n",
    "        plt.plot(x, np.percentile(rads_dict[key],x), label=label)\n",
    "        print(f\"{label}: {rads_dict[key].size = }\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "175b6e34-21f4-4e06-b164-3918192d1fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# integrate spectrum across wavelen\n",
    "if do_debug and __name__ == '__main__':\n",
    "    # freq: minimum range 1e9~1e20 Hz (covering microwave to x-ray)\n",
    "    # wavelen 1e-11m ~ 0.1m\n",
    "    wavlens = (np.logspace(-10, 0, 10000) * units.m).cgs\n",
    "    Ts      = set_as_quantity(sdf['T'].iloc[pts_order_used], mpdf.units['temp']).cgs\n",
    "    Aeffjs   = set_as_quantity(jfact_used, mpdf.units['dist']**2).cgs\n",
    "    L_wavs  = L_wav_nb(wavlens.cgs.value, Ts.cgs.value, Aeffjs.cgs.value)\n",
    "    L_wavs *= units.erg / units.s / units.cm\n",
    "    L_wavs  = L_wavs.to(units.Lsun / units.cm)\n",
    "    \n",
    "    L_int = np.trapezoid(L_wavs, wavlens).to(units.Lsun)\n",
    "    print(f\"{L_int = }\\n{lum   = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9e650250-c3a6-42de-97dc-787d024c7072",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_debug and __name__ == '__main__':\n",
    "    # plotting - spec in wavlen space\n",
    "    spec_dist = 10 * units.parsec\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    y = (L_wavs/(4*pi*spec_dist**2)).to((units.erg / units.s / units.cm**2) / units.angstrom)\n",
    "    x = wavlens.to(units.angstrom)\n",
    "    ax.loglog(x, y)\n",
    "    ax.set_title(f\"SED (viewed at {spec_dist:.1f} with gray opacity)\\n{job_profile['plot_title_suffix']}\")\n",
    "    ax.set_xlabel(f\"$\\\\lambda$ / {x.unit.to_string('latex_inline')}\")\n",
    "    ax.set_ylabel(f\"$f_{{\\\\lambda}}$ / {y.unit.to_string('latex_inline')}\")\n",
    "    ax.set_xlim(5e2, 1e6)\n",
    "    ax.set_ylim(1e-9, 1e-3)\n",
    "    ax.text(\n",
    "        0.98, 0.02,\n",
    "        f\"Time = {mpdf.get_time():.1f}\\n\" + \\\n",
    "        f\" $L$ = {lum.value:.0f} {lum.unit.to_string('latex_inline')}\",\n",
    "        #color = \"black\",\n",
    "        ha = 'right', va = 'bottom',\n",
    "        transform=ax.transAxes,\n",
    "    )\n",
    "    fig.savefig(f\"{output_dir}Spec_{job_nickname}_{file_index:05d}_{no_xy_txt}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "abc8e430-2d22-4f7a-ab4b-daf42a7f0720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make movie\n",
    "if do_debug and __name__ == '__main__':\n",
    "    from moviepy.editor import ImageSequenceClip\n",
    "    \n",
    "    file_indexes = [ 0, 400, 800, 1200, 1600, 2000, 4800, 6400, 8000, 15600, 17600 ]\n",
    "    \n",
    "    outfilenames = [f\"{output_dir}Spec_{job_nickname}_{fi:05d}_{no_xy_txt}.png\" for fi in file_indexes]\n",
    "    moviefilename = f'{output_dir}Spec_{job_nickname}_{no_xy_txt}__movie.mp4'\n",
    "    with ImageSequenceClip(outfilenames, fps=2) as vid:\n",
    "        vid.write_videofile(moviefilename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df61d31e-46e1-4132-9b7c-d74d8b356671",
   "metadata": {},
   "source": [
    "    # spectrum generation - Test\n",
    "    \n",
    "    # freq: minimum range 1e9~1e20 Hz (covering microwave to x-ray)\n",
    "    freqs = (np.logspace(9, 20, 10000) * units.Hz).si\n",
    "    wavlens = const.c / freqs\n",
    "    Ts = set_as_quantity(sdf['T'], mpdf.units['temp'])\n",
    "    for T in [1*units.K, np.min(Ts), np.max(Ts), 2e6*units.K]:\n",
    "        B_vus = B_vu(freqs, T)\n",
    "        S_int = np.trapezoid(B_vus, freqs).si\n",
    "        S_sig = (const.sigma_sb * T**4 / pi).si\n",
    "        print(f\"{T = :10.2f},    {S_int = :.4e},    {S_sig = :.4e},    {(S_int/S_sig-1)*100 = :.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302ee5c0-7a0a-45d6-bee7-1edaf113626f",
   "metadata": {},
   "source": [
    "    nused = len(Ts)\n",
    "    L = 0\n",
    "    srcfuncs_u = srcfuncs[pts_order_used] * (mpdf.units['sigma_sb'] * mpdf.units['temp']**4)\n",
    "    for i in prange(nused):\n",
    "        L += 4 * pi * srcfuncs_u[i] * Aeffjs[i]\n",
    "    L = L.to(units.Lsun)\n",
    "    L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcff348-b8e0-49ae-b10e-2076889a93d6",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "    # integrate spectrum across frequency\n",
    "    if do_debug and __name__ == '__main__':\n",
    "        # freq: minimum range 1e9~1e20 Hz (covering microwave to x-ray)\n",
    "        freqs = (np.logspace(9, 20, 10000) * units.Hz).cgs\n",
    "        Ts    = set_as_quantity(sdf['T'].iloc[pts_order_used], mpdf.units['temp'])\n",
    "        Aeffjs = set_as_quantity(jfact_used, mpdf.units['dist']**2)\n",
    "        L_vus = L_vu_nb(freqs.cgs.value, Ts.cgs.value, Aeffjs.cgs.value)\n",
    "        L_vus *= units.erg\n",
    "        L_vus = L_vus.to(units.Lsun / units.Hz)\n",
    "        \n",
    "        L_int = np.trapezoid(L_vus, freqs).to(units.Lsun)\n",
    "        print(f\"{L_int = }\\n{lum   = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07679f6d-e866-454f-ba74-951f7feaa42d",
   "metadata": {},
   "source": [
    "    # plotting - spec in freq space\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    y = L_vus\n",
    "    x = freqs\n",
    "    ax.loglog(x, y)\n",
    "    ax.set_title(f\"Spectrum (Gray opacity)\\n{job_profile['plot_title_suffix']}\")\n",
    "    ax.set_xlabel(f\"$\\\\nu$ / {x.unit.to_string('latex_inline')}\")\n",
    "    ax.set_ylabel(f\"$L_{{\\\\nu}}$ / {y.unit.to_string('latex_inline')}\")\n",
    "    ax.text(\n",
    "        0.98, 0.98,\n",
    "        f\"Time = {mpdf.get_time():.1f}\\n\" + \\\n",
    "        f\" $L$ = {lum.value:.0f} {lum.unit.to_string('latex_inline')}\",\n",
    "        #color = \"black\",\n",
    "        ha = 'right', va = 'top',\n",
    "        transform=ax.transAxes,\n",
    "    )\n",
    "    fig.savefig(f\"{output_dir}Spec_{job_nickname}_{file_index:05d}_{no_xy_txt}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e9616b-de8b-477c-a9a2-35e48c52204d",
   "metadata": {},
   "source": [
    "### Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "888652e1-66bf-483f-a588-8b623e59b27c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if do_debug and __name__ == '__main__':\n",
    "    q_xy = np.pi/7*2 + np.linspace(0., 1., 10)\n",
    "    q_z = -pi/5*1+ np.linspace(0., 1., 10)\n",
    "    ndim = 3\n",
    "    \n",
    "    w_col, w_csz, w_col_a, w_csz_a = get_col_kernel_funcs(kernel)\n",
    "    w_col_sar = kernel.get_column_kernel_func(1001)\n",
    "    print(w_col_a(q_xy, ndim))\n",
    "    for i in range(len(q_xy)):\n",
    "        a = w_col(q_xy[i], ndim)\n",
    "        b = w_col_sar(q_xy[i], ndim)\n",
    "        print( (a - b) / (a + b), '\\t', a, '\\t', b )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "89898bf9-4941-465a-b8a5-0a4d55397c54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if do_debug and __name__ == '__main__':\n",
    "    hfact = mpdf.params['hfact']\n",
    "    ndim  = 3\n",
    "    parallel = True\n",
    "    pts_used = pts[pts_order_used]\n",
    "    srcfuncs_used = srcfuncs[pts_order_used]\n",
    "    hs_used  = hs[ pts_order_used]\n",
    "    T_used   =  np.array(sdf['T'])[pts_order_used]\n",
    "    R1_used = np.sum(pts_used**2, axis=1)**0.5\n",
    "    vals_grad_used = get_sph_gradient(\n",
    "        sdf,\n",
    "        val_names   = ['srcfunc', 'T'],\n",
    "        locs        = pts_used,\n",
    "        vals_at_locs= srcfuncs_used,\n",
    "        hs_at_locs  = hs_used,\n",
    "        kernel      = kernel,\n",
    "        hfact       = hfact,\n",
    "        sdf_kdtree  = None,\n",
    "        ndim        = ndim,\n",
    "        xyzs_names_list=xyzs_names_list,\n",
    "        parallel    = parallel,\n",
    "        verbose     = verbose,\n",
    "    )    # get_sph_gradient returns a (nlocs, ndim, nvals)-shaped np.ndarray\n",
    "    srcfuncs_grad_used = vals_grad_used[:, :, 0]\n",
    "    T_grad_used        = vals_grad_used[:, :, 1]\n",
    "    srcfuncs_grad_frac = np.sum(srcfuncs_grad_used**2, axis=1)**0.5 * hs_used / srcfuncs_used\n",
    "    T_grad_frac = np.sum(T_grad_used**2, axis=1)**0.5 * hs_used /T_used\n",
    "    print(f\"{srcfuncs_grad_frac = }\\n{T_grad_frac = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "48dc1ca5-b97b-488e-9f49-4c9e5d0dcb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_debug and __name__ == '__main__':\n",
    "    output_dir = f\"../fig/20240708_Tgrad-for-L-per-part/{job_nickname}_{file_index:05d}_\"\n",
    "    print(f\"{output_dir = }\")\n",
    "    print(f\"{lum    =:12.2f}\\n{lum_err=:12.2f}    ({(lum_err/lum).to(units.percent) :7.2f})\")\n",
    "    print(mpdf.get_time())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f4aac3-84ca-4ecb-ba3c-2f16e1b7d731",
   "metadata": {},
   "source": [
    "#### Old Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5c608ad5-1547-42d3-b23b-4546520a5b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_debug and __name__ == '__main__':\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    ax.loglog(R1_used, srcfuncs_grad_frac, '.')\n",
    "    #plt.scatter(R1_used, T_grad_frac, marker='.', label='$ \\\\delta T / T $')\n",
    "    ax.set_xlim(1e3 , 1e5)\n",
    "    ax.set_ylim(1e-4, 1e1)\n",
    "    ax.set_xlabel('$R_1$')\n",
    "    ax.set_ylabel('$ \\\\delta S / S $')\n",
    "    fig.savefig(f\"{output_dir}dS_S-R1.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fa993c25-9f0f-49a0-8ef2-7725c56669b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if do_debug and __name__ == '__main__':\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    ax.loglog(jfact_used, srcfuncs_grad_frac, '.')\n",
    "    #plt.scatter(R1_used, T_grad_frac, marker='.', label='$ \\\\delta T / T $')\n",
    "    #ax.set_xlim(1e3, 1e5)\n",
    "    ax.set_ylim(1e-4, 1e1)\n",
    "    ax.set_xlabel('weight for $L$')\n",
    "    ax.set_ylabel('$ \\\\delta S / S $')\n",
    "    fig.savefig(f\"{output_dir}dS_S-jfact.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "08c7c5bd-336b-41f9-8ebf-c65062144272",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_debug and __name__ == '__main__':\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    ax.loglog(R1_used, T_grad_frac, '.')\n",
    "    #plt.scatter(R1_used, T_grad_frac, marker='.', label='$ \\\\delta T / T $')\n",
    "    ax.set_xlim(1e3 , 1e5)\n",
    "    ax.set_ylim(1e-4, 1e1)\n",
    "    ax.set_xlabel('$R_1$')\n",
    "    ax.set_ylabel('$ \\\\delta T / T $')\n",
    "    fig.savefig(f\"{output_dir}dT_T-R1.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9ca7977d-ba77-4b77-b461-eb9c03f05fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_debug and __name__ == '__main__':\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    ax.loglog(jfact_used, T_grad_frac, '.')\n",
    "    #plt.scatter(R1_used, T_grad_frac, marker='.', label='$ \\\\delta T / T $')\n",
    "    #ax.set_xlim(1e3, 1e5)\n",
    "    ax.set_ylim(1e-4, 1e1)\n",
    "    ax.set_xlabel('weight for $L$')\n",
    "    ax.set_ylabel('$ \\\\delta T / T $')\n",
    "    fig.savefig(f\"{output_dir}dT_T-jfact.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "429f35be-c8a7-4cb7-8eaf-7ac9ce2bebac",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_debug and __name__ == '__main__':\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    ax.loglog(R1_used, jfact_used, '.')\n",
    "    #plt.scatter(R1_used, T_grad_frac, marker='.', label='$ \\\\delta T / T $')\n",
    "    #ax.set_xlim(1e3, 1e5)\n",
    "    ax.set_xlim(1e3 , 1e5)\n",
    "    #ax.set_ylim(1e-4, 1e1)\n",
    "    ax.set_xlabel('$R_1$')\n",
    "    ax.set_ylabel('weight for $L$')\n",
    "    fig.savefig(f\"{output_dir}jfact-R1.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ebdd93-9a23-4890-95d2-cb1a01e4865e",
   "metadata": {},
   "source": [
    "#### Good Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b6891580-1ae8-4dc0-9151-b4d5ce921d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_debug and __name__ == '__main__':\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    dt = np.linspace(0.01, 10.)\n",
    "    ax.loglog(dt, (1. - np.exp(-dt)))\n",
    "    ax.loglog(dt, (1. - (1 + dt) * np.exp(-dt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "42b56391-fd7c-4608-a403-732102e41364",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_debug and __name__ == '__main__':\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    inds = np.where(jfact_used > np.max(jfact_used) / 1e3)\n",
    "    col_kernel = sdf.kernel.get_column_kernel_func(samples=1000)\n",
    "    kappa_used = np.array(sdf['kappa'])[pts_order_used]#, mpdf.units['opacity'])#.cgs\n",
    "    dtau_max_used = kappa_used * np.array(sdf['m'] / sdf['h']**2)[pts_order_used] * col_kernel(0, 3)\n",
    "    cax = ax.scatter(\n",
    "        R1_used[inds],\n",
    "        srcfuncs_used[inds],\n",
    "        #dtau_max_used[inds],\n",
    "        c=np.log10(jfact_used[inds]), marker='.')\n",
    "    #ax.axhline(1.0, linestyle='--', color='grey')\n",
    "    ax.semilogy()\n",
    "    cmap = fig.colorbar(cax)\n",
    "    cmap.set_label('weight for $L$ (log10)')\n",
    "    ax.set_title(f\"Most influential particles for $L$\\n{job_profile['plot_title_suffix']}\")\n",
    "    ax.text(\n",
    "        0.98, 0.98,\n",
    "        f\" $L$ = ({lum.to_value(1e3*units.solLum):.1f}\" + \\\n",
    "        f\"$\\pm$ {lum_err.to_value(1e3*units.solLum):.1f}\" + \\\n",
    "        f\")$\\\\times 10^3$ {units.solLum.to_string('latex_inline')}\" + \\\n",
    "        f\"({(lum_err/lum).to(units.percent) :4.1f}) \" + \\\n",
    "        f\"\\nTime = {mpdf.get_time():.1f}\\n\" + \\\n",
    "        '',\n",
    "        ha = 'right', va = 'top',\n",
    "        transform=ax.transAxes,\n",
    "    )\n",
    "    ax.set_xlabel('$R_1$')\n",
    "    #ax.set_ylabel('$ \\\\triangle \\\\tau_\\\\mathrm{{{max}}} $')\n",
    "    #fig.savefig(f\"{output_dir}R1-dtaumax.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4843ed48-dbd7-43a9-aac3-4ecb4bde38b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_debug and __name__ == '__main__':\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    inds = np.where(jfact_used > np.max(jfact_used) / 1e3)\n",
    "    cax = ax.scatter(T_used[inds], (T_grad_frac * T_used)[inds], c=np.log10(jfact_used[inds]), marker='.', label='$ \\\\delta T $')\n",
    "    ax.loglog(T_used[inds], T_used[inds], '-', color='C1', label='$ \\\\delta T  / T = 1$')\n",
    "    ax.loglog(T_used[inds], T_used[inds] * 0.1, '-', color='C3', label='$ \\\\delta T  / T = 0.1$')\n",
    "    cmap = fig.colorbar(cax)\n",
    "    cmap.set_label('weight for $L$ (log10)')\n",
    "    #plt.scatter(R1_used, T_grad_frac, marker='.', label='$ \\\\delta T / T $')\n",
    "    #ax.set_xlim(1e3, 1e5)\n",
    "    #ax.set_xlim(1.6e3, 7e1)\n",
    "    #ax.set_ylim(1e0, 3e3)\n",
    "    ax.set_title(f\"Most influential particles for $L$\\n{job_profile['plot_title_suffix']}\")\n",
    "    ax.text(\n",
    "        0.98, 0.98,\n",
    "        f\" $L$ = ({lum.to_value(1e3*units.solLum):.1f}\" + \\\n",
    "        f\"$\\pm$ {lum_err.to_value(1e3*units.solLum):.1f}\" + \\\n",
    "        f\")$\\\\times 10^3$ {units.solLum.to_string('latex_inline')}\" + \\\n",
    "        f\"({(lum_err/lum).to(units.percent) :4.1f}) \" + \\\n",
    "        f\"\\nTime = {mpdf.get_time():.1f}\\n\" + \\\n",
    "        '',\n",
    "        ha = 'right', va = 'top',\n",
    "        transform=ax.transAxes,\n",
    "    )\n",
    "    ax.set_xlabel('$T$')\n",
    "    ax.set_ylabel('$ \\\\delta T $')\n",
    "    ax.legend()\n",
    "    fig.savefig(f\"{output_dir}T-dT.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c611797c-47ec-410c-b868-f3a44f18579d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_debug and __name__ == '__main__':\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    srcfunc_unit = units.erg / units.s / units.cm**2\n",
    "    srcfunc_unit_cgs = (1. * mpdf.units['sigma_sb'] * mpdf.units['temp']**4).to_value(srcfunc_unit)\n",
    "    \n",
    "    inds = np.where(jfact_used > np.max(jfact_used) / 1e3)\n",
    "    cax = ax.scatter(srcfuncs_used[inds] * srcfunc_unit_cgs, (srcfuncs_grad_frac * srcfuncs_used * srcfunc_unit_cgs)[inds],\n",
    "                     c=np.log10(jfact_used[inds]), marker='.', label='$ \\\\delta S $')\n",
    "    ax.loglog(srcfuncs_used[inds] * srcfunc_unit_cgs, srcfuncs_used[inds] * srcfunc_unit_cgs,\n",
    "              '-', color='C1', label='$ \\\\delta S  / S = 1$')\n",
    "    ax.loglog(srcfuncs_used[inds] * srcfunc_unit_cgs, srcfuncs_used[inds] * srcfunc_unit_cgs * 0.1,\n",
    "              '-', color='C3', label='$ \\\\delta S  / S = 0.1$')\n",
    "    cmap = fig.colorbar(cax)\n",
    "    cmap.set_label('weight for $L$ (log10)')\n",
    "    #plt.scatter(R1_used, T_grad_frac, marker='.', label='$ \\\\delta T / T $')\n",
    "    #ax.set_xlim(1e3, 1e5)\n",
    "    #ax.set_xlim(1.1828830116184552e8, 433.3651902612169)\n",
    "    #ax.set_ylim(5e2, 1e8)\n",
    "    ax.set_title(f\"Most influential particles for $L$\\n{job_profile['plot_title_suffix']}\")\n",
    "    ax.text(\n",
    "        0.98, 0.98,\n",
    "        f\" $L$ = ({lum.to_value(1e3*units.solLum):.1f}\" + \\\n",
    "        f\"$\\pm$ {lum_err.to_value(1e3*units.solLum):.1f}\" + \\\n",
    "        f\")$\\\\times 10^3$ {units.solLum.to_string('latex_inline')}\" + \\\n",
    "        f\"({(lum_err/lum).to(units.percent) :4.1f}) \" + \\\n",
    "        f\"\\nTime = {mpdf.get_time():.1f}\\n\" + \\\n",
    "        '',\n",
    "        ha = 'right', va = 'top',\n",
    "        transform=ax.transAxes,\n",
    "    )\n",
    "    ax.set_xlabel(f\"$S$ / {srcfunc_unit.to_string('latex_inline')}\")\n",
    "    ax.set_ylabel('$ \\\\delta S $')\n",
    "    ax.legend()\n",
    "    fig.savefig(f\"{output_dir}S-dS.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b89f446-18c0-4b53-a8c6-6a208b05a055",
   "metadata": {},
   "source": [
    ".\n",
    "\n",
    "## Main\n",
    "\n",
    ".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "50dfe709-c9a0-4008-a213-3b98d31b3c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Note: Density column rho already exist in self.time = np.float64(0.0).\n",
      "Start: 2025-01-30T01:12:29.939545+00:00\n",
      "\tWorking on 2md_00000_xyz...\n",
      "    Debug  :    run_code() ==> <module>() ==> integrate_along_rays_gridxy():\n",
      "\tlum = np.float64(5.0817241363132896e-08), lum2 = np.float64(5.081724136313288e-08)\n",
      "\trads.shape=(4096,), ray_areas.shape=(4096,)\n",
      "\tsrcfuncs[jused].shape=(18885,), jfact_used.shape=(18885,)\n",
      "**  Warning:    <module>() ==> integrate_along_rays_gridxy() ==> get_sph_gradient_phantom():\n",
      "\tOnly half of the process is parallelized. Pending improvements.\n",
      "    Debug  :    run_code() ==> <module>() ==> integrate_along_rays_gridxy():\n",
      "\t18885 particles actually participated calculation (1.37% of all particles, average 4.61 per ray.)\n",
      "In Progress: 2025-01-30T01:13:07.371875+00:00\n",
      "Time Used: 0:00:37.432330\n",
      "\n",
      "\n",
      "2md_00000_xyz:\n",
      "\n",
      "\tlum = 31595.386 solLum, lum_err = 1069.892 solLum\n",
      "\n",
      "Time = 0.00 yr\n",
      "\n",
      "L_int = <Quantity 31595.3993937 solLum>\n",
      "(L_int/lum-1.).to(units.percent) = <Quantity 4.33075011e-05 %>\n",
      "\n",
      "\n",
      "Lum err       : lum_err =      1069.89 solLum    (rel err  =   3.39 %)\n",
      "Lum           : lum1    =     31595.39 solLum\n",
      "Old estimation: lume    =     10604.00 solLum    (rel diff = +99.49 %)\n",
      "\n",
      "    Debug  :    main():\n",
      "\tlum = 31595.38571052642 solLum\n",
      "\tarea (from <1>) = ( 31.6%) 7.631322157701332 AU2\n",
      "\tarea (from tau) = ( 30.5%) 7.359540976856381 AU2\n",
      "\tsize (from <1>) = 2.76248477963252 AU\n",
      "\tsize (from tau) = 2.712847392843243 AU\n",
      "\ttotal possible area = 24.154390898400422 AU2\n",
      "\tlower bound of the # of particles at photosphere, weighted avg over lum per pixels = 12.294054134830269 \n",
      "\n",
      "int(rads.size/2)-1 = 2047\n",
      "rads[int(rads.size/2)-1].cgs = <Quantity 0. g / (rad2 s3)>\n",
      "30.46875 % rays hit photosphere\n",
      "np.std(rads[inds_active]) / np.average(rads[inds_active]) = <Quantity 1.06418342>\n",
      "lum       = 31595.39 solLum\n",
      "lum_in_ph = 30903.91 solLum    (97.81 %)\n",
      "Ended: 2025-01-30T01:13:09.008161+00:00\n",
      "Time Used: 0:00:39.068616\n",
      "\n",
      "Start: 2025-01-30T01:13:09.008189+00:00\n",
      "\tWorking on 2md_00000_xzy...\n",
      "    Debug  :    run_code() ==> <module>() ==> integrate_along_rays_gridxy():\n",
      "\tlum = np.float64(4.9573410062026266e-08), lum2 = np.float64(4.957341006202627e-08)\n",
      "\trads.shape=(4096,), ray_areas.shape=(4096,)\n",
      "\tsrcfuncs[jused].shape=(18750,), jfact_used.shape=(18750,)\n",
      "**  Warning:    <module>() ==> integrate_along_rays_gridxy() ==> get_sph_gradient_phantom():\n",
      "\tOnly half of the process is parallelized. Pending improvements.\n",
      "    Debug  :    run_code() ==> <module>() ==> integrate_along_rays_gridxy():\n",
      "\t18750 particles actually participated calculation (1.36% of all particles, average 4.57 per ray.)\n",
      "In Progress: 2025-01-30T01:13:36.839382+00:00\n",
      "Time Used: 0:00:27.831193\n",
      "\n",
      "\n",
      "2md_00000_xzy:\n",
      "\n",
      "\tlum = 30822.039 solLum, lum_err = 1051.034 solLum\n",
      "\n",
      "Time = 0.00 yr\n",
      "\n",
      "L_int = <Quantity 30822.05267745 solLum>\n",
      "(L_int/lum-1.).to(units.percent) = <Quantity 4.33075011e-05 %>\n",
      "\n",
      "\n",
      "Lum err       : lum_err =      1051.03 solLum    (rel err  =   3.41 %)\n",
      "Lum           : lum1    =     30822.04 solLum\n",
      "Old estimation: lume    =     10226.56 solLum    (rel diff = +100.35 %)\n",
      "\n",
      "    Debug  :    main():\n",
      "\tlum = 30822.039329197643 solLum\n",
      "\tarea (from <1>) = ( 28.8%) 7.582665138403135 AU2\n",
      "\tarea (from tau) = ( 27.7%) 7.296173030028358 AU2\n",
      "\tsize (from <1>) = 2.7536639479796974 AU\n",
      "\tsize (from tau) = 2.701142911811287 AU\n",
      "\ttotal possible area = 26.330506370921718 AU2\n",
      "\tlower bound of the # of particles at photosphere, weighted avg over lum per pixels = 12.482441338024252 \n",
      "\n",
      "int(rads.size/2)-1 = 2047\n",
      "rads[int(rads.size/2)-1].cgs = <Quantity 312.91734832 g / (rad2 s3)>\n",
      "27.7099609375 % rays hit photosphere\n",
      "np.std(rads[inds_active]) / np.average(rads[inds_active]) = <Quantity 0.98881925>\n",
      "lum       = 30822.04 solLum\n",
      "lum_in_ph = 30129.50 solLum    (97.75 %)\n",
      "Ended: 2025-01-30T01:13:37.419083+00:00\n",
      "Time Used: 0:00:28.410894\n",
      "\n",
      "Start: 2025-01-30T01:13:37.419112+00:00\n",
      "\tWorking on 2md_00000_yzx...\n",
      "    Debug  :    run_code() ==> <module>() ==> integrate_along_rays_gridxy():\n",
      "\tlum = np.float64(5.19777495863712e-08), lum2 = np.float64(5.19777495863712e-08)\n",
      "\trads.shape=(4096,), ray_areas.shape=(4096,)\n",
      "\tsrcfuncs[jused].shape=(19005,), jfact_used.shape=(19005,)\n",
      "**  Warning:    <module>() ==> integrate_along_rays_gridxy() ==> get_sph_gradient_phantom():\n",
      "\tOnly half of the process is parallelized. Pending improvements.\n",
      "    Debug  :    run_code() ==> <module>() ==> integrate_along_rays_gridxy():\n",
      "\t19005 particles actually participated calculation (1.38% of all particles, average 4.63 per ray.)\n",
      "In Progress: 2025-01-30T01:14:03.279651+00:00\n",
      "Time Used: 0:00:25.860539\n",
      "\n",
      "\n",
      "2md_00000_yzx:\n",
      "\n",
      "\tlum = 32316.926 solLum, lum_err = 1096.483 solLum\n",
      "\n",
      "Time = 0.00 yr\n",
      "\n",
      "L_int = <Quantity 32316.9403477 solLum>\n",
      "(L_int/lum-1.).to(units.percent) = <Quantity 4.33075011e-05 %>\n",
      "\n",
      "\n",
      "Lum err       : lum_err =      1096.48 solLum    (rel err  =   3.39 %)\n",
      "Lum           : lum1    =     32316.93 solLum\n",
      "Old estimation: lume    =     10578.22 solLum    (rel diff = +101.36 %)\n",
      "\n",
      "    Debug  :    main():\n",
      "\tlum = 32316.92635204682 solLum\n",
      "\tarea (from <1>) = ( 30.6%) 7.652338723796763 AU2\n",
      "\tarea (from tau) = ( 29.5%) 7.3926857225365925 AU2\n",
      "\tsize (from <1>) = 2.7662860885665395 AU\n",
      "\tsize (from tau) = 2.718949378443187 AU\n",
      "\ttotal possible area = 25.025157619429656 AU2\n",
      "\tlower bound of the # of particles at photosphere, weighted avg over lum per pixels = 12.348947995816717 \n",
      "\n",
      "int(rads.size/2)-1 = 2047\n",
      "rads[int(rads.size/2)-1].cgs = <Quantity 215.28131543 g / (rad2 s3)>\n",
      "29.541015625 % rays hit photosphere\n",
      "np.std(rads[inds_active]) / np.average(rads[inds_active]) = <Quantity 1.04731217>\n",
      "lum       = 32316.93 solLum\n",
      "lum_in_ph = 31630.95 solLum    (97.88 %)\n",
      "Ended: 2025-01-30T01:14:03.914821+00:00\n",
      "Time Used: 0:00:26.495709\n",
      "\n",
      "    Note: Density column rho already exist in self.time = np.float64(20000.0).\n",
      "Start: 2025-01-30T01:14:05.814898+00:00\n",
      "\tWorking on 2md_00400_xyz...\n",
      "    Debug  :    run_code() ==> <module>() ==> integrate_along_rays_gridxy():\n",
      "\tlum = np.float64(2.9503964738218955e-06), lum2 = np.float64(2.950396473821895e-06)\n",
      "\trads.shape=(4096,), ray_areas.shape=(4096,)\n",
      "\tsrcfuncs[jused].shape=(7212,), jfact_used.shape=(7212,)\n",
      "**  Warning:    <module>() ==> integrate_along_rays_gridxy() ==> get_sph_gradient_phantom():\n",
      "\tOnly half of the process is parallelized. Pending improvements.\n",
      "    Debug  :    run_code() ==> <module>() ==> integrate_along_rays_gridxy():\n",
      "\t7212 particles actually participated calculation (0.52% of all particles, average 1.76 per ray.)\n",
      "In Progress: 2025-01-30T01:14:25.066998+00:00\n",
      "Time Used: 0:00:19.252100\n",
      "\n",
      "\n",
      "2md_00400_xyz:\n",
      "\n",
      "\tlum = 1834395.416 solLum, lum_err = 106454.670 solLum\n",
      "\n",
      "Time = 1.01 yr\n",
      "\n",
      "L_int = <Quantity 1834396.21001921 solLum>\n",
      "(L_int/lum-1.).to(units.percent) = <Quantity 4.33075012e-05 %>\n",
      "\n",
      "\n",
      "Lum err       : lum_err =    106454.67 solLum    (rel err  =   5.80 %)\n",
      "Lum           : lum1    =   1834395.42 solLum\n",
      "Old estimation: lume    =   1617878.27 solLum    (rel diff = +12.54 %)\n",
      "\n",
      "    Debug  :    main():\n",
      "\tlum = 1834395.4155883933 solLum\n",
      "\tarea (from <1>) = ( 16.7%) 79.26544721752268 AU2\n",
      "\tarea (from tau) = ( 16.5%) 78.10419038124141 AU2\n",
      "\tsize (from <1>) = 8.90311446728181 AU\n",
      "\tsize (from tau) = 8.8376575166297 AU\n",
      "\ttotal possible area = 474.6509848687905 AU2\n",
      "\tlower bound of the # of particles at photosphere, weighted avg over lum per pixels = 3.1984292658319133 \n",
      "\n",
      "int(rads.size/2)-1 = 2047\n",
      "rads[int(rads.size/2)-1].cgs = <Quantity 0. g / (rad2 s3)>\n",
      "16.455078125 % rays hit photosphere\n",
      "np.std(rads[inds_active]) / np.average(rads[inds_active]) = <Quantity 1.24308535>\n",
      "lum       = 1834395.42 solLum\n",
      "lum_in_ph = 1825847.42 solLum    (99.53 %)\n",
      "Ended: 2025-01-30T01:14:25.293317+00:00\n",
      "Time Used: 0:00:19.478419\n",
      "\n",
      "Start: 2025-01-30T01:14:25.293341+00:00\n",
      "\tWorking on 2md_00400_xzy...\n",
      "    Debug  :    run_code() ==> <module>() ==> integrate_along_rays_gridxy():\n",
      "\tlum = np.float64(1.685863268785717e-06), lum2 = np.float64(1.6858632687857173e-06)\n",
      "\trads.shape=(4096,), ray_areas.shape=(4096,)\n",
      "\tsrcfuncs[jused].shape=(7446,), jfact_used.shape=(7446,)\n",
      "**  Warning:    <module>() ==> integrate_along_rays_gridxy() ==> get_sph_gradient_phantom():\n",
      "\tOnly half of the process is parallelized. Pending improvements.\n",
      "    Debug  :    run_code() ==> <module>() ==> integrate_along_rays_gridxy():\n",
      "\t7446 particles actually participated calculation (0.54% of all particles, average 1.81 per ray.)\n",
      "In Progress: 2025-01-30T01:14:44.876543+00:00\n",
      "Time Used: 0:00:19.583202\n",
      "\n",
      "\n",
      "2md_00400_xzy:\n",
      "\n",
      "\tlum = 1048177.721 solLum, lum_err = 26675.804 solLum\n",
      "\n",
      "Time = 1.01 yr\n",
      "\n",
      "L_int = <Quantity 1048178.1748014 solLum>\n",
      "(L_int/lum-1.).to(units.percent) = <Quantity 4.33075012e-05 %>\n",
      "\n",
      "\n",
      "Lum err       : lum_err =     26675.80 solLum    (rel err  =   2.54 %)\n",
      "Lum           : lum1    =   1048177.72 solLum\n",
      "Old estimation: lume    =    954671.89 solLum    (rel diff =  +9.34 %)\n",
      "\n",
      "    Debug  :    main():\n",
      "\tlum = 1048177.7208618192 solLum\n",
      "\tarea (from <1>) = ( 22.4%) 61.61987574566126 AU2\n",
      "\tarea (from tau) = ( 21.9%) 60.182067976758425 AU2\n",
      "\tsize (from <1>) = 7.84983284826252 AU\n",
      "\tsize (from tau) = 7.757710227686932 AU\n",
      "\ttotal possible area = 275.11802503661 AU2\n",
      "\tlower bound of the # of particles at photosphere, weighted avg over lum per pixels = 4.133427413352091 \n",
      "\n",
      "int(rads.size/2)-1 = 2047\n",
      "rads[int(rads.size/2)-1].cgs = <Quantity 0. g / (rad2 s3)>\n",
      "21.875 % rays hit photosphere\n",
      "np.std(rads[inds_active]) / np.average(rads[inds_active]) = <Quantity 0.46623796>\n",
      "lum       = 1048177.72 solLum\n",
      "lum_in_ph = 1039286.51 solLum    (99.15 %)\n",
      "Ended: 2025-01-30T01:14:45.150916+00:00\n",
      "Time Used: 0:00:19.857575\n",
      "\n",
      "Start: 2025-01-30T01:14:45.150944+00:00\n",
      "\tWorking on 2md_00400_yzx...\n",
      "    Debug  :    run_code() ==> <module>() ==> integrate_along_rays_gridxy():\n",
      "\tlum = np.float64(2.0489846626029505e-06), lum2 = np.float64(2.0489846626029505e-06)\n",
      "\trads.shape=(4096,), ray_areas.shape=(4096,)\n",
      "\tsrcfuncs[jused].shape=(7075,), jfact_used.shape=(7075,)\n",
      "**  Warning:    <module>() ==> integrate_along_rays_gridxy() ==> get_sph_gradient_phantom():\n",
      "\tOnly half of the process is parallelized. Pending improvements.\n",
      "    Debug  :    run_code() ==> <module>() ==> integrate_along_rays_gridxy():\n",
      "\t7075 particles actually participated calculation (0.51% of all particles, average 1.72 per ray.)\n",
      "In Progress: 2025-01-30T01:15:05.258005+00:00\n",
      "Time Used: 0:00:20.107061\n",
      "\n",
      "\n",
      "2md_00400_yzx:\n",
      "\n",
      "\tlum = 1273946.775 solLum, lum_err = 26922.451 solLum\n",
      "\n",
      "Time = 1.01 yr\n",
      "\n",
      "L_int = <Quantity 1273947.32633931 solLum>\n",
      "(L_int/lum-1.).to(units.percent) = <Quantity 4.33075012e-05 %>\n",
      "\n",
      "\n",
      "Lum err       : lum_err =     26922.45 solLum    (rel err  =   2.11 %)\n",
      "Lum           : lum1    =   1273946.77 solLum\n",
      "Old estimation: lume    =   1137446.14 solLum    (rel diff = +11.32 %)\n",
      "\n",
      "    Debug  :    main():\n",
      "\tlum = 1273946.7746247987 solLum\n",
      "\tarea (from <1>) = ( 25.9%) 73.65815087407748 AU2\n",
      "\tarea (from tau) = ( 25.4%) 72.40784144482569 AU2\n",
      "\tsize (from <1>) = 8.582432689749304 AU\n",
      "\tsize (from tau) = 8.509279725383676 AU\n",
      "\ttotal possible area = 284.90155481076465 AU2\n",
      "\tlower bound of the # of particles at photosphere, weighted avg over lum per pixels = 4.303028074981607 \n",
      "\n",
      "int(rads.size/2)-1 = 2047\n",
      "rads[int(rads.size/2)-1].cgs = <Quantity 0. g / (rad2 s3)>\n",
      "25.4150390625 % rays hit photosphere\n",
      "np.std(rads[inds_active]) / np.average(rads[inds_active]) = <Quantity 0.48498877>\n",
      "lum       = 1273946.77 solLum\n",
      "lum_in_ph = 1267608.30 solLum    (99.50 %)\n",
      "Ended: 2025-01-30T01:15:05.500745+00:00\n",
      "Time Used: 0:00:20.349801\n",
      "\n",
      "    Note: Density column rho already exist in self.time = np.float64(40000.0).\n",
      "Start: 2025-01-30T01:15:07.272614+00:00\n",
      "\tWorking on 2md_00800_xyz...\n",
      "    Debug  :    run_code() ==> <module>() ==> integrate_along_rays_gridxy():\n",
      "\tlum = np.float64(4.841250137655423e-06), lum2 = np.float64(4.841250137632132e-06)\n",
      "\trads.shape=(4096,), ray_areas.shape=(4096,)\n",
      "\tsrcfuncs[jused].shape=(29822,), jfact_used.shape=(29822,)\n",
      "**  Warning:    <module>() ==> integrate_along_rays_gridxy() ==> get_sph_gradient_phantom():\n",
      "\tOnly half of the process is parallelized. Pending improvements.\n",
      "    Debug  :    run_code() ==> <module>() ==> integrate_along_rays_gridxy():\n",
      "\t29822 particles actually participated calculation (2.17% of all particles, average 7.28 per ray.)\n",
      "In Progress: 2025-01-30T01:15:42.563805+00:00\n",
      "Time Used: 0:00:35.291191\n",
      "\n",
      "\n",
      "2md_00800_xyz:\n",
      "\n",
      "\tlum = 3010024.970 solLum, lum_err = 31950.109 solLum\n",
      "\n",
      "Time = 2.02 yr\n",
      "\n",
      "L_int = <Quantity 3010026.27376495 solLum>\n",
      "(L_int/lum-1.).to(units.percent) = <Quantity 4.330702e-05 %>\n",
      "\n",
      "\n",
      "Lum err       : lum_err =     31950.11 solLum    (rel err  =   1.06 %)\n",
      "Lum           : lum1    =   3010024.97 solLum\n",
      "Old estimation: lume    =   2910821.02 solLum    (rel diff =  +3.35 %)\n",
      "\n",
      "    Debug  :    main():\n",
      "\tlum = 3010024.9702128284 solLum\n",
      "\tarea (from <1>) = ( 16.7%) 527.3725416977894 AU2\n",
      "\tarea (from tau) = ( 14.5%) 456.57521354237315 AU2\n",
      "\tsize (from <1>) = 22.964593218643987 AU\n",
      "\tsize (from tau) = 21.36762068042142 AU\n",
      "\ttotal possible area = 3153.6797211965604 AU2\n",
      "\tlower bound of the # of particles at photosphere, weighted avg over lum per pixels = 10.233687680909906 \n",
      "\n",
      "int(rads.size/2)-1 = 2047\n",
      "rads[int(rads.size/2)-1].cgs = <Quantity 0. g / (rad2 s3)>\n",
      "14.4775390625 % rays hit photosphere\n",
      "np.std(rads[inds_active]) / np.average(rads[inds_active]) = <Quantity 1.28650003>\n",
      "lum       = 3010024.97 solLum\n",
      "lum_in_ph = 2927800.31 solLum    (97.27 %)\n",
      "Ended: 2025-01-30T01:15:43.414530+00:00\n",
      "Time Used: 0:00:36.141916\n",
      "\n",
      "Start: 2025-01-30T01:15:43.414561+00:00\n",
      "\tWorking on 2md_00800_xzy...\n",
      "    Debug  :    run_code() ==> <module>() ==> integrate_along_rays_gridxy():\n",
      "\tlum = np.float64(1.027969605448011e-06), lum2 = np.float64(1.0279696054311647e-06)\n",
      "\trads.shape=(4096,), ray_areas.shape=(4096,)\n",
      "\tsrcfuncs[jused].shape=(29704,), jfact_used.shape=(29704,)\n",
      "**  Warning:    <module>() ==> integrate_along_rays_gridxy() ==> get_sph_gradient_phantom():\n",
      "\tOnly half of the process is parallelized. Pending improvements.\n",
      "    Debug  :    run_code() ==> <module>() ==> integrate_along_rays_gridxy():\n",
      "\t29704 particles actually participated calculation (2.16% of all particles, average 7.25 per ray.)\n",
      "In Progress: 2025-01-30T01:16:14.113543+00:00\n",
      "Time Used: 0:00:30.698982\n",
      "\n",
      "\n",
      "2md_00800_xzy:\n",
      "\n",
      "\tlum = 639135.366 solLum, lum_err = 9371.455 solLum\n",
      "\n",
      "Time = 2.02 yr\n",
      "\n",
      "L_int = <Quantity 639135.64327684 solLum>\n",
      "(L_int/lum-1.).to(units.percent) = <Quantity 4.33058623e-05 %>\n",
      "\n",
      "\n",
      "Lum err       : lum_err =      9371.46 solLum    (rel err  =   1.47 %)\n",
      "Lum           : lum1    =    639135.37 solLum\n",
      "Old estimation: lume    =    625528.43 solLum    (rel diff =  +2.15 %)\n",
      "\n",
      "    Debug  :    main():\n",
      "\tlum = 639135.3664937555 solLum\n",
      "\tarea (from <1>) = ( 10.2%) 408.181411091324 AU2\n",
      "\tarea (from tau) = (  8.7%) 349.58643948898793 AU2\n",
      "\tsize (from <1>) = 20.20349997132487 AU\n",
      "\tsize (from tau) = 18.697230797339696 AU\n",
      "\ttotal possible area = 4010.941333744803 AU2\n",
      "\tlower bound of the # of particles at photosphere, weighted avg over lum per pixels = 3.764693094996854 \n",
      "\n",
      "int(rads.size/2)-1 = 2047\n",
      "rads[int(rads.size/2)-1].cgs = <Quantity 0. g / (rad2 s3)>\n",
      "8.7158203125 % rays hit photosphere\n",
      "np.std(rads[inds_active]) / np.average(rads[inds_active]) = <Quantity 1.67634582>\n",
      "lum       = 639135.37 solLum\n",
      "lum_in_ph = 631006.15 solLum    (98.73 %)\n",
      "Ended: 2025-01-30T01:16:14.995830+00:00\n",
      "Time Used: 0:00:31.581269\n",
      "\n",
      "Start: 2025-01-30T01:16:14.995862+00:00\n",
      "\tWorking on 2md_00800_yzx...\n",
      "    Debug  :    run_code() ==> <module>() ==> integrate_along_rays_gridxy():\n",
      "\tlum = np.float64(2.9088213881571836e-06), lum2 = np.float64(2.9088213881571844e-06)\n",
      "\trads.shape=(4096,), ray_areas.shape=(4096,)\n",
      "\tsrcfuncs[jused].shape=(27922,), jfact_used.shape=(27922,)\n",
      "**  Warning:    <module>() ==> integrate_along_rays_gridxy() ==> get_sph_gradient_phantom():\n",
      "\tOnly half of the process is parallelized. Pending improvements.\n",
      "    Debug  :    run_code() ==> <module>() ==> integrate_along_rays_gridxy():\n",
      "\t27922 particles actually participated calculation (2.03% of all particles, average 6.81 per ray.)\n",
      "In Progress: 2025-01-30T01:16:49.890273+00:00\n",
      "Time Used: 0:00:34.894411\n",
      "\n",
      "\n",
      "2md_00800_yzx:\n",
      "\n",
      "\tlum = 1808546.298 solLum, lum_err = 12476.258 solLum\n",
      "\n",
      "Time = 2.02 yr\n",
      "\n",
      "L_int = <Quantity 1808547.08084242 solLum>\n",
      "(L_int/lum-1.).to(units.percent) = <Quantity 4.33075011e-05 %>\n",
      "\n",
      "\n",
      "Lum err       : lum_err =     12476.26 solLum    (rel err  =   0.69 %)\n",
      "Lum           : lum1    =   1808546.30 solLum\n",
      "Old estimation: lume    =   1790215.85 solLum    (rel diff =  +1.02 %)\n",
      "\n",
      "    Debug  :    main():\n",
      "\tlum = 1808546.297606209 solLum\n",
      "\tarea (from <1>) = ( 11.8%) 446.2746517090371 AU2\n",
      "\tarea (from tau) = ( 10.0%) 375.2525986564956 AU2\n",
      "\tsize (from <1>) = 21.12521364883766 AU\n",
      "\tsize (from tau) = 19.371437702362094 AU\n",
      "\ttotal possible area = 3767.241774747563 AU2\n",
      "\tlower bound of the # of particles at photosphere, weighted avg over lum per pixels = 13.333396417539051 \n",
      "\n",
      "int(rads.size/2)-1 = 2047\n",
      "rads[int(rads.size/2)-1].cgs = <Quantity 0.03072282 g / (rad2 s3)>\n",
      "9.9609375 % rays hit photosphere\n",
      "np.std(rads[inds_active]) / np.average(rads[inds_active]) = <Quantity 1.36277014>\n",
      "lum       = 1808546.30 solLum\n",
      "lum_in_ph = 1766142.18 solLum    (97.66 %)\n",
      "Ended: 2025-01-30T01:16:50.764275+00:00\n",
      "Time Used: 0:00:35.768413\n",
      "\n",
      "    Note: Density column rho already exist in self.time = np.float64(0.0).\n",
      "Start: 2025-01-30T01:16:53.140099+00:00\n",
      "\tWorking on 4md_00000_xyz...\n",
      "    Debug  :    run_code() ==> <module>() ==> integrate_along_rays_gridxy():\n",
      "\tlum = np.float64(1.2946774974942595e-07), lum2 = np.float64(1.2946774974942595e-07)\n",
      "\trads.shape=(4096,), ray_areas.shape=(4096,)\n",
      "\tsrcfuncs[jused].shape=(10604,), jfact_used.shape=(10604,)\n",
      "**  Warning:    <module>() ==> integrate_along_rays_gridxy() ==> get_sph_gradient_phantom():\n",
      "\tOnly half of the process is parallelized. Pending improvements.\n",
      "    Debug  :    run_code() ==> <module>() ==> integrate_along_rays_gridxy():\n",
      "\t10604 particles actually participated calculation (0.77% of all particles, average 2.58 per ray.)\n",
      "In Progress: 2025-01-30T01:17:12.320513+00:00\n",
      "Time Used: 0:00:19.180414\n",
      "\n",
      "\n",
      "4md_00000_xyz:\n",
      "\n",
      "\tlum = 80495.977 solLum, lum_err = 3297.201 solLum\n",
      "\n",
      "Time = 0.00 yr\n",
      "\n",
      "L_int = <Quantity 80496.01183004 solLum>\n",
      "(L_int/lum-1.).to(units.percent) = <Quantity 4.33075011e-05 %>\n",
      "\n",
      "\n",
      "Lum err       : lum_err =      3297.20 solLum    (rel err  =   4.10 %)\n",
      "Lum           : lum1    =     80495.98 solLum\n",
      "Old estimation: lume    =     21124.88 solLum    (rel diff = +116.85 %)\n",
      "\n",
      "    Debug  :    main():\n",
      "\tlum = 80495.97696923936 solLum\n",
      "\tarea (from <1>) = ( 47.6%) 10.866051989836397 AU2\n",
      "\tarea (from tau) = ( 47.2%) 10.766466028824023 AU2\n",
      "\tsize (from <1>) = 3.2963695165797775 AU\n",
      "\tsize (from tau) = 3.2812293471843788 AU\n",
      "\ttotal possible area = 22.813991129882673 AU2\n",
      "\tlower bound of the # of particles at photosphere, weighted avg over lum per pixels = 7.356451266544031 \n",
      "\n",
      "int(rads.size/2)-1 = 2047\n",
      "rads[int(rads.size/2)-1].cgs = <Quantity 27900.90177083 g / (rad2 s3)>\n",
      "47.1923828125 % rays hit photosphere\n",
      "np.std(rads[inds_active]) / np.average(rads[inds_active]) = <Quantity 1.07511805>\n",
      "lum       = 80495.98 solLum\n",
      "lum_in_ph = 79792.99 solLum    (99.13 %)\n",
      "Ended: 2025-01-30T01:17:12.664098+00:00\n",
      "Time Used: 0:00:19.523999\n",
      "\n",
      "Start: 2025-01-30T01:17:12.664127+00:00\n",
      "\tWorking on 4md_00000_xzy...\n",
      "    Debug  :    run_code() ==> <module>() ==> integrate_along_rays_gridxy():\n",
      "\tlum = np.float64(1.253371862794871e-07), lum2 = np.float64(1.2533718627948706e-07)\n",
      "\trads.shape=(4096,), ray_areas.shape=(4096,)\n",
      "\tsrcfuncs[jused].shape=(10595,), jfact_used.shape=(10595,)\n",
      "**  Warning:    <module>() ==> integrate_along_rays_gridxy() ==> get_sph_gradient_phantom():\n",
      "\tOnly half of the process is parallelized. Pending improvements.\n",
      "    Debug  :    run_code() ==> <module>() ==> integrate_along_rays_gridxy():\n",
      "\t10595 particles actually participated calculation (0.77% of all particles, average 2.58 per ray.)\n",
      "In Progress: 2025-01-30T01:17:30.727193+00:00\n",
      "Time Used: 0:00:18.063066\n",
      "\n",
      "\n",
      "4md_00000_xzy:\n",
      "\n",
      "\tlum = 77927.818 solLum, lum_err = 3200.566 solLum\n",
      "\n",
      "Time = 0.00 yr\n",
      "\n",
      "L_int = <Quantity 77927.85190925 solLum>\n",
      "(L_int/lum-1.).to(units.percent) = <Quantity 4.33075011e-05 %>\n",
      "\n",
      "\n",
      "Lum err       : lum_err =      3200.57 solLum    (rel err  =   4.11 %)\n",
      "Lum           : lum1    =     77927.82 solLum\n",
      "Old estimation: lume    =     20957.84 solLum    (rel diff = +115.22 %)\n",
      "\n",
      "    Debug  :    main():\n",
      "\tlum = 77927.81816065812 solLum\n",
      "\tarea (from <1>) = ( 47.4%) 10.815476030793834 AU2\n",
      "\tarea (from tau) = ( 47.1%) 10.745721874802989 AU2\n",
      "\tsize (from <1>) = 3.2886891052201688 AU\n",
      "\tsize (from tau) = 3.278066789252926 AU\n",
      "\ttotal possible area = 22.80542839336427 AU2\n",
      "\tlower bound of the # of particles at photosphere, weighted avg over lum per pixels = 7.3568932907596185 \n",
      "\n",
      "int(rads.size/2)-1 = 2047\n",
      "rads[int(rads.size/2)-1].cgs = <Quantity 17.13737199 g / (rad2 s3)>\n",
      "47.119140625 % rays hit photosphere\n",
      "np.std(rads[inds_active]) / np.average(rads[inds_active]) = <Quantity 1.07815249>\n",
      "lum       = 77927.82 solLum\n",
      "lum_in_ph = 77275.92 solLum    (99.16 %)\n",
      "Ended: 2025-01-30T01:17:31.061913+00:00\n",
      "Time Used: 0:00:18.397786\n",
      "\n",
      "Start: 2025-01-30T01:17:31.061940+00:00\n",
      "\tWorking on 4md_00000_yzx...\n",
      "    Debug  :    run_code() ==> <module>() ==> integrate_along_rays_gridxy():\n",
      "\tlum = np.float64(1.3146555679998604e-07), lum2 = np.float64(1.3146555679998606e-07)\n",
      "\trads.shape=(4096,), ray_areas.shape=(4096,)\n",
      "\tsrcfuncs[jused].shape=(10617,), jfact_used.shape=(10617,)\n",
      "**  Warning:    <module>() ==> integrate_along_rays_gridxy() ==> get_sph_gradient_phantom():\n",
      "\tOnly half of the process is parallelized. Pending improvements.\n",
      "    Debug  :    run_code() ==> <module>() ==> integrate_along_rays_gridxy():\n",
      "\t10617 particles actually participated calculation (0.77% of all particles, average 2.59 per ray.)\n",
      "In Progress: 2025-01-30T01:17:49.829901+00:00\n",
      "Time Used: 0:00:18.767961\n",
      "\n",
      "\n",
      "4md_00000_yzx:\n",
      "\n",
      "\tlum = 81738.104 solLum, lum_err = 3450.638 solLum\n",
      "\n",
      "Time = 0.00 yr\n",
      "\n",
      "L_int = <Quantity 81738.13969807 solLum>\n",
      "(L_int/lum-1.).to(units.percent) = <Quantity 4.33075012e-05 %>\n",
      "\n",
      "\n",
      "Lum err       : lum_err =      3450.64 solLum    (rel err  =   4.22 %)\n",
      "Lum           : lum1    =     81738.10 solLum\n",
      "Old estimation: lume    =     21311.13 solLum    (rel diff = +117.28 %)\n",
      "\n",
      "    Debug  :    main():\n",
      "\tlum = 81738.10429934367 solLum\n",
      "\tarea (from <1>) = ( 47.3%) 10.967342191016174 AU2\n",
      "\tarea (from tau) = ( 46.8%) 10.845749358993938 AU2\n",
      "\tsize (from <1>) = 3.3116977807487467 AU\n",
      "\tsize (from tau) = 3.2932885326059633 AU\n",
      "\ttotal possible area = 23.18590259626262 AU2\n",
      "\tlower bound of the # of particles at photosphere, weighted avg over lum per pixels = 7.354788044953888 \n",
      "\n",
      "int(rads.size/2)-1 = 2047\n",
      "rads[int(rads.size/2)-1].cgs = <Quantity 0. g / (rad2 s3)>\n",
      "46.77734375 % rays hit photosphere\n",
      "np.std(rads[inds_active]) / np.average(rads[inds_active]) = <Quantity 1.12796137>\n",
      "lum       = 81738.10 solLum\n",
      "lum_in_ph = 80974.18 solLum    (99.07 %)\n",
      "Ended: 2025-01-30T01:17:50.220425+00:00\n",
      "Time Used: 0:00:19.158485\n",
      "\n",
      "    Note: Density column rho already exist in self.time = np.float64(20000.0).\n",
      "Start: 2025-01-30T01:17:52.288503+00:00\n",
      "\tWorking on 4md_00400_xyz...\n",
      "    Debug  :    run_code() ==> <module>() ==> integrate_along_rays_gridxy():\n",
      "\tlum = np.float64(3.968862360716444e-06), lum2 = np.float64(3.9688623606645855e-06)\n",
      "\trads.shape=(4096,), ray_areas.shape=(4096,)\n",
      "\tsrcfuncs[jused].shape=(1366,), jfact_used.shape=(1366,)\n",
      "**  Warning:    <module>() ==> integrate_along_rays_gridxy() ==> get_sph_gradient_phantom():\n",
      "\tOnly half of the process is parallelized. Pending improvements.\n",
      "    Debug  :    run_code() ==> <module>() ==> integrate_along_rays_gridxy():\n",
      "\t1366 particles actually participated calculation (0.09% of all particles, average 0.33 per ray.)\n",
      "In Progress: 2025-01-30T01:18:09.083798+00:00\n",
      "Time Used: 0:00:16.795295\n",
      "\n",
      "\n",
      "4md_00400_xyz:\n",
      "\n",
      "\tlum = 2467621.889 solLum, lum_err = 69205.318 solLum\n",
      "\n",
      "Time = 1.01 yr\n",
      "\n",
      "L_int = <Quantity 2467622.9574868 solLum>\n",
      "(L_int/lum-1.).to(units.percent) = <Quantity 4.33061946e-05 %>\n",
      "\n",
      "\n",
      "Lum err       : lum_err =     69205.32 solLum    (rel err  =   2.80 %)\n",
      "Lum           : lum1    =   2467621.89 solLum\n",
      "Old estimation: lume    =   2403535.35 solLum    (rel diff =  +2.63 %)\n",
      "\n",
      "    Debug  :    main():\n",
      "\tlum = 2467621.888853663 solLum\n",
      "\tarea (from <1>) = ( 12.0%) 51.361572832152646 AU2\n",
      "\tarea (from tau) = ( 11.2%) 47.94429865939363 AU2\n",
      "\tsize (from <1>) = 7.166698879690191 AU\n",
      "\tsize (from tau) = 6.9241821653819615 AU\n",
      "\ttotal possible area = 428.7769591896862 AU2\n",
      "\tlower bound of the # of particles at photosphere, weighted avg over lum per pixels = 1.5923324073183902 \n",
      "\n",
      "int(rads.size/2)-1 = 2047\n",
      "rads[int(rads.size/2)-1].cgs = <Quantity 0. g / (rad2 s3)>\n",
      "11.181640625 % rays hit photosphere\n",
      "np.std(rads[inds_active]) / np.average(rads[inds_active]) = <Quantity 0.60393098>\n",
      "lum       = 2467621.89 solLum\n",
      "lum_in_ph = 2429240.57 solLum    (98.44 %)\n",
      "Ended: 2025-01-30T01:18:09.193529+00:00\n",
      "Time Used: 0:00:16.905026\n",
      "\n",
      "Start: 2025-01-30T01:18:09.193551+00:00\n",
      "\tWorking on 4md_00400_xzy...\n",
      "    Debug  :    run_code() ==> <module>() ==> integrate_along_rays_gridxy():\n",
      "\tlum = np.float64(2.0429803891900078e-06), lum2 = np.float64(2.042980389187853e-06)\n",
      "\trads.shape=(4096,), ray_areas.shape=(4096,)\n",
      "\tsrcfuncs[jused].shape=(1319,), jfact_used.shape=(1319,)\n",
      "**  Warning:    <module>() ==> integrate_along_rays_gridxy() ==> get_sph_gradient_phantom():\n",
      "\tOnly half of the process is parallelized. Pending improvements.\n",
      "    Debug  :    run_code() ==> <module>() ==> integrate_along_rays_gridxy():\n",
      "\t1319 particles actually participated calculation (0.09% of all particles, average 0.32 per ray.)\n",
      "In Progress: 2025-01-30T01:18:25.136143+00:00\n",
      "Time Used: 0:00:15.942592\n",
      "\n",
      "\n",
      "4md_00400_xzy:\n",
      "\n",
      "\tlum = 1270213.645 solLum, lum_err = 39546.328 solLum\n",
      "\n",
      "Time = 1.01 yr\n",
      "\n",
      "L_int = <Quantity 1270214.19538749 solLum>\n",
      "(L_int/lum-1.).to(units.percent) = <Quantity 4.33073957e-05 %>\n",
      "\n",
      "\n",
      "Lum err       : lum_err =     39546.33 solLum    (rel err  =   3.11 %)\n",
      "Lum           : lum1    =   1270213.65 solLum\n",
      "Old estimation: lume    =   1234956.69 solLum    (rel diff =  +2.81 %)\n",
      "\n",
      "    Debug  :    main():\n",
      "\tlum = 1270213.6452910404 solLum\n",
      "\tarea (from <1>) = ( 13.0%) 36.10865062611998 AU2\n",
      "\tarea (from tau) = ( 12.3%) 34.28712117231601 AU2\n",
      "\tsize (from <1>) = 6.009047397559781 AU\n",
      "\tsize (from tau) = 5.855520572273315 AU\n",
      "\ttotal possible area = 278.6508895273937 AU2\n",
      "\tlower bound of the # of particles at photosphere, weighted avg over lum per pixels = 2.3825670408119413 \n",
      "\n",
      "int(rads.size/2)-1 = 2047\n",
      "rads[int(rads.size/2)-1].cgs = <Quantity 0. g / (rad2 s3)>\n",
      "12.3046875 % rays hit photosphere\n",
      "np.std(rads[inds_active]) / np.average(rads[inds_active]) = <Quantity 0.56241957>\n",
      "lum       = 1270213.65 solLum\n",
      "lum_in_ph = 1245386.10 solLum    (98.05 %)\n",
      "Ended: 2025-01-30T01:18:25.240478+00:00\n",
      "Time Used: 0:00:16.046927\n",
      "\n",
      "Start: 2025-01-30T01:18:25.240500+00:00\n",
      "\tWorking on 4md_00400_yzx...\n",
      "    Debug  :    run_code() ==> <module>() ==> integrate_along_rays_gridxy():\n",
      "\tlum = np.float64(3.2282553317100676e-06), lum2 = np.float64(3.228255331708456e-06)\n",
      "\trads.shape=(4096,), ray_areas.shape=(4096,)\n",
      "\tsrcfuncs[jused].shape=(1328,), jfact_used.shape=(1328,)\n",
      "**  Warning:    <module>() ==> integrate_along_rays_gridxy() ==> get_sph_gradient_phantom():\n",
      "\tOnly half of the process is parallelized. Pending improvements.\n",
      "    Debug  :    run_code() ==> <module>() ==> integrate_along_rays_gridxy():\n",
      "\t1328 particles actually participated calculation (0.09% of all particles, average 0.32 per ray.)\n",
      "In Progress: 2025-01-30T01:18:40.824157+00:00\n",
      "Time Used: 0:00:15.583657\n",
      "\n",
      "\n",
      "4md_00400_yzx:\n",
      "\n",
      "\tlum = 2007152.880 solLum, lum_err = 64051.098 solLum\n",
      "\n",
      "Time = 1.01 yr\n",
      "\n",
      "L_int = <Quantity 2007153.74967527 solLum>\n",
      "(L_int/lum-1.).to(units.percent) = <Quantity 4.33074513e-05 %>\n",
      "\n",
      "\n",
      "Lum err       : lum_err =     64051.10 solLum    (rel err  =   3.19 %)\n",
      "Lum           : lum1    =   2007152.88 solLum\n",
      "Old estimation: lume    =   1968193.88 solLum    (rel diff =  +1.96 %)\n",
      "\n",
      "    Debug  :    main():\n",
      "\tlum = 2007152.8804285098 solLum\n",
      "\tarea (from <1>) = ( 18.7%) 48.811056962510115 AU2\n",
      "\tarea (from tau) = ( 17.9%) 46.928174120306466 AU2\n",
      "\tsize (from <1>) = 6.9864910335954855 AU\n",
      "\tsize (from tau) = 6.850414156845297 AU\n",
      "\ttotal possible area = 261.5208179547963 AU2\n",
      "\tlower bound of the # of particles at photosphere, weighted avg over lum per pixels = 1.764278919358723 \n",
      "\n",
      "int(rads.size/2)-1 = 2047\n",
      "rads[int(rads.size/2)-1].cgs = <Quantity 0. g / (rad2 s3)>\n",
      "17.9443359375 % rays hit photosphere\n",
      "np.std(rads[inds_active]) / np.average(rads[inds_active]) = <Quantity 0.60657929>\n",
      "lum       = 2007152.88 solLum\n",
      "lum_in_ph = 1973150.20 solLum    (98.31 %)\n",
      "Ended: 2025-01-30T01:18:40.939523+00:00\n",
      "Time Used: 0:00:15.699023\n",
      "\n",
      "    Note: Density column rho already exist in self.time = np.float64(40000.0).\n",
      "Start: 2025-01-30T01:18:42.768460+00:00\n",
      "\tWorking on 4md_00800_xyz...\n",
      "    Debug  :    run_code() ==> <module>() ==> integrate_along_rays_gridxy():\n",
      "\tlum = np.float64(5.318164961634031e-06), lum2 = np.float64(5.318164961619615e-06)\n",
      "\trads.shape=(4096,), ray_areas.shape=(4096,)\n",
      "\tsrcfuncs[jused].shape=(3165,), jfact_used.shape=(3165,)\n",
      "**  Warning:    <module>() ==> integrate_along_rays_gridxy() ==> get_sph_gradient_phantom():\n",
      "\tOnly half of the process is parallelized. Pending improvements.\n",
      "    Debug  :    run_code() ==> <module>() ==> integrate_along_rays_gridxy():\n",
      "\t3165 particles actually participated calculation (0.23% of all particles, average 0.77 per ray.)\n",
      "In Progress: 2025-01-30T01:19:02.298838+00:00\n",
      "Time Used: 0:00:19.530378\n",
      "\n",
      "\n",
      "4md_00800_xyz:\n",
      "\n",
      "\tlum = 3306544.565 solLum, lum_err = 85340.140 solLum\n",
      "\n",
      "Time = 2.02 yr\n",
      "\n",
      "L_int = <Quantity 3306545.99692315 solLum>\n",
      "(L_int/lum-1.).to(units.percent) = <Quantity 4.33072301e-05 %>\n",
      "\n",
      "\n",
      "Lum err       : lum_err =     85340.14 solLum    (rel err  =   2.58 %)\n",
      "Lum           : lum1    =   3306544.56 solLum\n",
      "Old estimation: lume    =   3045195.21 solLum    (rel diff =  +8.23 %)\n",
      "\n",
      "    Debug  :    main():\n",
      "\tlum = 3306544.56495029 solLum\n",
      "\tarea (from <1>) = (  3.6%) 171.14654697047717 AU2\n",
      "\tarea (from tau) = (  2.5%) 119.72228759983068 AU2\n",
      "\tsize (from <1>) = 13.082298994078876 AU\n",
      "\tsize (from tau) = 10.941768028971858 AU\n",
      "\ttotal possible area = 4760.99504863016 AU2\n",
      "\tlower bound of the # of particles at photosphere, weighted avg over lum per pixels = 3.8561227456400373 \n",
      "\n",
      "int(rads.size/2)-1 = 2047\n",
      "rads[int(rads.size/2)-1].cgs = <Quantity 0.06021245 g / (rad2 s3)>\n",
      "2.5146484375 % rays hit photosphere\n",
      "np.std(rads[inds_active]) / np.average(rads[inds_active]) = <Quantity 0.56814087>\n",
      "lum       = 3306544.56 solLum\n",
      "lum_in_ph = 3218684.58 solLum    (97.34 %)\n",
      "Ended: 2025-01-30T01:19:02.438856+00:00\n",
      "Time Used: 0:00:19.670396\n",
      "\n",
      "Start: 2025-01-30T01:19:02.438879+00:00\n",
      "\tWorking on 4md_00800_xzy...\n",
      "    Debug  :    run_code() ==> <module>() ==> integrate_along_rays_gridxy():\n",
      "\tlum = np.float64(2.8208960286233057e-06), lum2 = np.float64(2.820896028623068e-06)\n",
      "\trads.shape=(4096,), ray_areas.shape=(4096,)\n",
      "\tsrcfuncs[jused].shape=(3022,), jfact_used.shape=(3022,)\n",
      "**  Warning:    <module>() ==> integrate_along_rays_gridxy() ==> get_sph_gradient_phantom():\n",
      "\tOnly half of the process is parallelized. Pending improvements.\n",
      "    Debug  :    run_code() ==> <module>() ==> integrate_along_rays_gridxy():\n",
      "\t3022 particles actually participated calculation (0.22% of all particles, average 0.73 per ray.)\n",
      "In Progress: 2025-01-30T01:19:22.110325+00:00\n",
      "Time Used: 0:00:19.671446\n",
      "\n",
      "\n",
      "4md_00800_xzy:\n",
      "\n",
      "\tlum = 1753879.110 solLum, lum_err = 48407.967 solLum\n",
      "\n",
      "Time = 2.02 yr\n",
      "\n",
      "L_int = <Quantity 1753879.86993533 solLum>\n",
      "(L_int/lum-1.).to(units.percent) = <Quantity 4.33074927e-05 %>\n",
      "\n",
      "\n",
      "Lum err       : lum_err =     48407.97 solLum    (rel err  =   2.76 %)\n",
      "Lum           : lum1    =   1753879.11 solLum\n",
      "Old estimation: lume    =   1641039.03 solLum    (rel diff =  +6.65 %)\n",
      "\n",
      "    Debug  :    main():\n",
      "\tlum = 1753879.110374259 solLum\n",
      "\tarea (from <1>) = (  4.5%) 129.8396984154161 AU2\n",
      "\tarea (from tau) = (  3.0%) 84.71396284668505 AU2\n",
      "\tsize (from <1>) = 11.394722393082514 AU\n",
      "\tsize (from tau) = 9.204018842151783 AU\n",
      "\ttotal possible area = 2867.6726596696035 AU2\n",
      "\tlower bound of the # of particles at photosphere, weighted avg over lum per pixels = 4.117945456262332 \n",
      "\n",
      "int(rads.size/2)-1 = 2047\n",
      "rads[int(rads.size/2)-1].cgs = <Quantity 0.00738352 g / (rad2 s3)>\n",
      "2.9541015625 % rays hit photosphere\n",
      "np.std(rads[inds_active]) / np.average(rads[inds_active]) = <Quantity 0.5429691>\n",
      "lum       = 1753879.11 solLum\n",
      "lum_in_ph = 1707049.29 solLum    (97.33 %)\n",
      "Ended: 2025-01-30T01:19:22.256363+00:00\n",
      "Time Used: 0:00:19.817484\n",
      "\n",
      "Start: 2025-01-30T01:19:22.256387+00:00\n",
      "\tWorking on 4md_00800_yzx...\n",
      "    Debug  :    run_code() ==> <module>() ==> integrate_along_rays_gridxy():\n",
      "\tlum = np.float64(3.431734978697474e-06), lum2 = np.float64(3.431734978697475e-06)\n",
      "\trads.shape=(4096,), ray_areas.shape=(4096,)\n",
      "\tsrcfuncs[jused].shape=(3183,), jfact_used.shape=(3183,)\n",
      "**  Warning:    <module>() ==> integrate_along_rays_gridxy() ==> get_sph_gradient_phantom():\n",
      "\tOnly half of the process is parallelized. Pending improvements.\n",
      "    Debug  :    run_code() ==> <module>() ==> integrate_along_rays_gridxy():\n",
      "\t3183 particles actually participated calculation (0.23% of all particles, average 0.77 per ray.)\n",
      "In Progress: 2025-01-30T01:19:41.430146+00:00\n",
      "Time Used: 0:00:19.173759\n",
      "\n",
      "\n",
      "4md_00800_yzx:\n",
      "\n",
      "\tlum = 2133665.414 solLum, lum_err = 68118.304 solLum\n",
      "\n",
      "Time = 2.02 yr\n",
      "\n",
      "L_int = <Quantity 2133666.33758152 solLum>\n",
      "(L_int/lum-1.).to(units.percent) = <Quantity 4.33075012e-05 %>\n",
      "\n",
      "\n",
      "Lum err       : lum_err =     68118.30 solLum    (rel err  =   3.19 %)\n",
      "Lum           : lum1    =   2133665.41 solLum\n",
      "Old estimation: lume    =   2011610.15 solLum    (rel diff =  +5.89 %)\n",
      "\n",
      "    Debug  :    main():\n",
      "\tlum = 2133665.4135443475 solLum\n",
      "\tarea (from <1>) = (  5.1%) 141.46906342674075 AU2\n",
      "\tarea (from tau) = (  3.3%) 91.12226833895906 AU2\n",
      "\tsize (from <1>) = 11.894076821121542 AU\n",
      "\tsize (from tau) = 9.54579846523899 AU\n",
      "\ttotal possible area = 2764.7171193805652 AU2\n",
      "\tlower bound of the # of particles at photosphere, weighted avg over lum per pixels = 4.226395018318025 \n",
      "\n",
      "int(rads.size/2)-1 = 2047\n",
      "rads[int(rads.size/2)-1].cgs = <Quantity 0. g / (rad2 s3)>\n",
      "3.2958984375 % rays hit photosphere\n",
      "np.std(rads[inds_active]) / np.average(rads[inds_active]) = <Quantity 0.4613953>\n",
      "lum       = 2133665.41 solLum\n",
      "lum_in_ph = 2080031.17 solLum    (97.49 %)\n",
      "Ended: 2025-01-30T01:19:41.571138+00:00\n",
      "Time Used: 0:00:19.314751\n",
      "\n",
      "*   Note   :    run_code() ==> <module>() ==> hdf5_dump():\n",
      "\tWriting to ../interm/Tscaled_lcgen.64x64.hdf5  (will OVERWRITE if file already exist.; compress='gzip')\n",
      "*   Note   :    run_code() ==> <module>() ==> hdf5_dump():\n",
      "\tCompressing and saving to ../interm/Tscaled_lcgen.64x64.hdf5.gz;\n",
      "\tDeleting ../interm/Tscaled_lcgen.64x64.hdf5\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__' and not do_debug:\n",
    "\n",
    "\n",
    "    use_new_algo  = True\n",
    "    \n",
    "    # init combined data\n",
    "    comb = {}\n",
    "    \n",
    "    for job_nickname in job_nicknames: #['2md', ]:  #\n",
    "        job_profile = JOB_PROFILES_DICT[job_nickname]\n",
    "        job_name    = job_profile['job_name']\n",
    "        file_indexes= job_profile['file_indexes']  #[17600,]  #\n",
    "        params      = job_profile['params']\n",
    "        eos_opacity = get_eos_opacity(ieos=10, params=params)    #EoS_MESA_opacity(params, settings)\n",
    "        \n",
    "        comb[job_nickname] = {\n",
    "            xyzs: {\n",
    "                'times': np.full(len(file_indexes), np.nan) * units.yr,\n",
    "                'lums' : np.full(len(file_indexes), np.nan) * units.Lsun,\n",
    "                'lums_err': np.full(len(file_indexes), np.nan) * units.Lsun,\n",
    "                'areas': np.full(len(file_indexes), np.nan) * units.au**2,\n",
    "                # no of particles at the photosphere - lower bound (weighted average per pixel, weighted by lums contribution)\n",
    "                # i.e. how resolved the photosphere is\n",
    "                'N_res': np.full(len(file_indexes), -1) * units.dimensionless_unscaled,\n",
    "                'wavlens': wavlens,\n",
    "                'L_wavs': np.full((len(file_indexes), len(wavlens)), np.nan) * (units.Lsun/units.angstrom),\n",
    "                '_meta_': {\n",
    "                    'lums' : { 'Description': \"Luminosity.\", },\n",
    "                    'areas': { 'Description': (\n",
    "                        \"Visible size of the simulated object.\" +\n",
    "                        \"(i.e. pixel * (area per pixel) * (tau if tau<1 else 1)\"), },\n",
    "                    'N_res': { 'Description': (\n",
    "                            \"no of particles at the photosphere - lower bound\" +\n",
    "                            \"(weighted average per pixel, weighted by lums contribution per pixel)\"), },\n",
    "                },\n",
    "            } for xyzs in xyzs_list\n",
    "        }\n",
    "\n",
    "            \n",
    "        for ifile, file_index in enumerate(file_indexes):\n",
    "            # init\n",
    "    \n",
    "            mpdf = mpdf_read(job_name, file_index, eos_opacity, reset_xyz_by='R1', use_Tscales=use_Tscales, verbose=1)\n",
    "            mpdf.calc_sdf_params(['R1'])\n",
    "            sdf  = mpdf.data['gas']\n",
    "            # kernel = sdf.kernel\n",
    "            # kernel_rad = float(kernel.get_radius())\n",
    "            # col_kernel = kernel.get_column_kernel_func(samples=1000)\n",
    "            srcfuncs = mpdf.const['sigma_sb'] * sdf['T']**4 / pi\n",
    "            sdf['srcfunc'] = srcfuncs\n",
    "\n",
    "            with mupl.hdf5_open(\n",
    "                f\"{interm_dir}{job_nickname}_{file_index:05d}.lcgen.{no_xy_txt}.hdf5\",\n",
    "                'a', metadata) as out_interm_grp1:\n",
    "                #out_interm_grp1 = mupl.hdf5_subgroup(out_interm_file, f\"{file_index:05d}\", {})\n",
    "\n",
    "                for xyzs in xyzs_list:\n",
    "                    xyzs_names_list = [x for x in xyzs]\n",
    "        \n",
    "                    # record time used\n",
    "                    python_time_start = now()\n",
    "                    print(f\"Start: {python_time_start.isoformat()}\")\n",
    "                    print(f\"\\tWorking on {job_nickname}_{file_index:05d}_{xyzs}...\")\n",
    "        \n",
    "                    \n",
    "                    # get rays\n",
    "                    rays, areas, dXs = get_xy_grids_of_rays(\n",
    "                        sdf, no_xy=no_xy, frac_contained=100.,\n",
    "                        use_adaptive_grid=False, xyzs_names_list=xyzs_names_list)\n",
    "                    ray_areas = areas\n",
    "                    pts    = np.array(sdf[xyzs_names_list])\n",
    "                    hs     = np.array(sdf[ 'h' ])    # npart-shaped array\n",
    "                    \n",
    "                    rays_u = (rays * mpdf.units['dist']).to(units.au)\n",
    "                    areas_u = (areas * mpdf.units['dist']**2).to(units.au**2)\n",
    "        \n",
    "                    \n",
    "                    # do integration without error estimation\n",
    "                    srcfuncs = np.array(srcfuncs)\n",
    "                    srcfuncs_err = None # ask to re-calc below\n",
    "                    if use_new_algo:\n",
    "                        ans   = integrate_along_rays_gridxy(\n",
    "                            sdf, srcfuncs, srcfuncs_err, rays, ray_areas,\n",
    "                            xyzs_names_list=xyzs_names_list, parallel=True, verbose=verbose,\n",
    "                        )\n",
    "                        lum, lum_err, rads, areas_p, taus, inds, contr, pts_order_used, jfact_used, estis = ans\n",
    "                    else:\n",
    "                        ans   = integrate_along_ray_gridxy_old(\n",
    "                            sdf, srcfuncs, srcfuncs_err, rays, ray_areas,\n",
    "                            xyzs_names_list=xyzs_names_list, parallel=True, verbose=verbose,\n",
    "                        )\n",
    "                        lum, lum_err, rads, areas_p, taus, inds, contr, pts_order_used, jfact_used = ans\n",
    "\n",
    "                    \n",
    "                    # record time used\n",
    "                    python_time_ended = now()\n",
    "                    python_time__used  = python_time_ended - python_time_start\n",
    "                    print(f\"In Progress: {python_time_ended.isoformat()}\\nTime Used: {python_time__used}\\n\")\n",
    "\n",
    "                    \n",
    "                    lum     = set_as_quantity(lum,     mpdf.units['lum']).to(units.Lsun)\n",
    "                    lum_err = set_as_quantity(lum_err, mpdf.units['lum']).to(units.Lsun)\n",
    "                    print(f\"\\n{job_nickname}_{file_index:05d}_{xyzs}:\\n\\n\\t{lum = :.3f}, {lum_err = :.3f}\\n\")\n",
    "                    print(f\"Time = {mpdf.get_time(unit=units.yr):.2f}\\n\")\n",
    "\n",
    "                    \n",
    "                    # SEDs\n",
    "                    Ts      = set_as_quantity(sdf['T'].iloc[pts_order_used], mpdf.units['temp']).cgs\n",
    "                    Aeffjs  = set_as_quantity(jfact_used, mpdf.units['dist']**2).cgs\n",
    "                    L_wavs  = L_wav_nb(wavlens.cgs.value, Ts.cgs.value, Aeffjs.cgs.value)\n",
    "                    L_wavs *= units.erg / units.s / units.cm\n",
    "                    L_wavs  = L_wavs.to(units.Lsun / units.angstrom)\n",
    "                    \n",
    "                    L_int = np.trapezoid(L_wavs, wavlens).to(units.Lsun)\n",
    "                    print(f\"{L_int = }\\n{(L_int/lum-1.).to(units.percent) = }\\n\")\n",
    "                    print()\n",
    "\n",
    "\n",
    "                    rads  = (rads * mpdf.units['sigma_sb'] * mpdf.units['temp']**4 / units.sr).cgs\n",
    "                    inds *= units.dimensionless_unscaled\n",
    "                    contr = 100 * contr * units.percent\n",
    "                    lum1  = ((4 * pi * units.sr) * (rads * areas_u)).sum().to(units.solLum)\n",
    "                    print(    f\"Lum err       : {lum_err = :12.2f}\" +\n",
    "                          f\"    (rel err  = {(lum_err / lum1).to(units.percent): 6.2f})\")\n",
    "                    print(    f\"Lum           : {lum1    = :12.2f}\")\n",
    "                    if use_new_algo:\n",
    "                        estis  = (estis * mpdf.units['sigma_sb'] * mpdf.units['temp']**4 / units.sr).cgs\n",
    "                        lume = ((4 * pi * units.sr) * (estis * areas_u)).sum().to(units.solLum)\n",
    "                        print(f\"Old estimation: {lume    = :12.2f}\" +\n",
    "                              f\"    (rel diff = {(2 * (lum1 - lume) / (lum1 + lume)).to(units.percent):+6.2f})\")\n",
    "                    print()\n",
    "\n",
    "                    Aeffis= areas_p * areas_u\n",
    "                    area  = Aeffis.sum()\n",
    "                    area_2= (np.where(\n",
    "                        np.isnan(taus),\n",
    "                        1.,\n",
    "                        np.where(\n",
    "                            taus > PHOTOSPHERE_TAU,\n",
    "                            1.0,\n",
    "                            0.0,\n",
    "                        )) * areas_u).sum()\n",
    "                    #anses_fft = fft.fft2(rads.reshape(no_xy).value)\n",
    "\n",
    "                    try:\n",
    "                        mask = np.logical_and(~np.isnan(contr), contr.value)\n",
    "                        N_res = np.average(\n",
    "                            1. / contr[mask], weights=(rads*areas_u)[mask].value,\n",
    "                        ).to(units.dimensionless_unscaled)\n",
    "                    except ZeroDivisionError:\n",
    "                        N_res = 0.\n",
    "    \n",
    "                    if is_verbose(verbose, 'info'):\n",
    "                        say('info', 'main()', verbose,\n",
    "                            f\"lum = {lum}\",\n",
    "                            f\"area (from <1>) = ({area  /areas_u.sum()*100: 5.1f}%) {area}\",\n",
    "                            f\"area (from tau) = ({area_2/areas_u.sum()*100: 5.1f}%) {area_2}\",\n",
    "                            f\"size (from <1>) = {area**0.5}\",\n",
    "                            f\"size (from tau) = {area_2**0.5}\",\n",
    "                            f\"total possible area = {areas_u.sum()}\",\n",
    "                            f\"lower bound of the # of particles at photosphere, weighted avg over lum per pixels = {N_res} \",\n",
    "                        )\n",
    "                \n",
    "                    # save interm data\n",
    "                    data = {}\n",
    "                    data['lum'  ] = lum\n",
    "                    data['lum_err' ] = lum_err\n",
    "                    data['area_one'] = area\n",
    "                    data['area_tau'] = area_2\n",
    "                    data['N_res'] = N_res\n",
    "                    data['xyzs' ] = xyzs\n",
    "                    data['time' ] = mpdf.get_time()\n",
    "                    data['mpdf_params'] = mpdf.params\n",
    "                    data['rays' ] = rays_u[:, 0, :2]\n",
    "                    data['ray_unit_vec'] = get_ray_unit_vec(rays_u[0].value)\n",
    "                    data['area_per_ray'] = areas_u[0] #areas_u\n",
    "                    data['rads' ] = rads\n",
    "                    data['contr'] = contr\n",
    "                    data['wavlens'] = wavlens\n",
    "                    data['L_wavs'] = L_wavs\n",
    "                    # data['Aeffis'] = Aeffis\n",
    "                    data['Aeffjs'] = Aeffjs\n",
    "                    \n",
    "                    data['_meta_'] = {\n",
    "                        'N_res': comb[job_nickname][xyzs]['_meta_']['N_res'],\n",
    "                        'rays' : { 'Description': \"Pixel centers on the 2D plane defined by xyzs.\", },\n",
    "                        'rads' : { 'Description': \"Specific intensity per pixel.\", },\n",
    "                        'contr': {\n",
    "                            'Description': \"Maximum contributed particle's contribution towards the specific intensity, per pixel.\", },\n",
    "                    }\n",
    "    \n",
    "                    \n",
    "                    mupl.hdf5_dump(data, mupl.hdf5_subgroup(out_interm_grp1, xyzs, overwrite=True), {})\n",
    "        \n",
    "                    comb[job_nickname][xyzs]['times'][ifile] = data['time']\n",
    "                    comb[job_nickname][xyzs]['lums' ][ifile] = data['lum' ]\n",
    "                    comb[job_nickname][xyzs]['lums_err'][ifile] = data['lum_err']\n",
    "                    comb[job_nickname][xyzs]['areas'][ifile] = data['area_one']\n",
    "                    comb[job_nickname][xyzs]['N_res'][ifile] = data['N_res']\n",
    "                    comb[job_nickname][xyzs]['L_wavs'][ifile] = data['L_wavs']\n",
    "        \n",
    "                    \n",
    "                    # plotting\n",
    "                    if False:\n",
    "                        #save_label_addon = '-new' if use_new_algo else '-old'\n",
    "                        save_label_addon=''\n",
    "                        plt.close('all')\n",
    "                        fig, ax, outfilenames = plot_imshow(\n",
    "                            no_xy, rays_u, rads, data_label=\"$I$\",\n",
    "                            xyzs=xyzs, save_label=f\"image{save_label_addon}\",\n",
    "                            job_profile=job_profile, file_index=file_index, cmap='inferno', notes=data,\n",
    "                            output_dir=output_dir, verbose=verbose_loop)\n",
    "                        fig, ax, outfilenames = plot_imshow(\n",
    "                            no_xy, rays_u, inds%20, data_label=\"index % 20 of the most contributed\",\n",
    "                            xyzs=xyzs, save_label=f\"dinds{save_label_addon}\",\n",
    "                            job_profile=job_profile, file_index=file_index, cmap='turbo', notes=data,\n",
    "                            output_dir=output_dir, verbose=verbose_loop)\n",
    "                        fig, ax, outfilenames = plot_imshow(\n",
    "                            no_xy, rays_u, contr, data_label=\"contribution fraction of the most contributed\",\n",
    "                            xyzs=xyzs, save_label=f\"contr{save_label_addon}\",\n",
    "                            job_profile=job_profile, file_index=file_index, cmap='seismic', notes=data,\n",
    "                            output_dir=output_dir, verbose=verbose_loop)\n",
    "                        fig, ax, outfilenames = plot_imshow(\n",
    "                            no_xy, rays_u, areas_p, data_label=\"$<1>$\",\n",
    "                            xyzs=xyzs, save_label=f\"pones{save_label_addon}\",\n",
    "                            job_profile=job_profile, file_index=file_index, notes=data,\n",
    "                            norm=plt.Normalize(0., 1.),\n",
    "                            output_dir=output_dir, verbose=verbose_loop)\n",
    "                        #fig, ax, outfilenames = plot_imshow(\n",
    "                        #    no_xy, rays_u, np.abs(anses_fft), data_label=\"FFt of $I$\", xyzs=xyzs, save_label=f\"I-fft\",\n",
    "                        #    norm=mpl.colors.LogNorm(),\n",
    "                        #    job_profile=job_profile, file_index=file_index, notes=data, output_dir=output_dir, verbose=verbose_loop)\n",
    "\n",
    "\n",
    "                    # debug\n",
    "                    print()\n",
    "                    print(f\"{int(rads.size/2)-1 = }\\n{rads[int(rads.size/2)-1].cgs = }\")\n",
    "                    inds_active = np.logical_or(taus > PHOTOSPHERE_TAU, np.isnan(taus))\n",
    "                    print(f\"{np.count_nonzero(inds_active) / taus.size * 100} % rays hit photosphere\")\n",
    "                    print(f\"{np.std(rads[inds_active]) / np.average(rads[inds_active]) = }\")\n",
    "                    inds = np.logical_or(taus > PHOTOSPHERE_TAU, np.isnan(taus))\n",
    "                    lum_in_ph = (4*pi*units.sr*(rads[inds] * areas_u[inds]).sum()).to(units.Lsun)\n",
    "                    print(f\"{lum       = :.2f}\\n{lum_in_ph = :.2f}    ({(lum_in_ph / lum).to(units.percent):.2f})\")\n",
    "\n",
    "                    \n",
    "                    # # plotting - spec in wavlen space\n",
    "                    # spec_dist = 10 * units.parsec\n",
    "                    # fig, ax = plt.subplots(figsize=(10, 8))\n",
    "                    # y = (L_wavs/(4*pi*spec_dist**2)).to((units.erg / units.s / units.cm**2) / units.angstrom)\n",
    "                    # x = wavlens.to(units.angstrom)\n",
    "                    # ax.loglog(x, y)\n",
    "                    # ax.set_title(f\"SED (viewed at {spec_dist:.1f} with gray opacity)\\n{job_profile['plot_title_suffix']}\")\n",
    "                    # ax.set_xlabel(f\"$\\\\lambda$ / {x.unit.to_string('latex_inline')}\")\n",
    "                    # ax.set_ylabel(f\"$f_{{\\\\lambda}}$ / {y.unit.to_string('latex_inline')}\")\n",
    "                    # ax.set_xlim(5e2, 1e6)\n",
    "                    # ax.set_ylim(1e-9, 1e-3)\n",
    "                    # ax.text(\n",
    "                    #     0.98, 0.02,\n",
    "                    #     f\"Time = {mpdf.get_time():.1f}\\n\" + \\\n",
    "                    #     f\" $L$ = {lum.value:.0f} {lum.unit.to_string('latex_inline')}\",\n",
    "                    #     #color = \"black\",\n",
    "                    #     ha = 'right', va = 'bottom',\n",
    "                    #     transform=ax.transAxes,\n",
    "                    # )\n",
    "                    # fig.savefig(f\"{output_dir}Spec_{job_nickname}_{file_index:05d}_{no_xy_txt}.png\")\n",
    "        \n",
    "                    # record time used\n",
    "                    python_time_ended = now()\n",
    "                    python_time__used  = python_time_ended - python_time_start\n",
    "                    print(f\"Ended: {python_time_ended.isoformat()}\\nTime Used: {python_time__used}\\n\")\n",
    "\n",
    "\n",
    "        # save data for now\n",
    "        with open(f\"{interm_dir}lcgen.{no_xy_txt}.json\", 'w') as f:\n",
    "            mupl.json_dump(comb, f, metadata)\n",
    "\n",
    "        \n",
    "        # #plotting\n",
    "        # plt.close('all')\n",
    "        # fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        # for xyzs in xyzs_list:\n",
    "        #     ax.semilogy(\n",
    "        #         comb[job_nickname][xyzs]['times'].to_value(units.yr), comb[job_nickname][xyzs]['lums'].to_value(units.Lsun),\n",
    "        #         'o--', label=f\"Viewed from +{xyzs[2]}\")\n",
    "        # ax.legend()\n",
    "        # ax.set_xlabel('Time / yr')\n",
    "        # ax.set_ylabel('Luminosity / Lsun')\n",
    "        # ax.set_xlim(0., 45.)\n",
    "        # ax.set_ylim(1e4, 5e6)\n",
    "        # outfilename_noext = f\"{output_dir}LC_{job_nickname}_{no_xy_txt}\"\n",
    "\n",
    "        # if False:\n",
    "        #     # write pdf\n",
    "        #     outfilename = f\"{outfilename_noext}.pdf\"\n",
    "        #     fig.savefig(outfilename)\n",
    "        #     if is_verbose(verbose, 'note'):\n",
    "        #         say('note', None, verbose, f\"Fig saved to {outfilename}.\")\n",
    "        \n",
    "        # # write png (with plot title)\n",
    "        # ax.set_title(f\"Light curve ({job_nickname}, {no_xy_txt} rays)\")\n",
    "        # outfilename = f\"{outfilename_noext}.png\"\n",
    "        # fig.savefig(outfilename)\n",
    "        # if is_verbose(verbose, 'note'):\n",
    "        #     say('note', None, verbose, f\"Fig saved to {outfilename}.\")\n",
    "                \n",
    "    plt.close('all')\n",
    "    mupl.hdf5_dump(comb, f\"{interm_dir}lcgen.{no_xy_txt}.hdf5.gz\", metadata)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
