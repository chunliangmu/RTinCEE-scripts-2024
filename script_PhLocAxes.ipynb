{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3cda03b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Scripts for analyzing of phantom outputs.\\n\\nThis script writes json files for each dump (and one json file synthsizing all outputs)\\n    to plot photosphere size vs time or orbital separation.\\nIt does so by plotting photosphere intersection with traced rays originating from the primary star\\n    and shooting along the axes of the coordination frame.\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Scripts for analyzing of phantom outputs.\n",
    "\n",
    "This script writes json files for each dump (and one json file synthsizing all outputs)\n",
    "    to plot photosphere size vs time or orbital separation.\n",
    "It does so by plotting photosphere intersection with traced rays originating from the primary star\n",
    "    and shooting along the axes of the coordination frame.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b30fd6f",
   "metadata": {},
   "source": [
    "## Imports & Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20d3ef62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "import math\n",
    "import numpy as np\n",
    "from numpy import pi\n",
    "#import pandas\n",
    "from astropy import units\n",
    "from astropy import constants as const\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "#from moviepy.editor import ImageSequenceClip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d110bbc5-fb24-495d-b62a-33bf01b9a5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No ffmpeg exe could be found. Install ffmpeg on your system, or set the IMAGEIO_FFMPEG_EXE environment variable.\n"
     ]
    }
   ],
   "source": [
    "# import modules listed in ./lib/\n",
    "\n",
    "import clmuphantomlib as mupl\n",
    "from clmuphantomlib import get_col_kernel_funcs\n",
    "from clmuphantomlib.io import json_load, json_dump\n",
    "from clmuphantomlib.settings import DEFAULT_SETTINGS as settings\n",
    "from clmuphantomlib.log import error, warn, note, debug_info\n",
    "from clmuphantomlib.log import is_verbose, say\n",
    "from clmuphantomlib.units_util import set_as_quantity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea6adf1-b024-4f90-a2b2-d7bb5d154442",
   "metadata": {},
   "source": [
    "    ## import modules in arbitrary directory\n",
    "    \n",
    "    #import sys\n",
    "    \n",
    "    ## path to my python module lib directory\n",
    "    ## *** CHECK THIS! *** #\n",
    "    #SRC_LIB_PATH = sys.path[0] + '/lib'\n",
    "    \n",
    "    #if SRC_LIB_PATH not in sys.path:\n",
    "    #    sys.path.append(SRC_LIB_PATH)\n",
    "    ##print(*sys.path, sep='\\n')    # debug\n",
    "    #print(\n",
    "    #    \"\\n*   Please Make sure my module files are located in this directory (or change the SRC_LIB_PATH variable):\",\n",
    "    #    f\"\\n{SRC_LIB_PATH = }\\n\"\n",
    "    #)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "517d7e50-25fe-4750-8c5b-3e94519c1842",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# parallels & optimizations\n",
    "\n",
    "\n",
    "#import os\n",
    "## Fixing stupid numba killing kernel\n",
    "## See here https://github.com/numba/numba/issues/3016\n",
    "#os.environ['NUMBA_DISABLE_INTEL_SVML']  = '1'\n",
    "#from numba import njit, prange\n",
    "\n",
    "\n",
    "from multiprocessing import cpu_count, Pool #Process, Queue\n",
    "NPROCESSES = 1 if cpu_count() is None else max(cpu_count(), 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b37d5dde-8b39-4409-9bec-8ae2cabba7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*   Note   :    script:\n",
      "\tWill use 8 processes for parallelization\n"
     ]
    }
   ],
   "source": [
    "# settings\n",
    "#\n",
    "#   imported from script_input.py file\n",
    "\n",
    "from script_PhLocAxes__input import interm_dir, verbose, PHOTOSPHERE_TAU, JOB_PROFILES\n",
    "from _sharedFuncs import mpdf_read\n",
    "\n",
    "\n",
    "# set metadata\n",
    "with open(\"_metadata__input.json\", 'r') as f:\n",
    "    metadata = mupl.json_load(f)\n",
    "metadata['Title'] = \"Getting photosphere size on x, y, z axes\"\n",
    "metadata['Description'] = f\"\"\"Tracing 6 rays on +x, -x, +y, -y, +z, -z directon and get photosphere size, h, rho, u, T from them.\"\"\"\n",
    "\n",
    "\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "\n",
    "# print debug info\n",
    "if __name__ == '__main__' and is_verbose(verbose, 'note'):\n",
    "    # remember to check if name is '__main__' if you wanna say anything\n",
    "    #    so when you do multiprocessing the program doesn't freak out\n",
    "    say('note', \"script\", verbose, f\"Will use {NPROCESSES} processes for parallelization\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ae56404-ba6d-422e-929c-84537ad1f8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  import (my libs)\n",
    "from clmuphantomlib.log import say, is_verbose\n",
    "from clmuphantomlib.geometry import get_dist2_between_2pt, get_closest_pt_on_line, get_dist2_from_pt_to_line_nb, get_ray_unit_vec, get_rays_unit_vec\n",
    "from clmuphantomlib.sph_interp import get_sph_interp, get_h_from_rho, get_no_neigh\n",
    "from clmuphantomlib.units_util import set_as_quantity, set_as_quantity_temperature, get_units_field_name\n",
    "from clmuphantomlib.eos.base import EoS_Base\n",
    "#  import (general)\n",
    "import numpy as np\n",
    "from numpy import typing as npt\n",
    "import numba\n",
    "from numba import jit, prange\n",
    "import sarracen\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e63b52a2-8cf5-47d4-94c8-b424f911c419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_photosphere_on_ray(\n",
    "    pts_on_ray            : np.ndarray,\n",
    "    dtaus                 : None|np.ndarray,\n",
    "    pts_order             : np.ndarray,\n",
    "    sdf                   : sarracen.SarracenDataFrame,\n",
    "    ray                   : np.ndarray,\n",
    "    calc_params           : list       = ['loc', 'R1'],\n",
    "    hfact                 : float      = None,\n",
    "    mpart                 : float      = None,\n",
    "    eos                   : EoS_Base   = None,\n",
    "    sdf_units             : dict       = None,\n",
    "    ray_unit_vec          : np.ndarray = None,\n",
    "    kernel                : sarracen.kernels.base_kernel = None,\n",
    "    do_skip_zero_dtau_pts : bool       = True,\n",
    "    photosphere_tau       : float      = 1.,\n",
    "    return_as_quantity    : bool|None  = True,\n",
    "    verbose : int = 3,\n",
    ") -> tuple[dict, tuple[np.ndarray, np.ndarray, np.ndarray]]:\n",
    "    \"\"\"Calc the location where the photosphere intersect with the ray.\n",
    "\n",
    "    Assuming 3D.\n",
    "\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pts_on_ray, dtaus, pts_order\n",
    "        output from get_optical_depth().\n",
    "\n",
    "        pts_on_ray: np.ndarray\n",
    "            Orthogonal projections of the particles' locations onto the ray.\n",
    "        \n",
    "        dtaus: np.ndarray\n",
    "            Optical depth tau contributed by each particles. In order of the original particles order in the dump file.\n",
    "            Remember tau is a dimensionless quantity.\n",
    "\n",
    "            *** If None, will re-calc tau with the updated algorithm (2025-05-27) ***\n",
    "        \n",
    "        pts_order: np.ndarray\n",
    "            indices of the particles where dtaus are non-zero.\n",
    "            The indices are arranged by distances to the observer, i.e. the particles closest to the observer comes first, \n",
    "            and the furtherest comes last.\n",
    "\n",
    "    sdf: sarracen.SarracenDataFrame\n",
    "        Must contain columns: x, y, z, h    # kappa, rho,\n",
    "        \n",
    "    ray: (2, 3)-shaped numpy array, i.e. [pt1, pt2]\n",
    "        2 points required to determine a line.\n",
    "        The line is described as X(t) = pt1 + t*(pt2-pt1)\n",
    "        First  point pt1 is the reference of the distance if R1 is calc-ed.\n",
    "        Second point pt2 points to the observer, and is closer to the observer.\n",
    "\n",
    "    calc_params: list or tuple of str\n",
    "        parameters to be calculated / interpolated at the photosphere location.\n",
    "        Results will be put into the photosphere dict in the output.\n",
    "        Acceptable input: (Note: will always calc 'loc' if calc_params is not empty)\n",
    "            'is_found': will return bool.\n",
    "                Will always be outputted regardless of in calc_params or not.\n",
    "            'loc': will return (3,)-shaped numpy array.\n",
    "                photophere location.\n",
    "            'R1' : will return float.\n",
    "                distance between photosphere location and the ray[0].\n",
    "                Could be negative if loc is on the other side of the ray.\n",
    "            'nneigh': will return int.\n",
    "                Number of neighbour particles of the photosphere loc.\n",
    "            'rho': will return float.\n",
    "                density at the photosphere.\n",
    "            'u': will return float.\n",
    "                specific internel energy at the photosphere.\n",
    "            'h'  : will return float.\n",
    "                smoothing length at the photosphere.\n",
    "                Will always calc 'rho' if to calc 'h'.\n",
    "            'T'  : will return float.\n",
    "                Temperature at the photosphere.\n",
    "                Warning: if not supplied 'temp' keyword in sdf_units, will return in cgs units.\n",
    "    \n",
    "    hfact, mpart: float\n",
    "        Only useful if you are calc-ing 'h'\n",
    "        $h_\\\\mathrm{fact}$ and particle mass used in the phantom sim.\n",
    "        If None, will get from sdf.params['hfact'] and sdf.params['mass']\n",
    "\n",
    "    eos: .eos.base.EoS_BASE\n",
    "        Only useful if you are calc-ing 'T'\n",
    "        Equation of state object defined in eos/base.py\n",
    "\n",
    "    sdf_units: dict\n",
    "        Only useful if you are calc-ing 'T'\n",
    "        in which case, supply rho, u, and T units in this dict\n",
    "        e.g.\n",
    "        sdf_units = {\n",
    "            'density': units.Msun / units.Rsun**3,\n",
    "            'specificEnergy': units.Rsun**2 / units.s**2,\n",
    "            'temp': units.K,\n",
    "        }\n",
    "    \n",
    "    ray_unit_vec: (3,)-shaped np.ndarray\n",
    "        unit vector of ray. will calc this if not supplied.\n",
    "        \n",
    "    kernel: sarracen.kernels.base_kernel\n",
    "        Smoothing kernel for SPH data interpolation.\n",
    "        If None, will use the one in sdf.\n",
    "        \n",
    "    do_skip_zero_dtau_pts: bool\n",
    "        Whether or not to skip particles with zero dtaus (i.e. no contribution to opacity) to save computational time.\n",
    "        If skiped, these particles' locs will be excluded from results as well\n",
    "        \n",
    "    photosphere_tau: float\n",
    "        At what optical depth (tau) is the photosphere defined.\n",
    "\n",
    "    return_as_quantity: bool | None\n",
    "        If True or None, the results in photosphere will be returned as a astropy.units.Quantity according to sdf_units.\n",
    "        (pts_waypts, pts_waypts_t, taus_waypts) will also be returned as numpy array and NOT as Quantity.\n",
    "        The diff between True and None is that True will raise an error if units not supplied in sdf_units,\n",
    "        while None will just return as numpy array in such case.\n",
    "        \n",
    "    \n",
    "    verbose: int\n",
    "        How much warnings, notes, and debug info to be print on screen. \n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    photosphere, (pts_waypts, pts_waypts_t, taus_waypts)\n",
    "\n",
    "    photosphere: dict\n",
    "        dict of values found at the photosphere intersection point with the ray.\n",
    "        will always have \n",
    "\n",
    "    pts_waypts: (npart, 3)-shaped numpy array\n",
    "        location of the waypoints on ray\n",
    "\n",
    "    pts_waypts_t: (npart)-shaped numpy array\n",
    "        distance of the waypoints from ray[0]\n",
    "\n",
    "    taus_waypts: (npart)-shaped numpy array\n",
    "        optical depth at the waypoints.\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    # init\n",
    "    ray = np.array(ray)\n",
    "    if ray_unit_vec is None:\n",
    "        ray_unit_vec = get_ray_unit_vec(ray)\n",
    "    if kernel is None:\n",
    "        kernel = sdf.kernel\n",
    "    kernel_col, kernel_csz, _, _ = get_col_kernel_funcs(kernel)\n",
    "    if do_skip_zero_dtau_pts and dtaus is not None:\n",
    "        pts_order = pts_order[np.where(dtaus[pts_order])]\n",
    "    ray_0 = np.asarray(ray[0])\n",
    "    pts_ordered    = np.asarray(sdf[['x', 'y', 'z']].iloc[pts_order])\n",
    "    hs_ordered     = np.asarray(sdf[ 'h'           ].iloc[pts_order])\n",
    "    #kappas_ordered = np.array(sdf[ 'kappa'       ].iloc[pts_order])\n",
    "    #rhos_ordered   = np.array(sdf[ 'rho'         ].iloc[pts_order])\n",
    "    pts_on_ray_ordered = pts_on_ray[pts_order]\n",
    "    npart_ordered = pts_ordered.shape[0]\n",
    "\n",
    "\n",
    "    \n",
    "    # get waypts (way points) for pts (point locations) and taus (optical depths)\n",
    "    #  waypts are suitable for linear interpolation\n",
    "    #  taus_waypts[0] is 0; taus_waypts[-1] is total optical depth from the object\n",
    "\n",
    "    \n",
    "    #  step 1: determine the waypts location by assuming pts as balls with constant kappa and density\n",
    "    \n",
    "    #   step 1a: getting the size of pts balls on the ray\n",
    "    pts_dist2_to_ray = get_dist2_between_2pt(pts_ordered, pts_on_ray_ordered)\n",
    "    #    Assuming a h radius ball\n",
    "    pts_radius = kernel.get_radius() * hs_ordered\n",
    "    pts_size_on_ray = pts_radius**2 - pts_dist2_to_ray\n",
    "    # put a small number (1e-8*h) in case of negative pts_size_on_ray, so that the code does not freak out\n",
    "    pts_size_on_ray_min = 1e-8*hs_ordered\n",
    "    pts_size_on_ray = np.where(pts_size_on_ray < pts_size_on_ray_min**2, pts_size_on_ray_min, pts_size_on_ray**0.5)\n",
    "    #pts_size_on_ray = dtaus_ordered / (kappas_ordered * rhos_ordered)    # does not work because rho is not a constant within the particle\n",
    "\n",
    "    #   step 1b: getting the waypoint locs\n",
    "    pts_on_ray_t_ordered = np.sum((pts_on_ray_ordered - ray_0) * ray_unit_vec, axis=-1)\n",
    "    #    pts_waypts_t: the distance from waypts to ray_0 (negative if in the opposite direction)\n",
    "    pts_waypts_t = np.interp(    # 5 data points in between pts, so npart_ordered*5 - 1\n",
    "        np.linspace(0, npart_ordered-1, (npart_ordered-1)*5 + 1),\n",
    "        np.linspace(0, npart_ordered-1, npart_ordered),\n",
    "        pts_on_ray_t_ordered)\n",
    "    pts_waypts_t = np.concatenate((    # add before and after the first particles\n",
    "        np.linspace(pts_on_ray_t_ordered[0] + hs_ordered[0], pts_on_ray_t_ordered[0], 10),\n",
    "        pts_waypts_t,\n",
    "        np.linspace(pts_on_ray_t_ordered[-1] - hs_ordered[-1], pts_on_ray_t_ordered[-1], 10),\n",
    "    ))\n",
    "    pts_waypts = ray_0 + pts_waypts_t[:, np.newaxis] * ray_unit_vec[np.newaxis, :]\n",
    "\n",
    "    #   step 1c: sort waypoint locs\n",
    "    #    sorting should not be necessary, but just in case\n",
    "    # pts_waypts_t_left  = pts_waypts_t[0::2]\n",
    "    # pts_waypts_t_right = pts_waypts_t[1::2]\n",
    "    pts_waypts_inds = np.argsort(pts_waypts_t)[::-1]\n",
    "    pts_waypts   = pts_waypts[  pts_waypts_inds]\n",
    "    pts_waypts_t = pts_waypts_t[pts_waypts_inds]\n",
    "    \n",
    "    #  step 2: determine the waypts optical depth\n",
    "    taus_waypts = np.zeros(pts_waypts.shape[0])\n",
    "    if dtaus is None:\n",
    "        # re-calc\n",
    "        mkappa_div_h2_ordered = np.asarray(sdf['m'].iloc[pts_order] * sdf['kappa'].iloc[pts_order] / hs_ordered**2)\n",
    "        for j in range(npart_ordered):\n",
    "            h = hs_ordered[j]\n",
    "            q_xy_j = pts_dist2_to_ray[j]**0.5 / h\n",
    "            t_j = pts_waypts_t[j]\n",
    "            taus_waypts += mkappa_div_h2_k * kernel_csz(q_xy_j, -(pts_waypts_t - t_j)/h, ndim)\n",
    "    else:\n",
    "        # interpolate from given dtau (not the same as in LCGen)\n",
    "        for waypt_t, h, dtau in zip(pts_waypts_t, hs_ordered, dtaus[pts_order]):\n",
    "            hr = h * kernel.get_radius()\n",
    "            # Note: np.interp assumes xp increasing, so we need to reverse this\n",
    "            taus_waypts += np.interp(pts_waypts_t[::-1], [waypt_t-hr, waypt_t+hr], [dtau, 0.], left=dtau, right=0.)[::-1]\n",
    "        \n",
    "\n",
    "    # prepare answers\n",
    "    # is found?\n",
    "    if not taus_waypts.size:\n",
    "        taus_max = 0\n",
    "    elif np.isfinite(taus_waypts[-1]):\n",
    "        # in case there is nan in the later part of the array\n",
    "        taus_max = taus_waypts[-1]\n",
    "    else:\n",
    "        taus_max = np.nanmax(taus_waypts)\n",
    "    photosphere = {\n",
    "        'is_found': (taus_max > photosphere_tau)\n",
    "    }\n",
    "    \n",
    "    # get photosphere parameters\n",
    "    if calc_params:\n",
    "        # always calc location if anything needs to be calc-ed\n",
    "        photosphere['loc'] = np.array([\n",
    "            np.interp(photosphere_tau, taus_waypts, pts_waypts[:, ax], right=np.nan) if taus_waypts.size else np.nan\n",
    "            for ax in range(pts_waypts.shape[1])\n",
    "        ])\n",
    "\n",
    "        # do prerequisite check\n",
    "        calc_params = list(calc_params)\n",
    "        if 'h' in calc_params:\n",
    "            if 'rho' not in calc_params: calc_params.append('rho')\n",
    "        if 'T'   in calc_params:\n",
    "            if 'rho' not in calc_params: calc_params.append('rho')\n",
    "            if 'u'   not in calc_params: calc_params.append('u')\n",
    "\n",
    "        # first calc prerequisites\n",
    "        calc_these = []\n",
    "        for calc_name in calc_params:\n",
    "            if   calc_name == 'loc':\n",
    "                # already calc-ed\n",
    "                pass\n",
    "            elif calc_name == 'R1':\n",
    "                photosphere['R1']  = np.interp(photosphere_tau, taus_waypts, pts_waypts_t, right=np.nan) if taus_waypts.size else np.nan\n",
    "            elif calc_name in {'rho', 'u'}:\n",
    "                photosphere[calc_name]  = get_sph_interp(sdf, calc_name, photosphere['loc'], kernel=kernel, verbose=verbose)\n",
    "            elif calc_name in {'nneigh'}:\n",
    "                photosphere[calc_name]  = get_no_neigh(sdf, photosphere['loc'], kernel=kernel, verbose=verbose)\n",
    "            else:\n",
    "                calc_these.append(calc_name)\n",
    "    \n",
    "        # now the rest\n",
    "        for calc_name in calc_these:\n",
    "            if calc_name == 'h':\n",
    "                if hfact is None: hfact = sdf.params['hfact']\n",
    "                if mpart is None: mpart = sdf.params['mass']\n",
    "                photosphere['h']  = get_h_from_rho(photosphere['rho'], mpart, hfact)\n",
    "            elif calc_name == 'T':\n",
    "                if eos   is None: raise ValueError(\"get_photosphere_on_ray(): Please supply equation of state to calculate temperature.\")\n",
    "                try:\n",
    "                    photosphere['T']  = eos.get_temp(\n",
    "                        set_as_quantity(photosphere['rho'], sdf_units['density']),\n",
    "                        set_as_quantity(photosphere['u'  ], sdf_units['specificEnergy']))\n",
    "                    if 'temp' in sdf_units:\n",
    "                        photosphere['T'] = set_as_quantity_temperature(photosphere['T'], sdf_units['temp']).value\n",
    "                    else:\n",
    "                        photosphere['T'] = photosphere['T'].value\n",
    "                except ValueError:\n",
    "                    # eos interp could go out of bounds if it's a tabulated EoS\n",
    "                    # which will raise a Value Error\n",
    "                    photosphere['T'] = np.nan\n",
    "            else:\n",
    "                # just interpolate it (#IT JUST WORKS)\n",
    "                photosphere[calc_name]  = get_sph_interp(sdf, calc_name, photosphere['loc'], kernel=kernel, verbose=verbose)\n",
    "\n",
    "        # add units\n",
    "        if return_as_quantity or return_as_quantity is None:\n",
    "            for calc_name in photosphere.keys():\n",
    "                if calc_name not in {'is_found', 'nneigh'}:\n",
    "                    # find appropriate unit\n",
    "                    try:\n",
    "                        unit_field_name = get_units_field_name(calc_name)\n",
    "                    except NotImplementedError:\n",
    "                        # failed to find unit type\n",
    "                        unit_field_name = None\n",
    "                    if unit_field_name in sdf_units.keys():\n",
    "                        # add units\n",
    "                        photosphere[calc_name] = set_as_quantity(photosphere[calc_name], sdf_units[unit_field_name])\n",
    "                    # errors\n",
    "                    elif unit_field_name is None:\n",
    "                        if is_verbose(verbose, 'warn'):\n",
    "                            say('warn', 'get_photosphere_on_ray()', verbose,\n",
    "                                f\"Cannot find the corresponding unit for {calc_name}. Will return as numpy array instead.\")\n",
    "                    elif return_as_quantity is not None:\n",
    "                        raise ValueError(f\"Please supply {unit_field_name} in sdf_units.\")\n",
    "        \n",
    "        \n",
    "    return photosphere, (pts_waypts, pts_waypts_t, taus_waypts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40466d2f",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f063ed6",
   "metadata": {},
   "source": [
    "## Photosphere size vs time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f8132b3-209d-4680-b364-d9d5eb4cb219",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_ph_loc_axes(\n",
    "    job_profile : dict,\n",
    "    #job_name : str,\n",
    "    file_indexes : np.ndarray,\n",
    "    rays_dir_def : dict,    # dict of list\n",
    "    eoses : (mupl.eos.base.EoS_Base, mupl.eos.mesa.EoS_MESA_opacity),\n",
    "    photosphere_tau = PHOTOSPHERE_TAU,\n",
    "    verbose : int = 2,\n",
    "    interp_method: str = 'basic',    # 'basic' or 'improved'\n",
    "):\n",
    "\n",
    "    \"\"\"Writing the photosphere locations of each dump to json files.\n",
    "\n",
    "    Notes:\n",
    "    Using mpdf.params['hfact']\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    #mpdf = mupl.MyPhantomDataFrames()\n",
    "\n",
    "    \n",
    "    job_name = job_profile['job_name']\n",
    "    #X = job_profile['X']\n",
    "    #ieos = job_profile['ieos']\n",
    "\n",
    "    eos, eos_opacity = eoses\n",
    "\n",
    "    \n",
    "    # init rays directions\n",
    "    rays_dir = {}\n",
    "    for key in rays_dir_def.keys():\n",
    "        rays_dir[key] = np.array(rays_dir_def[key])\n",
    "\n",
    "\n",
    "    # main\n",
    "    for file_index in file_indexes:\n",
    "        \n",
    "        # init answer dict / array\n",
    "        photosphere_pars = { # [legend][par_name][time]\n",
    "            'time_yr': None,\n",
    "            'orbsep_Rsun': None,\n",
    "            'mpart_Msun' : None,\n",
    "            'data': {},\n",
    "            'rays_dir': rays_dir_def,\n",
    "            'rays': {},\n",
    "        }  \n",
    "        for key in rays_dir.keys():\n",
    "            photosphere_pars['data'][key] = {}\n",
    "\n",
    "        # read data\n",
    "        mpdf = mpdf_read(job_name, file_index, eos_opacity, mpdf=None, reset_xyz_by='CoM', do_extrap=False, verbose=verbose)\n",
    "        #mpdf.read(job_name, file_index, reset_xyz_by='CoM', verbose=verbose)\n",
    "        #if 'Tdust' in mpdf.data['gas'].columns:\n",
    "        #    mpdf.data['gas']['T'] = mpdf.data['gas']['Tdust']\n",
    "        #elif 'temperature' in mpdf.data['gas'].columns:\n",
    "        #    mpdf.data['gas']['T'] = mpdf.data['gas']['temperature']\n",
    "        #if 'kappa' not in mpdf.data['gas'].keys():\n",
    "        #    # get kappa from mesa table in cgs units\n",
    "        #    mpdf.data['gas']['kappa'] = eos_opacity.get_kappa(\n",
    "        #        mpdf.get_val('rho', copy=False),\n",
    "        #        mpdf.get_val('T', copy=False),\n",
    "        #        do_extrap=True,\n",
    "        #        return_as_quantity=False)\n",
    "        ## translate to phantom units\n",
    "        #mpdf.calc_sdf_params(\n",
    "        #    calc_params=['kappa',], #'R1',\n",
    "        #    calc_params_params={'ieos': ieos, 'X':X, 'overwrite':False, 'kappa_translate_from_cgs_units':True},\n",
    "        #    verbose=verbose,\n",
    "        #)\n",
    "        hfact = mpdf.params['hfact']\n",
    "        mpart = mpdf.params['mass']\n",
    "        \n",
    "        photosphere_pars['time_yr'] = mpdf.get_time().to_value(units.year)\n",
    "        photosphere_pars['orbsep_Rsun'] = mpdf.get_orb_sep().to_value(units.Rsun)\n",
    "        photosphere_pars['mpart_Msun']  = (mpart * mpdf.units['mass']).to_value(units.Msun)\n",
    "\n",
    "\n",
    "        ## construct kdtree (since we are not changing x, y, z label here)\n",
    "        #sdf_all_kdtree = kdtree.KDTree(np.array(sdf[['x', 'y', 'z']], copy=False))\n",
    "\n",
    "        # construct rays_dict\n",
    "        star_loc = np.array([mpdf.data['sink'][axis][0] for axis in 'xyz'])\n",
    "        rays_dict = {}    # legend: ray\n",
    "        for key in rays_dir.keys():\n",
    "            # init\n",
    "            ray = np.array([\n",
    "                star_loc,\n",
    "                star_loc + rays_dir[key],\n",
    "            ])\n",
    "            rays_dict[key] = ray\n",
    "            photosphere_pars['rays'][key] = ray.tolist()\n",
    "            ray_unit_vec = ray[1, :] - ray[0, :]\n",
    "            ray_unit_vec = ray_unit_vec / np.sum(ray_unit_vec**2)**0.5\n",
    "\n",
    "\n",
    "            # optimization- first select only the particles affecting the ray\n",
    "            #  because interpolation of m points with N particles scales with O(N*m),\n",
    "            #  reducing N can speed up calc significantly\n",
    "            sdf = mpdf.data['gas']\n",
    "            kernel_radius = sdf.kernel.get_radius()\n",
    "            hs = np.array(sdf['h'])\n",
    "            pts = np.array(sdf[['x', 'y', 'z']])    # (npart, 3)-shaped array\n",
    "            pts_on_ray = mupl.get_closest_pt_on_line(pts, ray)\n",
    "            sdf_selected_indices = (np.sum((pts - pts_on_ray)**2, axis=-1) <= (kernel_radius * hs)**2)\n",
    "            if verbose:\n",
    "                debug_info(\n",
    "                    'write_ph_loc_axes()', verbose,\n",
    "                    f\"{np.count_nonzero(sdf_selected_indices)} particles are close enough to the ray to have effects.\"\n",
    "                )\n",
    "            sdf = sdf.iloc[sdf_selected_indices]\n",
    "            pts = np.array(sdf[['x', 'y', 'z']])    # (npart, 3)-shaped array\n",
    "\n",
    "\n",
    "            # get optical depth\n",
    "            if verbose:\n",
    "                debug_info(\n",
    "                    'write_ph_loc_axes()', verbose,\n",
    "                    f\"{ray = }\"\n",
    "                )\n",
    "            pts_on_ray, dtaus, pts_order = mupl.light.get_optical_depth_by_ray_tracing_3D(sdf=sdf, ray=ray)\n",
    "            photosphere, (pts_waypts, pts_waypts_t, taus_waypts) = get_photosphere_on_ray(\n",
    "                pts_on_ray, None, pts_order, sdf, ray,    # remove dtaus to force recalc in LCGen way\n",
    "                calc_params = ['loc', 'R1', 'rho', 'u', 'h', 'T', 'kappa'],\n",
    "                hfact = hfact, mpart=mpart, eos=eos, sdf_units=mpdf.units,\n",
    "                ray_unit_vec=ray_unit_vec, verbose=verbose, photosphere_tau=photosphere_tau,\n",
    "                return_as_quantity=False,\n",
    "            )\n",
    "            photosphere_pars['data'][key] = photosphere\n",
    "            photosphere_pars['data'][key]['size'] = photosphere['R1']\n",
    "            # R1_on_ray  = np.logspace(1, np.log10((pts_waypts_t[0] + pts_waypts_t[1]) / 2), 1000)[::-1]\n",
    "            # R1_on_ray  = np.logspace(1, np.log10(pts_waypts_t[0]*(4095/4096) + pts_waypts_t[1]/4096), 8192)[::-1]\n",
    "            R1_on_ray  = pts_waypts_t[pts_waypts_t > 1]\n",
    "            tau_on_ray = np.interp(R1_on_ray, pts_waypts_t[::-1], taus_waypts[::-1])\n",
    "            pts_on_ray = ray[0][np.newaxis, :] + R1_on_ray[:, np.newaxis] * ray_unit_vec[np.newaxis, :]\n",
    "            rho_on_ray = mupl.sph_interp.get_sph_interp(sdf, 'rho', pts_on_ray, verbose=verbose, method=interp_method)\n",
    "            h_on_ray   = mupl.sph_interp.get_h_from_rho(rho_on_ray, mpart=mpart, hfact=hfact)\n",
    "            photosphere_pars['data'][key][ 'R1_on_ray'] = R1_on_ray\n",
    "            photosphere_pars['data'][key]['tau_on_ray'] = tau_on_ray\n",
    "            photosphere_pars['data'][key]['rho_on_ray'] = rho_on_ray\n",
    "            photosphere_pars['data'][key][  'h_on_ray'] = h_on_ray\n",
    "            photosphere_pars['data'][key][  'u_on_ray'] = mupl.sph_interp.get_sph_interp(\n",
    "                sdf, 'u'  , pts_on_ray, verbose=verbose, method=interp_method)\n",
    "            photosphere_pars['data'][key][  'T_on_ray'] = eos.get_temp(\n",
    "                set_as_quantity(photosphere_pars['data'][key]['rho_on_ray'], mpdf.units['density']),\n",
    "                set_as_quantity(photosphere_pars['data'][key]['u_on_ray']  , mpdf.units['specificEnergy']),\n",
    "                return_as_quantity=False, bounds_error=False)\n",
    "            photosphere_pars['data'][key]['kappa_on_ray']=mupl.sph_interp.get_sph_interp(\n",
    "                sdf,'kappa', pts_on_ray, verbose=verbose, method=interp_method)\n",
    "            if 'kappa_dust' in sdf:\n",
    "                photosphere_pars['data'][key]['kappaDust_on_ray']=mupl.sph_interp.get_sph_interp(\n",
    "                    sdf,'kappa_dust', pts_on_ray, verbose=verbose, method=interp_method)\n",
    "            photosphere_pars['data'][key]['nneigh_on_ray']=mupl.sph_interp.get_no_neigh(\n",
    "                sdf, pts_on_ray, hs_at_locs=h_on_ray, kernel_rad=kernel_radius)\n",
    "                \n",
    "            if verbose:\n",
    "                debug_info(    # debug\n",
    "                    'write_ph_loc_axes()', verbose,\n",
    "                    f\"{photosphere_loc = }\\n{photosphere_dist_to_ray0 = }\\n\",\n",
    "                    f\"{photosphere_taus = }\\n\",\n",
    "                    f\"{pts_on_ray_ordered[photosphere_loc_index:photosphere_loc_index+2] = }\",\n",
    "                )\n",
    "\n",
    "        with open(f\"{interm_dir}{job_profile['nickname']}_{file_index:05d}.photospherePars.xyz.json\", 'w') as f:\n",
    "            json_dump(photosphere_pars, f, metadata=metadata, indent=None)\n",
    "            if verbose: print(f\"\\n\\nWritten to {f.name}\\n\")\n",
    "                \n",
    "        del mpdf\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862a0ce8-57ed-4eeb-b09b-218f80982760",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe765991-3271-4c30-a63e-18628d8e272e",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_debug = True\n",
    "if do_debug and __name__ == '__main__':\n",
    "    from script_PhLocAxes__input import JOB_PROFILES_DICT\n",
    "    JOB_PROFILES = [JOB_PROFILES_DICT[key] for key in ('2md',)] #('2md', '4md')]\n",
    "    for job_profile in JOB_PROFILES:\n",
    "        job_profile['file_indexes'] = (0, 400) #(0, 400, 1200, 1600, 2000, 4800, 15600, 17600)\n",
    "    NPROCESSES = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "951dd520-ce47-42ce-9b21-f4f1951e1cff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*   Note   :    write_ph_loc_axes() ==> mpdf_read() ==> read():\n",
      "\t\n",
      "\n",
      "\tReading filename='../raw/luis_2md/light_00000'\n",
      "\n",
      "\n",
      "*   Note   :    mpdf_read() ==> read() ==> reset_xyz_by():\n",
      "\tReseting Origin to CoM ([9.36841334e-13 3.02453466e-14 2.47169186e-14])...\n",
      "*   Note   :    mpdf_read() ==> read() ==> reset_xyz_by():\n",
      "\tCoM location is now [ 0.00000000e+00  3.59257537e-16 -1.80945419e-16]\n",
      "**  Warning:    write_ph_loc_axes() ==> mpdf_read() ==> read():\n",
      "\tkappa column exists.\n",
      "\tWe here assume kappa is in phantom units self.units['opacity']=Unit(\"udist2 / umass\") \n",
      "\tHowever in phantom kappa is assumed to be in cgs unit.\n",
      "\tIf so, please CONVERT KAPPA MANNUALLY into PHANTOM units BEFORE proceeding, e.g.:\n",
      "\t\tmpdf.data['gas']['kappa'] = mupl.units_util.get_val_in_unit(\n",
      "\t\tmpdf.data['gas']['kappa'], units.cm**2/units.g, mpdf.units['opacity'])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (21806,) (3,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 36\u001b[0m\n\u001b[1;32m     29\u001b[0m eos_opacity  \u001b[38;5;241m=\u001b[39m mupl\u001b[38;5;241m.\u001b[39meos\u001b[38;5;241m.\u001b[39mmesa\u001b[38;5;241m.\u001b[39mEoS_MESA_opacity(job_profile[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m], settings)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m NPROCESSES \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     33\u001b[0m     \n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# single process\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m     \u001b[43mwrite_ph_loc_axes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjob_profile\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mjob_profile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_indexes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfile_indexes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrays_dir_def\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrays_dir_def\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43meoses\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43meos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meos_opacity\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphotosphere_tau\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mPHOTOSPHERE_TAU\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m     \n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# multi-process\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m file_index \u001b[38;5;129;01min\u001b[39;00m file_indexes:\n",
      "Cell \u001b[0;32mIn[8], line 121\u001b[0m, in \u001b[0;36mwrite_ph_loc_axes\u001b[0;34m(job_profile, file_indexes, rays_dir_def, eoses, photosphere_tau, verbose, interp_method)\u001b[0m\n\u001b[1;32m    116\u001b[0m     debug_info(\n\u001b[1;32m    117\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwrite_ph_loc_axes()\u001b[39m\u001b[38;5;124m'\u001b[39m, verbose,\n\u001b[1;32m    118\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mray\u001b[38;5;250m \u001b[39m\u001b[38;5;132;01m= }\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    119\u001b[0m     )\n\u001b[1;32m    120\u001b[0m pts_on_ray, dtaus, pts_order \u001b[38;5;241m=\u001b[39m mupl\u001b[38;5;241m.\u001b[39mlight\u001b[38;5;241m.\u001b[39mget_optical_depth_by_ray_tracing_3D(sdf\u001b[38;5;241m=\u001b[39msdf, ray\u001b[38;5;241m=\u001b[39mray)\n\u001b[0;32m--> 121\u001b[0m photosphere, (pts_waypts, pts_waypts_t, taus_waypts) \u001b[38;5;241m=\u001b[39m \u001b[43mget_photosphere_on_ray\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpts_on_ray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpts_order\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# remove dtaus to force recalc in LCGen way\u001b[39;49;00m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcalc_params\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mloc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mR1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrho\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mh\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mT\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkappa\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhfact\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mhfact\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmpart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmpart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msdf_units\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmpdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mray_unit_vec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mray_unit_vec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphotosphere_tau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mphotosphere_tau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_as_quantity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m photosphere_pars[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m][key] \u001b[38;5;241m=\u001b[39m photosphere\n\u001b[1;32m    129\u001b[0m photosphere_pars[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m][key][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m photosphere[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mR1\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[0;32mIn[7], line 187\u001b[0m, in \u001b[0;36mget_photosphere_on_ray\u001b[0;34m(pts_on_ray, dtaus, pts_order, sdf, ray, calc_params, hfact, mpart, eos, sdf_units, ray_unit_vec, kernel, do_skip_zero_dtau_pts, photosphere_tau, return_as_quantity, verbose)\u001b[0m\n\u001b[1;32m    178\u001b[0m pts_waypts_t \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39minterp(    \u001b[38;5;66;03m# 5 data points in between pts, so npart_ordered*5 - 1\u001b[39;00m\n\u001b[1;32m    179\u001b[0m     np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m, npart_ordered\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, (npart_ordered\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m5\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m    180\u001b[0m     np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m, npart_ordered\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, npart_ordered),\n\u001b[1;32m    181\u001b[0m     pts_on_ray_t_ordered)\n\u001b[1;32m    182\u001b[0m pts_waypts_t \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((    \u001b[38;5;66;03m# add before and after the first particles\u001b[39;00m\n\u001b[1;32m    183\u001b[0m     np\u001b[38;5;241m.\u001b[39mlinspace(pts_on_ray_t_ordered[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m hs_ordered[\u001b[38;5;241m0\u001b[39m], pts_on_ray_t_ordered[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m10\u001b[39m),\n\u001b[1;32m    184\u001b[0m     pts_waypts_t,\n\u001b[1;32m    185\u001b[0m     np\u001b[38;5;241m.\u001b[39mlinspace(pts_on_ray_t_ordered[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m hs_ordered[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], pts_on_ray_t_ordered[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m10\u001b[39m),\n\u001b[1;32m    186\u001b[0m ))\n\u001b[0;32m--> 187\u001b[0m pts_waypts \u001b[38;5;241m=\u001b[39m ray_0 \u001b[38;5;241m+\u001b[39m \u001b[43mpts_waypts_t\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mray_unit_vec\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;66;03m#   step 1c: sort waypoint locs\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m#    sorting should not be necessary, but just in case\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;66;03m# pts_waypts_t_left  = pts_waypts_t[0::2]\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;66;03m# pts_waypts_t_right = pts_waypts_t[1::2]\u001b[39;00m\n\u001b[1;32m    193\u001b[0m pts_waypts_inds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(pts_waypts_t)[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (21806,) (3,) "
     ]
    }
   ],
   "source": [
    "# main process\n",
    "\n",
    "\n",
    "\n",
    "# init rays directions\n",
    "rays_dir_def = {\n",
    "    # legend: ray direction name\n",
    "    '+x'  : [ 1., 0., 0.],\n",
    "    '+y'  : [ 0., 1., 0.],\n",
    "    '+z'  : [ 0., 0., 1.],\n",
    "    '-x'  : [-1., 0., 0.],\n",
    "    '-y'  : [ 0.,-1., 0.],\n",
    "    '-z'  : [ 0., 0.,-1.],\n",
    "}\n",
    "\n",
    "\n",
    "# run main\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    \n",
    "    # get ph loc for each dump file\n",
    "    args = []\n",
    "    for job_profile in JOB_PROFILES:\n",
    "    \n",
    "        file_indexes = job_profile['file_indexes']\n",
    "        #job_name     = job_profile['job_name']\n",
    "        eos          = mupl.get_eos(job_profile['ieos'], job_profile['params'], settings)\n",
    "        eos_opacity  = mupl.eos.mesa.EoS_MESA_opacity(job_profile['params'], settings)\n",
    "    \n",
    "        \n",
    "        if NPROCESSES <= 1:\n",
    "            \n",
    "            # single process\n",
    "    \n",
    "            write_ph_loc_axes(\n",
    "                job_profile = job_profile, file_indexes = file_indexes, rays_dir_def = rays_dir_def,\n",
    "                eoses = (eos, eos_opacity), photosphere_tau = PHOTOSPHERE_TAU, verbose = verbose,\n",
    "            )\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            # multi-process\n",
    "\n",
    "            for file_index in file_indexes:\n",
    "                args.append((\n",
    "                    job_profile,\n",
    "                    [file_index],\n",
    "                    rays_dir_def,\n",
    "                    (eos, eos_opacity),\n",
    "                    PHOTOSPHERE_TAU,\n",
    "                    0,\n",
    "                ))\n",
    "                \n",
    "            with Pool(processes=NPROCESSES) as pool:\n",
    "                pool.starmap(write_ph_loc_axes, args)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b2c0d7-7d76-4452-b7e5-c5b1a47f9aa3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    # syntheize the files into one big file\n",
    "    \n",
    "    for job_profile in JOB_PROFILES:\n",
    "    \n",
    "        job_name     = job_profile['job_name']\n",
    "        file_indexes = job_profile['file_indexes']\n",
    "    \n",
    "    \n",
    "        # init\n",
    "        photosphere_pars_all = { # [legend][par_name][time]\n",
    "            'time_yr': [],\n",
    "            'orbsep_Rsun': [],\n",
    "            'data': {},\n",
    "            'rays_dir': rays_dir_def,\n",
    "            'rays': {},\n",
    "        }  \n",
    "        for key in rays_dir_def.keys():\n",
    "            photosphere_pars_all['data'][key] = {\n",
    "                'size': [],\n",
    "                'R1'  : [],\n",
    "                'rho' : [],\n",
    "                'u'   : [],\n",
    "                'h'   : [],\n",
    "                'T'   : [],\n",
    "                'R1_on_ray' : [],\n",
    "                'tau_on_ray': [],\n",
    "                'rho_on_ray': [],\n",
    "                'u_on_ray'  : [],\n",
    "                'T_on_ray'  : [],\n",
    "            }\n",
    "            photosphere_pars_all['rays'][key] = []\n",
    "    \n",
    "        \n",
    "        # fetch\n",
    "        for file_index in file_indexes:\n",
    "            with open(f\"{interm_dir}{job_profile['nickname']}_{file_index:05}.photospherePars.xyz.json\", 'r') as f:\n",
    "                \n",
    "                if verbose: print(f\"\\n\\nLoading {f.name}... \", end='')\n",
    "                \n",
    "                photosphere_pars = json_load(f)\n",
    "                for it in ['time_yr', 'orbsep_Rsun']:\n",
    "                    photosphere_pars_all[it].append(photosphere_pars[it])\n",
    "                for key in rays_dir_def.keys():\n",
    "                    for it in photosphere_pars_all['data'][key].keys():\n",
    "                        obj = photosphere_pars['data'][key][it]\n",
    "                        if isinstance(obj, np.ndarray):\n",
    "                            obj = obj.tolist()\n",
    "                        photosphere_pars_all['data'][key][it].append(obj)\n",
    "                    photosphere_pars_all['rays'][key].append(photosphere_pars['rays'][key]) \n",
    "    \n",
    "                if verbose: print(f\"Done.\\n\")\n",
    "\n",
    "        # make numpy array\n",
    "        for ray_dir in rays_dir_def.keys():\n",
    "            d = photosphere_pars_all['data'][ray_dir]\n",
    "            for k in d.keys():\n",
    "                d[k]= np.asanyarray(d[k])\n",
    "                \n",
    "        \n",
    "        # write\n",
    "        with open(f\"{interm_dir}{job_profile['nickname']}.photospherePars.xyz.json\", 'w') as f:\n",
    "            json_dump(photosphere_pars_all, f, metadata=metadata, indent=None)\n",
    "            if verbose: print(f\"\\n\\nWritten to {f.name}.\\n\")\n",
    "\n",
    "\n",
    "    print(\"\\n\\n\\n*** All Done. ***\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b453afc6-0763-41a1-8898-71ad601aab78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
