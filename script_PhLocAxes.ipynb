{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3cda03b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Scripts for analyzing of phantom outputs.\\n\\nThis script writes json files for each dump (and one json file synthsizing all outputs)\\n    to plot photosphere size vs time or orbital separation.\\nIt does so by plotting photosphere intersection with traced rays originating from the primary star\\n    and shooting along the axes of the coordination frame.\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Scripts for analyzing of phantom outputs.\n",
    "\n",
    "This script writes json files for each dump (and one json file synthsizing all outputs)\n",
    "    to plot photosphere size vs time or orbital separation.\n",
    "It does so by plotting photosphere intersection with traced rays originating from the primary star\n",
    "    and shooting along the axes of the coordination frame.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b30fd6f",
   "metadata": {},
   "source": [
    "## Imports & Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20d3ef62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "import math\n",
    "import numpy as np\n",
    "from numpy import pi\n",
    "#import pandas\n",
    "from astropy import units\n",
    "from astropy import constants as const\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "#from moviepy.editor import ImageSequenceClip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d110bbc5-fb24-495d-b62a-33bf01b9a5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules listed in ./lib/\n",
    "\n",
    "import clmuphantomlib as mupl\n",
    "from clmuphantomlib.readwrite import json_load, json_dump\n",
    "from clmuphantomlib.settings import DEFAULT_SETTINGS as settings\n",
    "from clmuphantomlib.log import error, warn, note, debug_info\n",
    "from clmuphantomlib.log import is_verbose, say\n",
    "from clmuphantomlib.units_util import set_as_quantity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea6adf1-b024-4f90-a2b2-d7bb5d154442",
   "metadata": {},
   "source": [
    "    ## import modules in arbitrary directory\n",
    "    \n",
    "    #import sys\n",
    "    \n",
    "    ## path to my python module lib directory\n",
    "    ## *** CHECK THIS! *** #\n",
    "    #SRC_LIB_PATH = sys.path[0] + '/lib'\n",
    "    \n",
    "    #if SRC_LIB_PATH not in sys.path:\n",
    "    #    sys.path.append(SRC_LIB_PATH)\n",
    "    ##print(*sys.path, sep='\\n')    # debug\n",
    "    #print(\n",
    "    #    \"\\n*   Please Make sure my module files are located in this directory (or change the SRC_LIB_PATH variable):\",\n",
    "    #    f\"\\n{SRC_LIB_PATH = }\\n\"\n",
    "    #)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "517d7e50-25fe-4750-8c5b-3e94519c1842",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# parallels & optimizations\n",
    "\n",
    "\n",
    "#import os\n",
    "## Fixing stupid numba killing kernel\n",
    "## See here https://github.com/numba/numba/issues/3016\n",
    "#os.environ['NUMBA_DISABLE_INTEL_SVML']  = '1'\n",
    "#from numba import njit, prange\n",
    "\n",
    "\n",
    "from multiprocessing import cpu_count, Pool #Process, Queue\n",
    "NPROCESSES = 1 if cpu_count() is None else max(cpu_count(), 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b37d5dde-8b39-4409-9bec-8ae2cabba7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*   Note   :    script:\n",
      "\tWill use 8 processes for parallelization\n"
     ]
    }
   ],
   "source": [
    "# settings\n",
    "#\n",
    "#   imported from script_input.py file\n",
    "\n",
    "from script_PhLocAxes__input import verbose, PHOTOSPHERE_TAU, JOB_PROFILES\n",
    "from _sharedFuncs import mpdf_read\n",
    "\n",
    "\n",
    "# set metadata\n",
    "with open(\"_metadata__input.json\", 'r') as f:\n",
    "    metadata = mupl.json_load(f)\n",
    "metadata['Title'] = \"Getting photosphere size on x, y, z axes\"\n",
    "metadata['Description'] = f\"\"\"Tracing 6 rays on +x, -x, +y, -y, +z, -z directon and get photosphere size, h, rho, u, T from them.\"\"\"\n",
    "\n",
    "\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "\n",
    "# print debug info\n",
    "if __name__ == '__main__' and is_verbose(verbose, 'note'):\n",
    "    # remember to check if name is '__main__' if you wanna say anything\n",
    "    #    so when you do multiprocessing the program doesn't freak out\n",
    "    say('note', \"script\", verbose, f\"Will use {NPROCESSES} processes for parallelization\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40466d2f",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f063ed6",
   "metadata": {},
   "source": [
    "## Photosphere size vs time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f8132b3-209d-4680-b364-d9d5eb4cb219",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_ph_loc_axes(\n",
    "    #job_profile : dict,\n",
    "    job_name : str,\n",
    "    file_indexes : np.ndarray,\n",
    "    rays_dir_def : dict,    # dict of list\n",
    "    eoses : (mupl.eos.base.EoS_Base, mupl.eos.mesa.EoS_MESA_opacity),\n",
    "    photosphere_tau = PHOTOSPHERE_TAU,\n",
    "    verbose : int = 2,\n",
    "):\n",
    "\n",
    "    \"\"\"Writing the photosphere locations of each dump to json files.\n",
    "\n",
    "    Notes:\n",
    "    Using mpdf.params['hfact']\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    #mpdf = mupl.MyPhantomDataFrames()\n",
    "\n",
    "    \n",
    "    #job_name = job_profile['job_name']\n",
    "    #X = job_profile['X']\n",
    "    #ieos = job_profile['ieos']\n",
    "\n",
    "    eos, eos_opacity = eoses\n",
    "\n",
    "    \n",
    "    # init rays directions\n",
    "    rays_dir = {}\n",
    "    for key in rays_dir_def.keys():\n",
    "        rays_dir[key] = np.array(rays_dir_def[key])\n",
    "\n",
    "\n",
    "    # main\n",
    "    for file_index in file_indexes:\n",
    "        \n",
    "        # init answer dict / array\n",
    "        photosphere_pars = { # [legend][par_name][time]\n",
    "            'time_yr': None,\n",
    "            'orbsep_Rsun': None,\n",
    "            'data': {},\n",
    "            'rays_dir': rays_dir_def,\n",
    "            'rays': {},\n",
    "        }  \n",
    "        for key in rays_dir.keys():\n",
    "            photosphere_pars['data'][key] = {}\n",
    "\n",
    "        # read data\n",
    "        mpdf = mpdf_read(job_name, file_index, eos_opacity, mpdf=None, reset_xyz_by='CoM', do_extrap=False, verbose=verbose)\n",
    "        #mpdf.read(job_name, file_index, reset_xyz_by='CoM', verbose=verbose)\n",
    "        #if 'Tdust' in mpdf.data['gas'].columns:\n",
    "        #    mpdf.data['gas']['T'] = mpdf.data['gas']['Tdust']\n",
    "        #elif 'temperature' in mpdf.data['gas'].columns:\n",
    "        #    mpdf.data['gas']['T'] = mpdf.data['gas']['temperature']\n",
    "        #if 'kappa' not in mpdf.data['gas'].keys():\n",
    "        #    # get kappa from mesa table in cgs units\n",
    "        #    mpdf.data['gas']['kappa'] = eos_opacity.get_kappa(\n",
    "        #        mpdf.get_val('rho', copy=False),\n",
    "        #        mpdf.get_val('T', copy=False),\n",
    "        #        do_extrap=True,\n",
    "        #        return_as_quantity=False)\n",
    "        ## translate to phantom units\n",
    "        #mpdf.calc_sdf_params(\n",
    "        #    calc_params=['kappa',], #'R1',\n",
    "        #    calc_params_params={'ieos': ieos, 'X':X, 'overwrite':False, 'kappa_translate_from_cgs_units':True},\n",
    "        #    verbose=verbose,\n",
    "        #)\n",
    "        hfact = mpdf.params['hfact']\n",
    "        mpart = mpdf.params['mass']\n",
    "        \n",
    "        photosphere_pars['time_yr'] = mpdf.get_time().to_value(units.year)\n",
    "        photosphere_pars['orbsep_Rsun'] = mpdf.get_orb_sep().to_value(units.Rsun)\n",
    "\n",
    "        # construct rays_dict\n",
    "        star_loc = np.array([mpdf.data['sink'][axis][0] for axis in 'xyz'])\n",
    "        rays_dict = {}    # legend: ray\n",
    "        for key in rays_dir.keys():\n",
    "            # init\n",
    "            ray = np.array([\n",
    "                star_loc,\n",
    "                star_loc + rays_dir[key],\n",
    "            ])\n",
    "            rays_dict[key] = ray\n",
    "            photosphere_pars['rays'][key] = ray.tolist()\n",
    "            ray_unit_vec = ray[1, :] - ray[0, :]\n",
    "            ray_unit_vec = ray_unit_vec / np.sum(ray_unit_vec**2)**0.5\n",
    "\n",
    "\n",
    "            # optimization- first select only the particles affecting the ray\n",
    "            #  because interpolation of m points with N particles scales with O(N*m),\n",
    "            #  reducing N can speed up calc significantly\n",
    "            sdf = mpdf.data['gas']\n",
    "            kernel_radius = sdf.kernel.get_radius()\n",
    "            hs = np.array(sdf['h'])\n",
    "            pts = np.array(sdf[['x', 'y', 'z']])    # (npart, 3)-shaped array\n",
    "            pts_on_ray = mupl.get_closest_pt_on_line(pts, ray)\n",
    "            sdf_selected_indices = (np.sum((pts - pts_on_ray)**2, axis=-1) <= (kernel_radius * hs)**2)\n",
    "            if verbose:\n",
    "                debug_info(\n",
    "                    'write_ph_loc_axes()', verbose,\n",
    "                    f\"{np.count_nonzero(sdf_selected_indices)} particles are close enough to the ray to have effects.\"\n",
    "                )\n",
    "            sdf = sdf.iloc[sdf_selected_indices]\n",
    "            pts = np.array(sdf[['x', 'y', 'z']])    # (npart, 3)-shaped array\n",
    "\n",
    "\n",
    "            # get optical depth\n",
    "            if verbose:\n",
    "                debug_info(\n",
    "                    'write_ph_loc_axes()', verbose,\n",
    "                    f\"{ray = }\"\n",
    "                )\n",
    "            pts_on_ray, dtaus, pts_order = mupl.light.get_optical_depth_by_ray_tracing_3D(sdf=sdf, ray=ray)\n",
    "            photosphere, (pts_waypts, pts_waypts_t, taus_waypts) = mupl.light.get_photosphere_on_ray(\n",
    "                pts_on_ray, dtaus, pts_order, sdf, ray,\n",
    "                calc_params = ['loc', 'R1', 'rho', 'u', 'h', 'T', 'kappa'],\n",
    "                hfact = hfact, mpart=mpart, eos=eos, sdf_units=mpdf.units,\n",
    "                ray_unit_vec=ray_unit_vec, verbose=verbose,\n",
    "                return_as_quantity=False,\n",
    "            )\n",
    "            photosphere_pars['data'][key] = photosphere\n",
    "            photosphere_pars['data'][key]['size'] = photosphere['R1']\n",
    "            R1_on_ray  = np.logspace(1, np.log10((pts_waypts_t[0] + pts_waypts_t[1]) / 2), 1000)[::-1]\n",
    "            tau_on_ray = np.interp(R1_on_ray, pts_waypts_t[::-1], taus_waypts[::-1])\n",
    "            pts_on_ray = ray[0][np.newaxis, :] + R1_on_ray[:, np.newaxis] * ray_unit_vec[np.newaxis, :]\n",
    "            photosphere_pars['data'][key][ 'R1_on_ray'] = R1_on_ray\n",
    "            photosphere_pars['data'][key]['tau_on_ray'] = tau_on_ray\n",
    "            photosphere_pars['data'][key]['rho_on_ray'] = mupl.sph_interp.get_sph_interp(sdf, 'rho', pts_on_ray, verbose=verbose)\n",
    "            photosphere_pars['data'][key][  'u_on_ray'] = mupl.sph_interp.get_sph_interp(sdf, 'u'  , pts_on_ray, verbose=verbose)\n",
    "            photosphere_pars['data'][key][  'T_on_ray'] = eos.get_temp(\n",
    "                set_as_quantity(photosphere['rho_on_ray'], mpdf.units['density']),\n",
    "                set_as_quantity(photosphere['u_on_ray']  , mpdf.units['specificEnergy']),\n",
    "                return_as_quantity=False, bounds_error=False)\n",
    "            photosphere_pars['data'][key]['kappa_on_ray']=mupl.sph_interp.get_sph_interp(sdf,'kappa',pts_on_ray, verbose=verbose)\n",
    "            photosphere_pars['data'][key]['kappaDust_on_ray']=mupl.sph_interp.get_sph_interp(sdf,'kappa_dust',pts_on_ray, verbose=verbose)\n",
    "                \n",
    "            if verbose:\n",
    "                debug_info(    # debug\n",
    "                    'write_ph_loc_axes()', verbose,\n",
    "                    f\"{photosphere_loc = }\\n{photosphere_dist_to_ray0 = }\\n\",\n",
    "                    f\"{photosphere_taus = }\\n\",\n",
    "                    f\"{pts_on_ray_ordered[photosphere_loc_index:photosphere_loc_index+2] = }\",\n",
    "                )\n",
    "\n",
    "        with open(f\"{mpdf.get_filename()}.photospherePars.xyz.json\", 'w') as f:\n",
    "            json_dump(photosphere_pars, f, metadata=metadata, indent=None)\n",
    "            if verbose: print(f\"\\n\\nWritten to {f.name}\\n\")\n",
    "                \n",
    "        del mpdf\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862a0ce8-57ed-4eeb-b09b-218f80982760",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe765991-3271-4c30-a63e-18628d8e272e",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_debug = True\n",
    "if do_debug and __name__ == '__main__':\n",
    "    from script_PhLocAxes__input import JOB_PROFILES_DICT\n",
    "    JOB_PROFILES = [JOB_PROFILES_DICT[key] for key in ('2md', '4md')]\n",
    "    for job_profile in JOB_PROFILES:\n",
    "        job_profile['file_indexes'] = (4800, 17600)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "951dd520-ce47-42ce-9b21-f4f1951e1cff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# main process\n",
    "\n",
    "\n",
    "\n",
    "# init rays directions\n",
    "rays_dir_def = {\n",
    "    # legend: ray direction name\n",
    "    '+x'  : [ 1., 0., 0.],\n",
    "    '+y'  : [ 0., 1., 0.],\n",
    "    '+z'  : [ 0., 0., 1.],\n",
    "    '-x'  : [-1., 0., 0.],\n",
    "    '-y'  : [ 0.,-1., 0.],\n",
    "    '-z'  : [ 0., 0.,-1.],\n",
    "}\n",
    "\n",
    "\n",
    "# run main\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    \n",
    "    # get ph loc for each dump file\n",
    "    args = []\n",
    "    for job_profile in JOB_PROFILES:\n",
    "    \n",
    "        file_indexes = job_profile['file_indexes']\n",
    "        job_name     = job_profile['job_name']\n",
    "        eos          = mupl.get_eos(job_profile['ieos'], job_profile['params'], settings)\n",
    "        eos_opacity  = mupl.eos.mesa.EoS_MESA_opacity(job_profile['params'], settings)\n",
    "    \n",
    "        \n",
    "        if NPROCESSES <= 1:\n",
    "            \n",
    "            # single process\n",
    "    \n",
    "            write_ph_loc_axes(\n",
    "                job_name = job_name, file_indexes = file_indexes, rays_dir_def = rays_dir_def,\n",
    "                eoses = (eos, eos_opacity), photosphere_tau = PHOTOSPHERE_TAU, verbose = verbose,\n",
    "            )\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            # multi-process\n",
    "\n",
    "            for file_index in file_indexes:\n",
    "                args.append((\n",
    "                    job_name,\n",
    "                    [file_index],\n",
    "                    rays_dir_def,\n",
    "                    (eos, eos_opacity),\n",
    "                    PHOTOSPHERE_TAU,\n",
    "                    0,\n",
    "                ))\n",
    "\n",
    "    if NPROCESSES > 1:\n",
    "        with Pool(processes=NPROCESSES) as pool:\n",
    "            pool.starmap(write_ph_loc_axes, args)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41b2c0d7-7d76-4452-b7e5-c5b1a47f9aa3",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../photosphere/luis_2md/light_04800__photospherePars__xyz.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# fetch\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_index \u001b[38;5;129;01min\u001b[39;00m file_indexes:\n\u001b[0;32m---> 37\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mjob_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfile_index\u001b[49m\u001b[38;5;132;43;01m:\u001b[39;49;00m\u001b[38;5;124;43m05\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m__photospherePars__xyz.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m verbose: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mLoading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m... \u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     41\u001b[0m         photosphere_pars \u001b[38;5;241m=\u001b[39m json_load(f)\n",
      "File \u001b[0;32m~/anaconda3/envs/clmu_1/lib/python3.11/site-packages/IPython/core/interactiveshell.py:308\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    303\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    304\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m     )\n\u001b[0;32m--> 308\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../photosphere/luis_2md/light_04800__photospherePars__xyz.json'"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    # syntheize the files into one big file\n",
    "    \n",
    "    for job_profile in JOB_PROFILES:\n",
    "    \n",
    "        job_name     = job_profile['job_name']\n",
    "        file_indexes = job_profile['file_indexes']\n",
    "    \n",
    "    \n",
    "        # init\n",
    "        photosphere_pars_all = { # [legend][par_name][time]\n",
    "            'time_yr': [],\n",
    "            'orbsep_Rsun': [],\n",
    "            'data': {},\n",
    "            'rays_dir': rays_dir_def,\n",
    "            'rays': {},\n",
    "        }  \n",
    "        for key in rays_dir_def.keys():\n",
    "            photosphere_pars_all['data'][key] = {\n",
    "                'size': [],\n",
    "                'rho' : [],\n",
    "                'u'   : [],\n",
    "                'h'   : [],\n",
    "                'T'   : [],\n",
    "                'R1_on_ray' : [],\n",
    "                'tau_on_ray': [],\n",
    "                'rho_on_ray': [],\n",
    "                'u_on_ray'  : [],\n",
    "                'T_on_ray'  : [],\n",
    "            }\n",
    "            photosphere_pars_all['rays'][key] = []\n",
    "    \n",
    "        \n",
    "        # fetch\n",
    "        for file_index in file_indexes:\n",
    "            with open(f\"{job_name}_{file_index:05}__photospherePars__xyz.json\", 'r') as f:\n",
    "                \n",
    "                if verbose: print(f\"\\n\\nLoading {f.name}... \", end='')\n",
    "                \n",
    "                photosphere_pars = json_load(f)\n",
    "                for it in ['time_yr', 'orbsep_Rsun']:\n",
    "                    photosphere_pars_all[it].append(photosphere_pars[it])\n",
    "                for key in rays_dir_def.keys():\n",
    "                    for it in photosphere_pars_all['data'][key].keys():\n",
    "                        obj = photosphere_pars['data'][key][it]\n",
    "                        if isinstance(obj, np.ndarray):\n",
    "                            obj = obj.tolist()\n",
    "                        photosphere_pars_all['data'][key][it].append(obj)\n",
    "                    photosphere_pars_all['rays'][key].append(photosphere_pars['rays'][key]) \n",
    "    \n",
    "                if verbose: print(f\"Done.\\n\")\n",
    "    \n",
    "        \n",
    "        # write\n",
    "        with open(f\"{job_name}__photospherePars__xyz.json\", 'w') as f:\n",
    "            json_dump(photosphere_pars_all, f, metadata=metadata, indent=None)\n",
    "            if verbose: print(f\"\\n\\nWritten to {f.name}.\\n\")\n",
    "\n",
    "\n",
    "    print(\"\\n\\n\\n*** All Done. ***\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f587fbf-94ac-4cf1-ba14-4313467ad842",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
