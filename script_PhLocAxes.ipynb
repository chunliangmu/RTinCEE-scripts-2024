{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3cda03b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Scripts for analyzing of phantom outputs.\\n\\nThis script writes hdf5 files for each dump (and one hdf5 file synthsizing all outputs)\\n    to plot photosphere size vs time or orbital separation.\\nIt does so by plotting photosphere intersection with traced rays originating from the primary star\\n    and shooting along the axes of the coordination frame.\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Scripts for analyzing of phantom outputs.\n",
    "\n",
    "This script writes hdf5 files for each dump (and one hdf5 file synthsizing all outputs)\n",
    "    to plot photosphere size vs time or orbital separation.\n",
    "It does so by plotting photosphere intersection with traced rays originating from the primary star\n",
    "    and shooting along the axes of the coordination frame.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b30fd6f",
   "metadata": {},
   "source": [
    "## Imports & Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20d3ef62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "import math\n",
    "import numpy as np\n",
    "from numpy import pi\n",
    "#import pandas\n",
    "from astropy import units\n",
    "from astropy import constants as const\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "#from moviepy.editor import ImageSequenceClip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d110bbc5-fb24-495d-b62a-33bf01b9a5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules listed in ./lib/\n",
    "\n",
    "import clmuphantomlib as mupl\n",
    "from clmuphantomlib import get_col_kernel_funcs\n",
    "from clmuphantomlib.io import json_load, json_dump, hdf5_load, hdf5_dump\n",
    "from clmuphantomlib.settings import DEFAULT_SETTINGS as settings\n",
    "from clmuphantomlib.log import error, warn, note, debug_info\n",
    "from clmuphantomlib.log import is_verbose, say\n",
    "from clmuphantomlib.units_util import set_as_quantity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea6adf1-b024-4f90-a2b2-d7bb5d154442",
   "metadata": {},
   "source": [
    "    ## import modules in arbitrary directory\n",
    "    \n",
    "    #import sys\n",
    "    \n",
    "    ## path to my python module lib directory\n",
    "    ## *** CHECK THIS! *** #\n",
    "    #SRC_LIB_PATH = sys.path[0] + '/lib'\n",
    "    \n",
    "    #if SRC_LIB_PATH not in sys.path:\n",
    "    #    sys.path.append(SRC_LIB_PATH)\n",
    "    ##print(*sys.path, sep='\\n')    # debug\n",
    "    #print(\n",
    "    #    \"\\n*   Please Make sure my module files are located in this directory (or change the SRC_LIB_PATH variable):\",\n",
    "    #    f\"\\n{SRC_LIB_PATH = }\\n\"\n",
    "    #)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "517d7e50-25fe-4750-8c5b-3e94519c1842",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# parallels & optimizations\n",
    "\n",
    "\n",
    "#import os\n",
    "## Fixing stupid numba killing kernel\n",
    "## See here https://github.com/numba/numba/issues/3016\n",
    "#os.environ['NUMBA_DISABLE_INTEL_SVML']  = '1'\n",
    "#from numba import njit, prange\n",
    "\n",
    "\n",
    "from multiprocessing import cpu_count, Pool #Process, Queue\n",
    "NPROCESSES = 1 if cpu_count() is None else max(cpu_count(), 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b37d5dde-8b39-4409-9bec-8ae2cabba7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*   Note   :    script:\n",
      "\tWill use 8 processes for parallelization\n"
     ]
    }
   ],
   "source": [
    "# settings\n",
    "#\n",
    "#   imported from script_input.py file\n",
    "\n",
    "from script_PhLocAxes__input import interm_dir, verbose, PHOTOSPHERE_TAU, JOB_PROFILES\n",
    "from _sharedFuncs import mpdf_read\n",
    "\n",
    "\n",
    "# set metadata\n",
    "with open(\"_metadata__input.json\", 'r') as f:\n",
    "    metadata = mupl.json_load(f)\n",
    "metadata['Title'] = \"Getting photosphere size on x, y, z axes\"\n",
    "metadata['Description'] = f\"\"\"Tracing 6 rays on +x, -x, +y, -y, +z, -z directon and get photosphere size, h, rho, u, T from them.\"\"\"\n",
    "\n",
    "\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "\n",
    "# print debug info\n",
    "if __name__ == '__main__' and is_verbose(verbose, 'note'):\n",
    "    # remember to check if name is '__main__' if you wanna say anything\n",
    "    #    so when you do multiprocessing the program doesn't freak out\n",
    "    say('note', \"script\", verbose, f\"Will use {NPROCESSES} processes for parallelization\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ae56404-ba6d-422e-929c-84537ad1f8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  import (my libs)\n",
    "from clmuphantomlib.log import say, is_verbose\n",
    "from clmuphantomlib.geometry import get_dist2_between_2pt, get_closest_pt_on_line, get_dist2_from_pt_to_line_nb, get_ray_unit_vec, get_rays_unit_vec\n",
    "from clmuphantomlib.sph_interp import get_sph_interp, get_h_from_rho, get_no_neigh\n",
    "from clmuphantomlib.units_util import set_as_quantity, set_as_quantity_temperature, get_units_field_name\n",
    "from clmuphantomlib.eos.base import EoS_Base\n",
    "#  import (general)\n",
    "import numpy as np\n",
    "from numpy import typing as npt\n",
    "import numba\n",
    "from numba import jit, prange\n",
    "import sarracen\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326be894-b973-44e1-b1a9-8648888b20e9",
   "metadata": {},
   "source": [
    "    # test runs\n",
    "    @jit(nopython=True, parallel=False, fastmath=True)\n",
    "    def _integrate_along_rays_gridxy_sub_parallel_olim__one_ray(\n",
    "        pts_ordered          : npt.NDArray[np.float64],    # (npart, 3)-shaped\n",
    "        hs_ordered           : npt.NDArray[np.float64],    # (npart,  )-shaped\n",
    "        mkappa_div_h2_ordered: npt.NDArray[np.float64],    # (npart,  )-shaped\n",
    "        srcfuncs_ordered     : npt.NDArray[np.float64],    # (npart,  )-shaped\n",
    "        ray_xy               : npt.NDArray[np.float64],    # (2,      )-shaped\n",
    "        ray_area             : np.float64,    # (nray,   )-shaped\n",
    "        kernel_rad           : float,\n",
    "        kernel_col           : numba.core.registry.CPUDispatcher,\n",
    "        kernel_csz           : numba.core.registry.CPUDispatcher,\n",
    "        kernel_w             : numba.core.registry.CPUDispatcher,\n",
    "        pts_order            : npt.NDArray[np.float64],    # (npart,  )-shaped\n",
    "        rel_tol              : float = 1e-16, # because float64 has only 16 digits accuracy\n",
    "        sample_midtau        : float = 10.,   # middle point tau of sample fraction, where linear scale changes to log scale\n",
    "        sample_maxtau        : float = 100.,  # max tau of sample fraction (must > midtau, preferably >= 2 * midtau)\n",
    "        nsample_pp           : int   = 1000,  # no of sample points for tau in (0., midtau] (linear) and (midtau, maxtau] (log)\n",
    "                                              #    total no of sample points is hence twice as nsample_pp\n",
    "        z_olim_kc            : float = 1.0,  # col kernel limit for when srcfunc began to count\n",
    "    ) -> tuple[\n",
    "        npt.NDArray[np.float64],    # anses\n",
    "        npt.NDArray[np.float64],    # olims\n",
    "        npt.NDArray[np.float64],    # pones\n",
    "        npt.NDArray[np.float64],    # ptaus\n",
    "        npt.NDArray[np.int64  ],    # indes\n",
    "        npt.NDArray[np.float64],    # contr\n",
    "        npt.NDArray[np.float64],    # jfact\n",
    "        npt.NDArray[np.float64],    # jfact_olims\n",
    "        npt.NDArray[np.float64],    # estis\n",
    "    ]:\n",
    "        \"\"\"Single ray version of _integrate_along_rays_gridxy_sub_parallel_olim in script_LCGen, as of 2025-06-05\n",
    "    \n",
    "        See that func for help info\n",
    "        \"\"\"\n",
    "        #raise NotImplementedError\n",
    "    \n",
    "        nray  = len(rays_xy)\n",
    "        npart = len(srcfuncs_ordered)\n",
    "        ndim  = pts_ordered.shape[-1]\n",
    "        anses = np.zeros(nray)\n",
    "        olims = np.zeros(nray)\n",
    "        indes = np.zeros(nray, dtype=np.int64)    # indexes of max contribution particle\n",
    "        contr = np.zeros(nray)     # relative contribution of the max contribution particle\n",
    "        # ifact = np.zeros(nray)     # effective xsec for i-th ray  # NOTE: ifact = pones * ray_areas\n",
    "        jfact = np.zeros(npart)    # effective xsec for j-th particle\n",
    "        jfact_olims = np.zeros(npart)    # effective xsec for j-th particle\n",
    "        pones = np.zeros(nray)\n",
    "        ptaus = np.full(nray, np.nan)    # lower bound of the optical depth\n",
    "        estis = np.zeros(nray)\n",
    "        SAMPLE_TAUS = np.concatenate((\n",
    "            np.linspace(0., sample_midtau, nsample_pp+1)[1:],\n",
    "            np.logspace(np.log10(sample_midtau), np.log10(sample_maxtau), nsample_pp+1)[1:],\n",
    "        ))\n",
    "        nsample = SAMPLE_TAUS.size    # should be 2*nsample_pp\n",
    "    \n",
    "        # error tolerance of tau (part 1)\n",
    "        # #    this is aonly here for reference- value will be updated later\n",
    "        # tol_tau_base = np.log(srcfuncs_ordered.sum()) - np.log(rel_tol)\n",
    "    \n",
    "    \n",
    "        # cache calcs\n",
    "        hrs_ordered = hs_ordered * kernel_rad\n",
    "        #   limits (approx)\n",
    "        z_min = pts_ordered[-1, 2] - hrs_ordered[-1]\n",
    "        z_max = pts_ordered[ 0, 2] + hrs_ordered[ 0]\n",
    "    \n",
    "        \n",
    "        # === OLD: loop over ray ===\n",
    "    \n",
    "        ray_xy  = rays_xy[i]\n",
    "        ray_area= ray_areas[i]\n",
    "        tau     = 0.\n",
    "        rad     = 0.\n",
    "        drad    = 0.\n",
    "        rad_olim= 0.\n",
    "        rad_est = 0.   # estimation of radiance (i.e. rad)\n",
    "        # dans_max_tmp = 0.\n",
    "        dfac_max_tmp = 0.\n",
    "        ind     = -1\n",
    "        fac     = 0. # effectively <1>\n",
    "        dfac    = 0. # factor\n",
    "        used_j  = np.int64(0)    # j = used_indexes[used_j]\n",
    "        ji      = np.int64(0)\n",
    "        srcfuncs_tot_relevant = 0.\n",
    "        z_olim = 0.\n",
    "        # dLi_div_4pikappaj = 0.    # is sum_Sk_p_Aeffk_div_p_kappaj, for estimation os error from \\\\delta \\\\kappa\n",
    "    \n",
    "        #   xy-grid specific solution\n",
    "        ray_x = ray_xy[0]\n",
    "        ray_y = ray_xy[1]\n",
    "    \n",
    "        \n",
    "    \n",
    "        # First, try to find out how many relevant particles are there\n",
    "    \n",
    "        nused_i = 0    # no of used particles for i-th ray\n",
    "        for j in range(npart):\n",
    "            x_j = pts_ordered[j, 0]\n",
    "            y_j = pts_ordered[j, 1]\n",
    "            hr = hrs_ordered[j]\n",
    "            # check if the particle is within range\n",
    "            #   xy-grid specific solution\n",
    "            if ray_x - hr < x_j and x_j < ray_x + hr and ray_y - hr < y_j and y_j < ray_y + hr:\n",
    "                nused_i += 1\n",
    "                srcfuncs_tot_relevant += srcfuncs_ordered[j]\n",
    "    \n",
    "        \n",
    "        if nused_i <= 0:\n",
    "            return rad, rad_olim, fac, tau, ind, contr_i, jfact, jfact_olims, rad_est    # skip\n",
    "    \n",
    "        \n",
    "        # Then, get the indexes etc info of these relevant particles\n",
    "    \n",
    "        \n",
    "        used_indexes = np.full(nused_i, -1, dtype=np.int64)\n",
    "        used_dtaus   = np.full(nused_i, np.nan)\n",
    "        used_qs_xy   = np.full(nused_i, np.nan)\n",
    "        used_dzs     = np.full(nused_i, np.nan)\n",
    "        tol_tau_base_i = np.log(srcfuncs_tot_relevant) - np.log(rel_tol)    # error tolerance of tau (part 1)\n",
    "        usedp2_zs  = np.full(nused_i+2, np.nan)\n",
    "        usedp2_zs_max, usedp2_zs_min = z_min, z_max\n",
    "    \n",
    "        used_j = 0    # j = used_indexes[used_j]\n",
    "        tau    = 0.\n",
    "        rad_est= 0.   # estimation of radiance (i.e. rad)\n",
    "        for j in range(npart):\n",
    "            x_j = pts_ordered[j, 0]\n",
    "            y_j = pts_ordered[j, 1]\n",
    "            hr  = hrs_ordered[j]\n",
    "            \n",
    "            # check if the particle is within range\n",
    "            #   xy-grid specific solution\n",
    "            if ray_x - hr < x_j and x_j < ray_x + hr and ray_y - hr < y_j and y_j < ray_y + hr:\n",
    "                r2_xy = (x_j - ray_x)**2 + (y_j - ray_y)**2\n",
    "                dz2 = hr**2 - r2_xy\n",
    "                if dz2 > 0:\n",
    "                    h = hs_ordered[ j]\n",
    "                    q_xy = np.sqrt(r2_xy)/h\n",
    "    \n",
    "                    # log\n",
    "                    dz = np.sqrt(dz2)\n",
    "                    mkappa_div_h2 = mkappa_div_h2_ordered[j]\n",
    "                    srcfunc = srcfuncs_ordered[j]\n",
    "                    dtau = mkappa_div_h2 * kernel_col(q_xy, ndim)\n",
    "                            \n",
    "                    z_j = pts_ordered[j, 2]\n",
    "                    used_indexes[used_j] = j\n",
    "                    used_dtaus[  used_j] = dtau\n",
    "                    used_qs_xy[  used_j] = q_xy\n",
    "                    used_dzs[    used_j] = dz\n",
    "                    usedp2_zs[ used_j+1] = z_j\n",
    "                    usedp2_zs_max = max(usedp2_zs_max, z_j + dz)\n",
    "                    usedp2_zs_min = min(usedp2_zs_min, z_j - dz)\n",
    "                    used_j += 1\n",
    "    \n",
    "    \n",
    "    \n",
    "                    dfac = np.exp(-tau) * (1. - np.exp(-dtau))\n",
    "                    #drad = dfac * srcfunc\n",
    "                    rad_est += dfac * srcfunc #drad\n",
    "                    tau += dtau\n",
    "                    \n",
    "                    # terminate the calc for this ray if tau is sufficient large\n",
    "                    #    such that the relative error on rad is smaller than rel_tol\n",
    "                    # i.e. since when tau > np.log(srcfuncs_ordered.sum()) - np.log(rel_tol) - np.log(rad),\n",
    "                    #    we know that rad[i] - rad[i][k] < rel_tol * rad[i]\n",
    "                    # see my notes for derivation\n",
    "                    if tau > sample_maxtau or tau > tol_tau_base_i - np.log(rad_est):\n",
    "                        break\n",
    "        else:\n",
    "            ptaus[i]=tau\n",
    "        nused_i = used_j     # update used indexes size\n",
    "        used_dtaus = used_dtaus[:nused_i]\n",
    "        usedp2_zs[0] = usedp2_zs_max\n",
    "        usedp2_zs[nused_i+1] = usedp2_zs_min\n",
    "    \n",
    "    \n",
    "        \n",
    "        if nused_i <= 0:\n",
    "            return rad, rad_olim, fac, tau, ind, contr_i, jfact, jfact_olims, rad_est    # skip\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "        # Then, get optical depth etc info at interpolated positions\n",
    "    \n",
    "        used_ji_range = np.zeros((nused_i, 2), dtype=np.int64)    # initial ji for each part j\n",
    "        usedp2_taus = np.zeros(nused_i+2)\n",
    "        usedp2_taus[1:-1] = np.cumsum(used_dtaus) - used_dtaus/2\n",
    "        usedp2_taus[-1] = tau\n",
    "        zs_i = np.interp(\n",
    "            SAMPLE_TAUS,\n",
    "            usedp2_taus,\n",
    "            usedp2_zs[:nused_i+2]\n",
    "        )\n",
    "        \n",
    "        dzs_i  = np.zeros_like(zs_i)\n",
    "        taus_i = np.zeros_like(zs_i)\n",
    "        kcs_i  = np.zeros_like(zs_i)    # col kernel cummulative sum\n",
    "        for used_j in range(nused_i):\n",
    "            j    = used_indexes[used_j]\n",
    "            q_xy = used_qs_xy[used_j] # ((pt[0] - ray_x)**2 + (pt[1] - ray_y)**2)**0.5 / h\n",
    "            z    = pts_ordered[j, 2]\n",
    "            dz   = used_dzs[used_j]\n",
    "            hr   = hrs_ordered[j]\n",
    "            h    = hs_ordered[ j]\n",
    "            mkappa_div_h2 = mkappa_div_h2_ordered[j]\n",
    "            dtau = used_dtaus[used_j]\n",
    "            \n",
    "            # find initial ji\n",
    "            ji_0 = nsample - np.searchsorted(zs_i[::-1], z+dz)\n",
    "            ji_e = min(nsample, nsample - np.searchsorted(zs_i[::-1], z-dz) + 1)\n",
    "            used_ji_range[used_j, 0] = ji_0\n",
    "            used_ji_range[used_j, 1] = ji_e\n",
    "            if ji_0 >= nsample:\n",
    "                continue\n",
    "            for ji in range(ji_0, ji_e):\n",
    "                q_z  = (zs_i[ji] - z) / h\n",
    "                kc = kernel_csz(q_xy, -q_z, ndim)\n",
    "                kcs_i[ ji] += kc\n",
    "                taus_i[ji] += mkappa_div_h2 * kc\n",
    "            # kc = kernel_col(q_xy, ndim)\n",
    "            taus_i[ji:] += dtau\n",
    "            kcs_i[ ji:] += kc\n",
    "                \n",
    "        z_olim = zs_i[-1]\n",
    "        z_olim_not_found = True\n",
    "        for ji in range(1, nsample-1):\n",
    "            dzs_i[ji] = (zs_i[ji-1] - zs_i[ji+1]) / 2\n",
    "            if z_olim_not_found and kcs_i[ji] > z_olim_kc:\n",
    "                z_olim = zs_i[ji]\n",
    "                z_olim_not_found = False\n",
    "        dzs_i[0] = (zs_i[0] - zs_i[1]) / 2\n",
    "        dzs_i[-1] = (zs_i[-2] - zs_i[-1]) / 2\n",
    "    \n",
    "    \n",
    "        \n",
    "        # Now, do radiative transfer\n",
    "    \n",
    "        \n",
    "        for used_j in range(nused_i):\n",
    "            ji_0, ji_e = used_ji_range[used_j]\n",
    "            if ji_0 >= nsample:\n",
    "                continue\n",
    "            j    = used_indexes[used_j]\n",
    "            q_xy = used_qs_xy[used_j]\n",
    "            q2_xy= q_xy**2\n",
    "            h    = hs_ordered[ j]\n",
    "            hr   = hrs_ordered[j]\n",
    "            mkappa_div_h2 = mkappa_div_h2_ordered[j]\n",
    "            srcfunc = srcfuncs_ordered[j]\n",
    "            z       = pts_ordered[j, 2]\n",
    "            dz      = used_dzs[used_j]\n",
    "            zmdz    = z - dz\n",
    "            zpdz    = z + dz\n",
    "            dfac  = 0.\n",
    "            dfac_olim = 0.    # same as dfac, except that discounts the part outside z_olim (outmost particle loc) to zero\n",
    "            ddfac = 0.    # temp storage\n",
    "            \n",
    "            # radiative transfer\n",
    "            for ji in range(ji_0, ji_e):\n",
    "                q_ji = np.sqrt(q2_xy + ((zs_i[ji] - z)/h)**2)\n",
    "                dqz_ji = dzs_i[ji]/h\n",
    "                ddfac = np.exp(-taus_i[ji]) * kernel_w(q_ji, ndim) * dqz_ji\n",
    "                dfac += ddfac\n",
    "                if zs_i[ji] < z_olim:\n",
    "                    dfac_olim += ddfac\n",
    "                \n",
    "    \n",
    "            dfac *= mkappa_div_h2\n",
    "            dfac_olim *= mkappa_div_h2\n",
    "            drad  = dfac * srcfunc\n",
    "            rad  += drad    # for getting <S>\n",
    "            fac  += dfac    # for getting <1>\n",
    "            rad_olim += dfac_olim * srcfunc\n",
    "            #tau = tau_pr + taus_j[nsample_half]\n",
    "            \n",
    "    \n",
    "            # using olim\n",
    "            jfact[j] += dfac * ray_area\n",
    "            jfact_olims[j] += dfac_olim * ray_area\n",
    "            \n",
    "            # # note down the largest contributor\n",
    "            # if drad > dans_max_tmp:\n",
    "            #     dans_max_tmp = drad\n",
    "            #     ind = pts_order[j]\n",
    "            if dfac_olim > dfac_max_tmp:\n",
    "                dfac_max_tmp = dfac_olim\n",
    "                ind = pts_order[j]\n",
    "    \n",
    "    \n",
    "        return zs_i, taus_i, kcs_i\n",
    "    \n",
    "    \n",
    "        # contr_i = dfac_max_tmp / fac if rad > 0 else 0.\n",
    "        # # Note: jfact and jfact_olims are array\n",
    "        # return rad, rad_olim, fac, tau, ind, contr_i, jfact, jfact_olims, rad_est\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "        # anses[i] = rad\n",
    "        # olims[i] = rad_olim\n",
    "        # pones[i] = fac\n",
    "        # ptaus[i] = tau\n",
    "        # indes[i] = ind\n",
    "        # contr[i] = contr_i\n",
    "        # estis[i] = rad_est\n",
    "        \n",
    "        # return anses, olims, pones, ptaus, indes, contr, jfact, jfact_olims, estis\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e63b52a2-8cf5-47d4-94c8-b424f911c419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_photosphere_on_ray(\n",
    "    pts_on_ray            : np.ndarray,\n",
    "    dtaus                 : np.ndarray,\n",
    "    pts_order             : np.ndarray,\n",
    "    sdf                   : sarracen.SarracenDataFrame,\n",
    "    ray                   : np.ndarray,\n",
    "    calc_params           : list       = ['loc', 'R1'],\n",
    "    hfact                 : float      = None,\n",
    "    mpart                 : float      = None,\n",
    "    eos                   : EoS_Base   = None,\n",
    "    sdf_units             : dict       = None,\n",
    "    ray_unit_vec          : np.ndarray = None,\n",
    "    kernel                : sarracen.kernels.base_kernel = None,\n",
    "    do_skip_zero_dtau_pts : bool       = True,\n",
    "    do_use_precise_calc   : bool       = True,\n",
    "    photosphere_tau       : float      = 1.,\n",
    "    waypts_max_tau        : None|float = None, #746.,    # exp(-746) is at float64 machine precision limit\n",
    "    waypts_nsample        : int        = 1024,\n",
    "    ndim                  : int        = 3,\n",
    "    return_as_quantity    : bool|None  = True,\n",
    "    verbose : int = 3,\n",
    ") -> tuple[dict, tuple[np.ndarray, np.ndarray, np.ndarray]]:\n",
    "    \"\"\"Calc the location where the photosphere intersect with the ray.\n",
    "\n",
    "    Assuming 3D.\n",
    "\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pts_on_ray, dtaus, pts_order\n",
    "        output from get_optical_depth().\n",
    "\n",
    "        pts_on_ray: np.ndarray\n",
    "            Orthogonal projections of the particles' locations onto the ray.\n",
    "        \n",
    "        dtaus: np.ndarray\n",
    "            Optical depth tau contributed by each particles. In order of the original particles order in the dump file.\n",
    "            Remember tau is a dimensionless quantity.\n",
    "        \n",
    "        pts_order: np.ndarray\n",
    "            indices of the particles where dtaus are non-zero.\n",
    "            The indices are arranged by distances to the observer, i.e. the particles closest to the observer comes first, \n",
    "            and the furtherest comes last.\n",
    "\n",
    "    sdf: sarracen.SarracenDataFrame\n",
    "        Must contain columns: x, y, z, h    # kappa, rho,\n",
    "        \n",
    "    ray: (2, 3)-shaped numpy array, i.e. [pt1, pt2]\n",
    "        2 points required to determine a line.\n",
    "        The line is described as X(t) = pt1 + t*(pt2-pt1)\n",
    "        First  point pt1 is the reference of the distance if R1 is calc-ed.\n",
    "        Second point pt2 points to the observer, and is closer to the observer.\n",
    "\n",
    "    calc_params: list or tuple of str\n",
    "        parameters to be calculated / interpolated at the photosphere location.\n",
    "        Results will be put into the photosphere dict in the output.\n",
    "        Acceptable input: (Note: will always calc 'loc' if calc_params is not empty)\n",
    "            'is_found': will return bool.\n",
    "                Will always be outputted regardless of in calc_params or not.\n",
    "            'loc': will return (3,)-shaped numpy array.\n",
    "                photophere location.\n",
    "            'R1' : will return float.\n",
    "                distance between photosphere location and the ray[0].\n",
    "                Could be negative if loc is on the other side of the ray.\n",
    "            'nneigh': will return int.\n",
    "                Number of neighbour particles of the photosphere loc.\n",
    "            'rho': will return float.\n",
    "                density at the photosphere.\n",
    "            'u': will return float.\n",
    "                specific internel energy at the photosphere.\n",
    "            'h'  : will return float.\n",
    "                smoothing length at the photosphere.\n",
    "                Will always calc 'rho' if to calc 'h'.\n",
    "            'T'  : will return float.\n",
    "                Temperature at the photosphere.\n",
    "                Warning: if not supplied 'temp' keyword in sdf_units, will return in cgs units.\n",
    "    \n",
    "    hfact, mpart: float\n",
    "        Only useful if you are calc-ing 'h'\n",
    "        $h_\\\\mathrm{fact}$ and particle mass used in the phantom sim.\n",
    "        If None, will get from sdf.params['hfact'] and sdf.params['mass']\n",
    "\n",
    "    eos: .eos.base.EoS_BASE\n",
    "        Only useful if you are calc-ing 'T'\n",
    "        Equation of state object defined in eos/base.py\n",
    "\n",
    "    sdf_units: dict\n",
    "        Only useful if you are calc-ing 'T'\n",
    "        in which case, supply rho, u, and T units in this dict\n",
    "        e.g.\n",
    "        sdf_units = {\n",
    "            'density': units.Msun / units.Rsun**3,\n",
    "            'specificEnergy': units.Rsun**2 / units.s**2,\n",
    "            'temp': units.K,\n",
    "        }\n",
    "    \n",
    "    ray_unit_vec: (3,)-shaped np.ndarray\n",
    "        unit vector of ray. will calc this if not supplied.\n",
    "        \n",
    "    kernel: sarracen.kernels.base_kernel\n",
    "        Smoothing kernel for SPH data interpolation.\n",
    "        If None, will use the one in sdf.\n",
    "        \n",
    "    do_skip_zero_dtau_pts: bool\n",
    "        Whether or not to skip particles with zero dtaus (i.e. no contribution to opacity) to save computational time.\n",
    "        If skiped, these particles' locs will be excluded from results as well\n",
    "\n",
    "    do_use_precise_calc: bool\n",
    "        If True, will use the new tau interpolation algorithm used in LCGen.\n",
    "        otherwise, will just interpolate from dtaus input (i.e. assuming each SPH particle is a point mass)\n",
    "        \n",
    "    photosphere_tau: float\n",
    "        At what optical depth (tau) is the photosphere defined.\n",
    "\n",
    "    waypts_max_tau: float\n",
    "        maximum optical depth that we will look inside.\n",
    "\n",
    "    return_as_quantity: bool | None\n",
    "        If True or None, the results in photosphere will be returned as a astropy.units.Quantity according to sdf_units.\n",
    "        (pts_waypts, pts_waypts_t, taus_waypts) will also be returned as numpy array and NOT as Quantity.\n",
    "        The diff between True and None is that True will raise an error if units not supplied in sdf_units,\n",
    "        while None will just return as numpy array in such case.\n",
    "        \n",
    "    \n",
    "    verbose: int\n",
    "        How much warnings, notes, and debug info to be print on screen. \n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    photosphere, (pts_waypts, pts_waypts_t, taus_waypts)\n",
    "\n",
    "    photosphere: dict\n",
    "        dict of values found at the photosphere intersection point with the ray.\n",
    "        will always have \n",
    "\n",
    "    pts_waypts: (npart, 3)-shaped numpy array\n",
    "        location of the waypoints on ray\n",
    "\n",
    "    pts_waypts_t: (npart)-shaped numpy array\n",
    "        distance of the waypoints from ray[0]\n",
    "\n",
    "    taus_waypts: (npart)-shaped numpy array\n",
    "        optical depth at the waypoints.\n",
    "\n",
    "    kcs_waypts: (npart)-shaped numpy array\n",
    "        summed column kernel value at the waypoints.\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    # init\n",
    "    ray = np.array(ray)\n",
    "    if ray_unit_vec is None:\n",
    "        ray_unit_vec = get_ray_unit_vec(ray)\n",
    "    if kernel is None:\n",
    "        kernel = sdf.kernel\n",
    "    kernel_col, kernel_csz, _, _ = get_col_kernel_funcs(kernel)\n",
    "    if do_skip_zero_dtau_pts:\n",
    "        pts_order = pts_order[np.where(dtaus[pts_order])]\n",
    "    ray_0 = np.asarray(ray[0])\n",
    "    pts_ordered    = np.asarray(sdf[['x', 'y', 'z']].iloc[pts_order])\n",
    "    hs_ordered     = np.asarray(sdf[ 'h'           ].iloc[pts_order])\n",
    "    #kappas_ordered = np.array(sdf[ 'kappa'       ].iloc[pts_order])\n",
    "    #rhos_ordered   = np.array(sdf[ 'rho'         ].iloc[pts_order])\n",
    "    dtaus_ordered  = dtaus[pts_order]\n",
    "    pts_on_ray_ordered = pts_on_ray[pts_order]\n",
    "    npart_ordered = pts_ordered.shape[0]\n",
    "\n",
    "\n",
    "    \n",
    "    # get waypts (way points) for pts (point locations) and taus (optical depths)\n",
    "    #  waypts are suitable for linear interpolation\n",
    "    #  taus_waypts[0] is 0; taus_waypts[-1] is total optical depth from the object\n",
    "\n",
    "    \n",
    "    #  step 1: determine the waypts location by assuming pts as balls with constant kappa and density\n",
    "    \n",
    "    #   step 1a: getting the size of pts balls on the ray\n",
    "    pts_dist2_to_ray = get_dist2_between_2pt(pts_ordered, pts_on_ray_ordered)\n",
    "    #    Assuming a h radius ball\n",
    "    pts_radius = kernel.get_radius() * hs_ordered\n",
    "    pts_size_on_ray = pts_radius**2 - pts_dist2_to_ray\n",
    "    # put a small number (1e-8*h) in case of negative pts_size_on_ray, so that the code does not freak out\n",
    "    pts_size_on_ray_min = 1e-8*hs_ordered\n",
    "    pts_size_on_ray = np.where(pts_size_on_ray < pts_size_on_ray_min**2, pts_size_on_ray_min, pts_size_on_ray**0.5)\n",
    "    #pts_size_on_ray = dtaus_ordered / (kappas_ordered * rhos_ordered)    # does not work because rho is not a constant within the particle\n",
    "\n",
    "    #   step 1b: getting the waypoint locs\n",
    "    pts_on_ray_t_ordered = np.sum((pts_on_ray_ordered - ray_0) * ray_unit_vec, axis=-1)\n",
    "    if waypts_max_tau is None:\n",
    "        # calc waypoints by interpolating ALL relavent particle's positions\n",
    "        #    pts_waypts_t: the distance from waypts to ray_0 (negative if in the opposite direction)\n",
    "        pts_waypts_t = np.interp(    # 5 data points in between pts, so npart_ordered*5 - 1\n",
    "            np.linspace(0, npart_ordered-1, (npart_ordered-1)*5 + 1),\n",
    "            np.linspace(0, npart_ordered-1, npart_ordered),\n",
    "            pts_on_ray_t_ordered)\n",
    "        pts_waypts_t = np.concatenate((    # add before and after the first particles\n",
    "            np.linspace(pts_on_ray_t_ordered[0] + pts_radius[0], pts_on_ray_t_ordered[0], 10),\n",
    "            pts_waypts_t,\n",
    "            np.linspace(pts_on_ray_t_ordered[-1] - pts_radius[-1], pts_on_ray_t_ordered[-1], 10),\n",
    "        ))\n",
    "    else:\n",
    "        # calc waypoints by interpolating between tau = 0 and tau = waypts_max_tau\n",
    "        ind = min(np.searchsorted(np.cumsum(dtaus_ordered), waypts_max_tau), npart_ordered-1)\n",
    "        pts_waypts_t = np.linspace(\n",
    "            pts_on_ray_t_ordered[0  ] + pts_radius[0  ],\n",
    "            pts_on_ray_t_ordered[ind] - pts_radius[ind],\n",
    "            waypts_nsample+1, endpoint=False)[1:]    # remove head point to avoid division by zero warnings\n",
    "    pts_waypts = ray_0 + pts_waypts_t[:, np.newaxis] * ray_unit_vec[np.newaxis, :]\n",
    "        \n",
    "\n",
    "    #   step 1c: sort waypoint locs\n",
    "    #    sorting should not be necessary, but just in case\n",
    "    pts_waypts_inds = np.argsort(pts_waypts_t)[::-1]\n",
    "    pts_waypts   = pts_waypts[  pts_waypts_inds]\n",
    "    pts_waypts_t = pts_waypts_t[pts_waypts_inds]\n",
    "    \n",
    "    #  step 2: determine the waypts optical depth\n",
    "    taus_waypts = np.zeros(pts_waypts.shape[0])\n",
    "    kcs_waypts  = np.zeros_like(taus_waypts)\n",
    "    if do_use_precise_calc:\n",
    "        # re-calc\n",
    "        mkappa_div_h2_ordered = np.asarray(sdf['m'].iloc[pts_order] * sdf['kappa'].iloc[pts_order] / hs_ordered**2)\n",
    "        for j in range(npart_ordered):\n",
    "            h = hs_ordered[j]\n",
    "            q_xy_j = pts_dist2_to_ray[j]**0.5 / h\n",
    "            t_j = pts_on_ray_t_ordered[j]\n",
    "            dkcs_waypts = np.asarray([\n",
    "                kernel_csz(q_xy_j, -(t_k - t_j)/h, ndim) for t_k in pts_waypts_t])\n",
    "            kcs_waypts  += dkcs_waypts\n",
    "            taus_waypts += mkappa_div_h2_ordered[j] * dkcs_waypts\n",
    "    else:\n",
    "        # interpolate from given dtau (not the same as in LCGen)\n",
    "        for waypt_t, h, dtau in zip(pts_waypts_t, hs_ordered, dtaus_ordered):\n",
    "            hr = h * kernel.get_radius()\n",
    "            # Note: np.interp assumes xp increasing, so we need to reverse this\n",
    "            taus_waypts += np.interp(pts_waypts_t[::-1], [waypt_t-hr, waypt_t+hr], [dtau, 0.], left=dtau, right=0.)[::-1]\n",
    "        \n",
    "\n",
    "    # prepare answers\n",
    "    # is found?\n",
    "    if not taus_waypts.size:\n",
    "        taus_max = 0\n",
    "    elif np.isfinite(taus_waypts[-1]):\n",
    "        # in case there is nan in the later part of the array\n",
    "        taus_max = taus_waypts[-1]\n",
    "    else:\n",
    "        taus_max = np.nanmax(taus_waypts)\n",
    "    photosphere = {\n",
    "        'is_found': (taus_max > photosphere_tau)\n",
    "    }\n",
    "    \n",
    "    # get photosphere parameters\n",
    "    if calc_params:\n",
    "        # always calc location if anything needs to be calc-ed\n",
    "        photosphere['loc'] = np.array([\n",
    "            np.interp(photosphere_tau, taus_waypts, pts_waypts[:, ax], right=np.nan) if taus_waypts.size else np.nan\n",
    "            for ax in range(pts_waypts.shape[1])\n",
    "        ])\n",
    "\n",
    "        # do prerequisite check\n",
    "        calc_params = list(calc_params)\n",
    "        if 'h' in calc_params:\n",
    "            if 'rho' not in calc_params: calc_params.append('rho')\n",
    "        if 'T'   in calc_params:\n",
    "            if 'rho' not in calc_params: calc_params.append('rho')\n",
    "            if 'u'   not in calc_params: calc_params.append('u')\n",
    "\n",
    "        # first calc prerequisites\n",
    "        calc_these = []\n",
    "        for calc_name in calc_params:\n",
    "            if   calc_name == 'loc':\n",
    "                # already calc-ed\n",
    "                pass\n",
    "            elif calc_name == 'R1':\n",
    "                photosphere['R1']  = np.interp(photosphere_tau, taus_waypts, pts_waypts_t, right=np.nan) if taus_waypts.size else np.nan\n",
    "            elif calc_name in {'rho', 'u'}:\n",
    "                photosphere[calc_name]  = get_sph_interp(sdf, calc_name, photosphere['loc'], kernel=kernel, verbose=verbose)\n",
    "            elif calc_name in {'nneigh'}:\n",
    "                photosphere[calc_name]  = get_no_neigh(sdf, photosphere['loc'], kernel=kernel, verbose=verbose)\n",
    "            else:\n",
    "                calc_these.append(calc_name)\n",
    "    \n",
    "        # now the rest\n",
    "        for calc_name in calc_these:\n",
    "            if calc_name == 'h':\n",
    "                if hfact is None: hfact = sdf.params['hfact']\n",
    "                if mpart is None: mpart = sdf.params['mass']\n",
    "                photosphere['h']  = get_h_from_rho(photosphere['rho'], mpart, hfact)\n",
    "            elif calc_name == 'T':\n",
    "                if eos   is None: raise ValueError(\"get_photosphere_on_ray(): Please supply equation of state to calculate temperature.\")\n",
    "                try:\n",
    "                    photosphere['T']  = eos.get_temp(\n",
    "                        set_as_quantity(photosphere['rho'], sdf_units['density']),\n",
    "                        set_as_quantity(photosphere['u'  ], sdf_units['specificEnergy']))\n",
    "                    if 'temp' in sdf_units:\n",
    "                        photosphere['T'] = set_as_quantity_temperature(photosphere['T'], sdf_units['temp']).value\n",
    "                    else:\n",
    "                        photosphere['T'] = photosphere['T'].value\n",
    "                except ValueError:\n",
    "                    # eos interp could go out of bounds if it's a tabulated EoS\n",
    "                    # which will raise a Value Error\n",
    "                    photosphere['T'] = np.nan\n",
    "            else:\n",
    "                # just interpolate it (#IT-JUST-WORKS)\n",
    "                photosphere[calc_name]  = get_sph_interp(sdf, calc_name, photosphere['loc'], kernel=kernel, verbose=verbose)\n",
    "\n",
    "        # add units\n",
    "        if return_as_quantity or return_as_quantity is None:\n",
    "            for calc_name in photosphere.keys():\n",
    "                if calc_name not in {'is_found', 'nneigh'}:\n",
    "                    # find appropriate unit\n",
    "                    try:\n",
    "                        unit_field_name = get_units_field_name(calc_name)\n",
    "                    except NotImplementedError:\n",
    "                        # failed to find unit type\n",
    "                        unit_field_name = None\n",
    "                    if unit_field_name in sdf_units.keys():\n",
    "                        # add units\n",
    "                        photosphere[calc_name] = set_as_quantity(photosphere[calc_name], sdf_units[unit_field_name])\n",
    "                    # errors\n",
    "                    elif unit_field_name is None:\n",
    "                        if is_verbose(verbose, 'warn'):\n",
    "                            say('warn', None, verbose,\n",
    "                                f\"Cannot find the corresponding unit for {calc_name}. Will return as numpy array instead.\")\n",
    "                    elif return_as_quantity is not None:\n",
    "                        raise ValueError(f\"Please supply {unit_field_name} in sdf_units.\")\n",
    "        \n",
    "        \n",
    "    return photosphere, (pts_waypts, pts_waypts_t, taus_waypts, kcs_waypts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40466d2f",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f063ed6",
   "metadata": {},
   "source": [
    "## Photosphere size vs time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f8132b3-209d-4680-b364-d9d5eb4cb219",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_ph_loc_axes(\n",
    "    job_profile : dict,\n",
    "    #job_name : str,\n",
    "    file_indexes : np.ndarray,\n",
    "    rays_dir_def : dict,    # dict of list\n",
    "    eoses : (mupl.eos.base.EoS_Base, mupl.eos.mesa.EoS_MESA_opacity),\n",
    "    photosphere_tau = PHOTOSPHERE_TAU,\n",
    "    verbose : int = 2,\n",
    "    interp_method: str = 'basic',    # 'basic' or 'improved'\n",
    "):\n",
    "\n",
    "    \"\"\"Writing the photosphere locations of each dump to json files.\n",
    "\n",
    "    Notes:\n",
    "    Using mpdf.params['hfact']\n",
    "    \"\"\"\n",
    "    # will calc some of it regardless of whether they are included in calc_params\n",
    "    calc_params = ['loc', 'R1', 'rho', 'h', 'T', 'kappa', 'kappaDust', 'srcfunc', 'Skapparho']\n",
    "\n",
    "    \n",
    "    #mpdf = mupl.MyPhantomDataFrames()\n",
    "\n",
    "    \n",
    "    job_name = job_profile['job_name']\n",
    "    #X = job_profile['X']\n",
    "    #ieos = job_profile['ieos']\n",
    "\n",
    "    eos, eos_opacity = eoses\n",
    "\n",
    "    \n",
    "    # init rays directions\n",
    "    rays_dir = {}\n",
    "    for key in rays_dir_def.keys():\n",
    "        rays_dir[key] = np.array(rays_dir_def[key])\n",
    "\n",
    "\n",
    "    # main\n",
    "    for file_index in file_indexes:\n",
    "        \n",
    "        # init answer dict / array\n",
    "        photosphere_pars = { # [legend][par_name][time]\n",
    "            'time_yr': None,\n",
    "            'orbsep_Rsun': None,\n",
    "            'mpart_Msun' : None,\n",
    "            'data': {},\n",
    "            'rays_dir': rays_dir_def,\n",
    "            'rays': {},\n",
    "        }  \n",
    "        for key in rays_dir.keys():\n",
    "            photosphere_pars['data'][key] = {}\n",
    "\n",
    "        # read data\n",
    "        mpdf = mpdf_read(job_name, file_index, eos_opacity, mpdf=None, reset_xyz_by='CoM', do_extrap=False, verbose=verbose)\n",
    "        sdf = mpdf.data['gas']\n",
    "        sdf['srcfunc'] = mpdf.const['sigma_sb'] * sdf['T']**4 / pi\n",
    "        sdf['Skapparho'] = sdf['srcfunc'] * sdf['kappa'] * sdf['rho']\n",
    "        \n",
    "        #mpdf.read(job_name, file_index, reset_xyz_by='CoM', verbose=verbose)\n",
    "        #if 'Tdust' in mpdf.data['gas'].columns:\n",
    "        #    mpdf.data['gas']['T'] = mpdf.data['gas']['Tdust']\n",
    "        #elif 'temperature' in mpdf.data['gas'].columns:\n",
    "        #    mpdf.data['gas']['T'] = mpdf.data['gas']['temperature']\n",
    "        #if 'kappa' not in mpdf.data['gas'].keys():\n",
    "        #    # get kappa from mesa table in cgs units\n",
    "        #    mpdf.data['gas']['kappa'] = eos_opacity.get_kappa(\n",
    "        #        mpdf.get_val('rho', copy=False),\n",
    "        #        mpdf.get_val('T', copy=False),\n",
    "        #        do_extrap=True,\n",
    "        #        return_as_quantity=False)\n",
    "        ## translate to phantom units\n",
    "        #mpdf.calc_sdf_params(\n",
    "        #    calc_params=['kappa',], #'R1',\n",
    "        #    calc_params_params={'ieos': ieos, 'X':X, 'overwrite':False, 'kappa_translate_from_cgs_units':True},\n",
    "        #    verbose=verbose,\n",
    "        #)\n",
    "        hfact = mpdf.params['hfact']\n",
    "        mpart = mpdf.params['mass']\n",
    "        \n",
    "        photosphere_pars['time_yr'] = mpdf.get_time().to_value(units.year)\n",
    "        photosphere_pars['orbsep_Rsun'] = mpdf.get_orb_sep().to_value(units.Rsun)\n",
    "        photosphere_pars['mpart_Msun']  = (mpart * mpdf.units['mass']).to_value(units.Msun)\n",
    "\n",
    "\n",
    "        ## construct kdtree (since we are not changing x, y, z label here)\n",
    "        #sdf_all_kdtree = kdtree.KDTree(np.array(sdf[['x', 'y', 'z']], copy=False))\n",
    "\n",
    "        # construct rays_dict\n",
    "        star_loc = np.array([mpdf.data['sink'][axis][0] for axis in 'xyz'])\n",
    "        rays_dict = {}    # legend: ray\n",
    "        for key in rays_dir.keys():\n",
    "            # init\n",
    "            ray = np.array([\n",
    "                star_loc,\n",
    "                star_loc + rays_dir[key],\n",
    "            ])\n",
    "            rays_dict[key] = ray\n",
    "            photosphere_pars['rays'][key] = ray.tolist()\n",
    "            ray_unit_vec = ray[1, :] - ray[0, :]\n",
    "            ray_unit_vec = ray_unit_vec / np.sum(ray_unit_vec**2)**0.5\n",
    "\n",
    "\n",
    "            # optimization- first select only the particles affecting the ray\n",
    "            #  because interpolation of m points with N particles scales with O(N*m),\n",
    "            #  reducing N can speed up calc significantly\n",
    "            sdf = mpdf.data['gas']\n",
    "            kernel_radius = sdf.kernel.get_radius()\n",
    "            hs = np.array(sdf['h'])\n",
    "            pts = np.array(sdf[['x', 'y', 'z']])    # (npart, 3)-shaped array\n",
    "            pts_on_ray = mupl.get_closest_pt_on_line(pts, ray)\n",
    "            sdf_selected_indices = (np.sum((pts - pts_on_ray)**2, axis=-1) <= (kernel_radius * hs)**2)\n",
    "            if verbose:\n",
    "                debug_info(\n",
    "                    'write_ph_loc_axes()', verbose,\n",
    "                    f\"{np.count_nonzero(sdf_selected_indices)} particles are close enough to the ray to have effects.\"\n",
    "                )\n",
    "            sdf = sdf.iloc[sdf_selected_indices]\n",
    "            pts = np.array(sdf[['x', 'y', 'z']])    # (npart, 3)-shaped array\n",
    "\n",
    "\n",
    "            # get optical depth\n",
    "            if verbose:\n",
    "                debug_info(\n",
    "                    'write_ph_loc_axes()', verbose,\n",
    "                    f\"{ray = }\"\n",
    "                )\n",
    "            pts_on_ray, dtaus, pts_order = mupl.light.get_optical_depth_by_ray_tracing_3D(sdf=sdf, ray=ray)\n",
    "            photosphere, (pts_waypts, pts_waypts_t, taus_waypts, kcs_waypts) = get_photosphere_on_ray(\n",
    "                pts_on_ray, dtaus, pts_order, sdf, ray,    # remove dtaus to force recalc in LCGen way\n",
    "                calc_params = calc_params,\n",
    "                hfact = hfact, mpart=mpart, eos=eos, sdf_units=mpdf.units,\n",
    "                ray_unit_vec=ray_unit_vec, verbose=verbose, photosphere_tau=photosphere_tau,\n",
    "                return_as_quantity=False,\n",
    "            )\n",
    "\n",
    "            if is_verbose(verbose, 'debug'):\n",
    "                say('debug', None, verbose,\n",
    "                    f\"{photosphere = }\\n\",\n",
    "                    f\"{len(taus_waypts) = }\\n\",\n",
    "                )\n",
    "            \n",
    "            photosphere_pars['data'][key] = photosphere\n",
    "            photosphere_pars['data'][key]['size'] = photosphere['R1']\n",
    "            ph_d = photosphere_pars['data'][key]\n",
    "\n",
    "            # save interpolated data on ray at waypoints\n",
    "            ph_d[ 'R1_on_ray'] = pts_waypts_t\n",
    "            ph_d['loc_on_ray'] = ray[0][np.newaxis, :] + ph_d['R1_on_ray'][:, np.newaxis] * ray_unit_vec[np.newaxis, :]\n",
    "            ph_d['tau_on_ray'] = np.interp(ph_d['R1_on_ray'], pts_waypts_t[::-1], taus_waypts[::-1])\n",
    "            ph_d[ 'kc_on_ray'] = np.interp(ph_d['R1_on_ray'], pts_waypts_t[::-1],  kcs_waypts[::-1])\n",
    "            ph_d['rho_on_ray'] = mupl.sph_interp.get_sph_interp(sdf, 'rho', ph_d['loc_on_ray'], verbose=verbose, method=interp_method)\n",
    "            ph_d[  'h_on_ray'] = mupl.sph_interp.get_h_from_rho(ph_d['rho_on_ray'], mpart=mpart, hfact=hfact)\n",
    "            # ph_d[  'u_on_ray'] = mupl.sph_interp.get_sph_interp(\n",
    "            #     sdf, 'u'  , ph_d['loc_on_ray'], verbose=verbose, method=interp_method)\n",
    "            # ph_d[  'T_on_ray'] = eos.get_temp(\n",
    "            #     set_as_quantity(photosphere_pars['data'][key]['rho_on_ray'], mpdf.units['density']),\n",
    "            #     set_as_quantity(photosphere_pars['data'][key]['u_on_ray']  , mpdf.units['specificEnergy']),\n",
    "            #     return_as_quantity=False, bounds_error=False)\n",
    "            for par in calc_params:\n",
    "                if par in {'R1', 'loc', 'tau', 'rho', 'h',}:    # 'u', 'T',\n",
    "                    continue    # already handled\n",
    "                elif par in sdf:    # just interpolate it    #IT-JUST-WORKS\n",
    "                    ph_d[f'{par}_on_ray'] = mupl.sph_interp.get_sph_interp(\n",
    "                        sdf, par, ph_d['loc_on_ray'], verbose=verbose, method=interp_method)\n",
    "                else:\n",
    "                    say('warn', None, verbose, f\"{par} not found in data dump '{job_name}_{file_index:05}'.\")\n",
    "            photosphere_pars['data'][key]['nneigh_on_ray']=mupl.sph_interp.get_no_neigh(\n",
    "                sdf, ph_d['loc_on_ray'], kernel_rad=kernel_radius)\n",
    "\n",
    "            # save actual relevant particle data (pts)\n",
    "            #    Rt_pts: equivalant of R1_on_ray but for particles\n",
    "            ph_d[ 'Rt_at_pts'] = np.sum((pts_on_ray[pts_order] - np.asarray(ray[0])) * ray_unit_vec, axis=-1)\n",
    "            ph_d['Rxy_at_pts'] = get_dist2_between_2pt(pts[pts_order], pts_on_ray[pts_order])**0.5\n",
    "            ph_d['tau_at_pts'] = np.cumsum(dtaus[pts_order])\n",
    "            for par in calc_params:\n",
    "                if par in {'R1', 'loc', 'tau',}:\n",
    "                    continue\n",
    "                elif par in sdf:\n",
    "                    ph_d[f'{par}_at_pts'] = np.asarray(sdf[par].iloc[pts_order])\n",
    "                else:\n",
    "                    say('warn', None, verbose, f\"{par} not found in data dump '{job_name}_{file_index:05}'.\")\n",
    "\n",
    "\n",
    "        hdf5_dump(\n",
    "            photosphere_pars,\n",
    "            f\"{interm_dir}{job_profile['nickname']}_{file_index:05d}.photospherePars.xyz.hdf5\",\n",
    "            metadata=metadata)\n",
    "        del mpdf\n",
    "\n",
    "    return photosphere_pars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862a0ce8-57ed-4eeb-b09b-218f80982760",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe765991-3271-4c30-a63e-18628d8e272e",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_debug = True\n",
    "if do_debug and __name__ == '__main__':\n",
    "    from script_PhLocAxes__input import JOB_PROFILES_DICT\n",
    "    JOB_PROFILES = [JOB_PROFILES_DICT[key] for key in ('2md',)] #('2md', '4md')]\n",
    "    for job_profile in JOB_PROFILES:\n",
    "        job_profile['file_indexes'] = (0, 1200, 4800, 8000, 17600) #(0, 400, 800, 1200, 1600, 2000, 2400, 2800, 3200, 4800, 6400, 8000, 15600, 17600)\n",
    "    # NPROCESSES = 1\n",
    "    verbose=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "951dd520-ce47-42ce-9b21-f4f1951e1cff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*   Note   :    starmapstar() ==> write_ph_loc_axes() ==> hdf5_dump():\n",
      "\tWriting to ../interm/2md_17600.photospherePars.xyz.hdf5  (will OVERWRITE if file already exist.; compress=False)\n",
      "*   Note   :    starmapstar() ==> write_ph_loc_axes() ==> hdf5_dump():\n",
      "\tWriting to ../interm/2md_08000.photospherePars.xyz.hdf5  (will OVERWRITE if file already exist.; compress=False)\n",
      "*   Note   :    starmapstar() ==> write_ph_loc_axes() ==> hdf5_dump():\n",
      "\tWriting to ../interm/2md_00000.photospherePars.xyz.hdf5  (will OVERWRITE if file already exist.; compress=False)\n",
      "*   Note   :    starmapstar() ==> write_ph_loc_axes() ==> hdf5_dump():\n",
      "\tWriting to ../interm/2md_04800.photospherePars.xyz.hdf5  (will OVERWRITE if file already exist.; compress=False)\n",
      "*   Note   :    starmapstar() ==> write_ph_loc_axes() ==> hdf5_dump():\n",
      "\tWriting to ../interm/2md_01200.photospherePars.xyz.hdf5  (will OVERWRITE if file already exist.; compress=False)\n"
     ]
    }
   ],
   "source": [
    "# main process\n",
    "\n",
    "\n",
    "\n",
    "# init rays directions\n",
    "rays_dir_def = {\n",
    "    # legend: ray direction name\n",
    "    '+x'  : [ 1., 0., 0.],\n",
    "    '+y'  : [ 0., 1., 0.],\n",
    "    '+z'  : [ 0., 0., 1.],\n",
    "    '-x'  : [-1., 0., 0.],\n",
    "    '-y'  : [ 0.,-1., 0.],\n",
    "    '-z'  : [ 0., 0.,-1.],\n",
    "}\n",
    "\n",
    "\n",
    "# run main\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    \n",
    "    # get ph loc for each dump file\n",
    "    args = []\n",
    "    for job_profile in JOB_PROFILES:\n",
    "    \n",
    "        file_indexes = job_profile['file_indexes']\n",
    "        #job_name     = job_profile['job_name']\n",
    "        eos          = mupl.get_eos(job_profile['ieos'], job_profile['params'], settings)\n",
    "        eos_opacity  = mupl.eos.mesa.EoS_MESA_opacity(job_profile['params'], settings)\n",
    "    \n",
    "        \n",
    "        if NPROCESSES <= 1:\n",
    "            \n",
    "            # single process\n",
    "    \n",
    "            photosphere_pars = write_ph_loc_axes(\n",
    "                job_profile = job_profile, file_indexes = file_indexes, rays_dir_def = rays_dir_def,\n",
    "                eoses = (eos, eos_opacity), photosphere_tau = PHOTOSPHERE_TAU, verbose = verbose,\n",
    "            )\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            # multi-process\n",
    "\n",
    "            for file_index in file_indexes:\n",
    "                args.append((\n",
    "                    job_profile,\n",
    "                    [file_index],\n",
    "                    rays_dir_def,\n",
    "                    (eos, eos_opacity),\n",
    "                    PHOTOSPHERE_TAU,\n",
    "                    0,\n",
    "                ))\n",
    "                \n",
    "            with Pool(processes=NPROCESSES) as pool:\n",
    "                pool.starmap(write_ph_loc_axes, args)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41b2c0d7-7d76-4452-b7e5-c5b1a47f9aa3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading _metadata__input.json... *   Note   :    run_code() ==> <module>() ==> hdf5_load():\n",
      "\tReading from ../interm/2md_00000.photospherePars.xyz.hdf5  (compress=False)\n",
      "Done.\n",
      "\n",
      "\n",
      "\n",
      "Loading _metadata__input.json... *   Note   :    run_code() ==> <module>() ==> hdf5_load():\n",
      "\tReading from ../interm/2md_01200.photospherePars.xyz.hdf5  (compress=False)\n",
      "Done.\n",
      "\n",
      "\n",
      "\n",
      "Loading _metadata__input.json... *   Note   :    run_code() ==> <module>() ==> hdf5_load():\n",
      "\tReading from ../interm/2md_04800.photospherePars.xyz.hdf5  (compress=False)\n",
      "Done.\n",
      "\n",
      "\n",
      "\n",
      "Loading _metadata__input.json... *   Note   :    run_code() ==> <module>() ==> hdf5_load():\n",
      "\tReading from ../interm/2md_08000.photospherePars.xyz.hdf5  (compress=False)\n",
      "Done.\n",
      "\n",
      "\n",
      "\n",
      "Loading _metadata__input.json... *   Note   :    run_code() ==> <module>() ==> hdf5_load():\n",
      "\tReading from ../interm/2md_17600.photospherePars.xyz.hdf5  (compress=False)\n",
      "Done.\n",
      "\n",
      "*   Note   :    run_code() ==> <module>() ==> hdf5_dump():\n",
      "\tWriting to ../interm/2md.photospherePars.xyz.hdf5  (will OVERWRITE if file already exist.; compress=False)\n",
      "\n",
      "\n",
      "\n",
      "*** All Done. ***\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    # syntheize the files into one big file\n",
    "    \n",
    "    for job_profile in JOB_PROFILES:\n",
    "    \n",
    "        job_name     = job_profile['job_name']\n",
    "        file_indexes = job_profile['file_indexes']\n",
    "    \n",
    "    \n",
    "        # init\n",
    "        photosphere_pars_all = { # [legend][par_name][time]\n",
    "            'time_yr': [],\n",
    "            'orbsep_Rsun': [],\n",
    "            'data': {},\n",
    "            'rays_dir': rays_dir_def,\n",
    "            'rays': {},\n",
    "        }  \n",
    "        for key in rays_dir_def.keys():\n",
    "            photosphere_pars_all['data'][key] = {\n",
    "                'size': [],\n",
    "                'R1'  : [],\n",
    "                'rho' : [],\n",
    "                # 'u'   : [],\n",
    "                'h'   : [],\n",
    "                'T'   : [],\n",
    "                'R1_on_ray' : [],\n",
    "                'tau_on_ray': [],\n",
    "                'rho_on_ray': [],\n",
    "                # 'u_on_ray'  : [],\n",
    "                'T_on_ray'  : [],\n",
    "            }\n",
    "            photosphere_pars_all['rays'][key] = []\n",
    "    \n",
    "        \n",
    "        # fetch\n",
    "        for file_index in file_indexes:\n",
    "                \n",
    "            if verbose: print(f\"\\n\\nLoading {f.name}... \", end='')\n",
    "            \n",
    "            photosphere_pars = hdf5_load(f\"{interm_dir}{job_profile['nickname']}_{file_index:05}.photospherePars.xyz.hdf5\")\n",
    "            for it in ['time_yr', 'orbsep_Rsun']:\n",
    "                photosphere_pars_all[it].append(photosphere_pars[it])\n",
    "            for key in rays_dir_def.keys():\n",
    "                for it in photosphere_pars_all['data'][key].keys():\n",
    "                    obj = photosphere_pars['data'][key][it]\n",
    "                    if isinstance(obj, np.ndarray):\n",
    "                        obj = obj.tolist()\n",
    "                    photosphere_pars_all['data'][key][it].append(np.asanyarray(obj))\n",
    "                photosphere_pars_all['rays'][key].append(photosphere_pars['rays'][key]) \n",
    "\n",
    "            if verbose: print(f\"Done.\\n\")\n",
    "        \n",
    "        # write\n",
    "        hdf5_dump(\n",
    "            photosphere_pars_all,\n",
    "            f\"{interm_dir}{job_profile['nickname']}.photospherePars.xyz.hdf5\",\n",
    "            metadata=metadata)\n",
    "\n",
    "\n",
    "    print(\"\\n\\n\\n*** All Done. ***\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db8b1a9-bb35-4d9c-a05e-30199576b1b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
